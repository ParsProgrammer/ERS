{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment the last two cell to clone the data and enable the GPU\n",
        "#! git clone https://github.com/ParsProgrammer/ERS.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cd ERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzVYQNxJm_W",
        "outputId": "8f98d417-6c2d-47ff-a041-84238103eba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.50.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.19.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.21.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.4.0)\n",
            "Requirement already satisfied: packaging in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (4.3.0)\n",
            "Requirement already satisfied: setuptools in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (61.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.7.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.16.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.9) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.9.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from packaging->tensorflow-gpu==2.9) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.26.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow-text in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-text) (2.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.6)\n",
            "Requirement already satisfied: packaging in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (61.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.50.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.28.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.26.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow-gpu==2.9\n",
        "%pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BTOV54y-ckzq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (4.34.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (9.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1u4wpNQOEUc9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import gc\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import datetime \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTkARkSuo9PR",
        "outputId": "8548446d-69cd-4727-8b8f-851569dd33d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==4.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==4.2) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==4.2) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==4.2) (1.21.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nltk in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (2022.7.25)\n",
            "Requirement already satisfied: click in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from click->nltk) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim==4.2\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnDuJU4fpMLE",
        "outputId": "ecfd7d10-b929-43ce-cd99-0e2213f4e376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/mobin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:03:09.000020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:03:09.015715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.015886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.016769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-12-18 10:03:09.019163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.019368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.019478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.342719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.342895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.343028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-18 10:03:09.343149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2132 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "else :\n",
        "  print(\"No GPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.2.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.28.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.50.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (61.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.9.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorboard_plugin_profile in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.11.1)\n",
            "Requirement already satisfied: gviz-api>=1.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (1.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (61.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (2.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (3.19.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard_plugin_profile) (2.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorboard\n",
        "%pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyypFCrlw2SV"
      },
      "source": [
        "# **Data Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p7IbgFIxRQb-"
      },
      "outputs": [],
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    name=b'\"verified\": \\\"true\\\",'\n",
        "    l=l.replace(b'\"verified\": true,',bytes(name))\n",
        "    name1=b'\"verified\": \\\"false\\\",'\n",
        "    l=l.replace(b'\"verified\": false,',bytes(name))\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('reviews_Grocery_and_Gourmet_Food_5.json.gz')\n",
        "\n",
        "# dataset link\n",
        "# Grocery and Gourmet Food\n",
        "# https://jmcauley.ucsd.edu/data/amazon/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_auxwvSRYsvu"
      },
      "source": [
        "Dataset Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E9FGFZINRQOn",
        "outputId": "0dad1216-9ec6-4528-fa44-32ab26a94602"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1VEELTKS8NLZB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Just another flavor of Kit Kat but the taste i...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A14R9XMZVJ6INB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I bought this on impulse and it comes from Jap...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A27IQHDZFQFNGG</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Really good. Great gift for any fan of green t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A31QY5TASILE89</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I had never had it before, was curious to see ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2LWK003FFMCI5</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I've been looking forward to trying these afte...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151249</th>\n",
              "      <td>A2L6QS8SVHT9RG</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>Delicious gluten-free oatmeal: we tried both t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151250</th>\n",
              "      <td>AFJFXN42RZ3G2</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>With the many selections of instant oatmeal ce...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151251</th>\n",
              "      <td>ASEBX8TBYWQWA</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>While I usually review CDs and DVDs, as well a...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151252</th>\n",
              "      <td>ANKQGTXHREOI5</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>My son and I enjoyed these oatmeal packets.  H...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151253</th>\n",
              "      <td>A2CF66KIQ3RKX3</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>I like to eat oatmeal i the mornings. I usuall...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>151254 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                userID      itemID  \\\n",
              "0       A1VEELTKS8NLZB  616719923X   \n",
              "1       A14R9XMZVJ6INB  616719923X   \n",
              "2       A27IQHDZFQFNGG  616719923X   \n",
              "3       A31QY5TASILE89  616719923X   \n",
              "4       A2LWK003FFMCI5  616719923X   \n",
              "...                ...         ...   \n",
              "151249  A2L6QS8SVHT9RG  B00KCJRVO2   \n",
              "151250   AFJFXN42RZ3G2  B00KCJRVO2   \n",
              "151251   ASEBX8TBYWQWA  B00KCJRVO2   \n",
              "151252   ANKQGTXHREOI5  B00KCJRVO2   \n",
              "151253  A2CF66KIQ3RKX3  B00KCJRVO2   \n",
              "\n",
              "                                               reviewText  rating  \n",
              "0       Just another flavor of Kit Kat but the taste i...     4.0  \n",
              "1       I bought this on impulse and it comes from Jap...     3.0  \n",
              "2       Really good. Great gift for any fan of green t...     4.0  \n",
              "3       I had never had it before, was curious to see ...     5.0  \n",
              "4       I've been looking forward to trying these afte...     4.0  \n",
              "...                                                   ...     ...  \n",
              "151249  Delicious gluten-free oatmeal: we tried both t...     4.0  \n",
              "151250  With the many selections of instant oatmeal ce...     4.0  \n",
              "151251  While I usually review CDs and DVDs, as well a...     5.0  \n",
              "151252  My son and I enjoyed these oatmeal packets.  H...     4.0  \n",
              "151253  I like to eat oatmeal i the mornings. I usuall...     4.0  \n",
              "\n",
              "[151254 rows x 4 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rename(columns={\"reviewerID\": \"userID\", \"asin\": \"itemID\",\"overall\":\"rating\"},inplace=True)\n",
        "df=df[['userID','itemID','reviewText','rating']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31052</th>\n",
              "      <td>A31LKJUARHR0Y</td>\n",
              "      <td>B000PGHLNS</td>\n",
              "      <td>I remember these cookies from the early 1980s ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145051</th>\n",
              "      <td>A3Q1J68QY1MZQ9</td>\n",
              "      <td>B00DBSG2WI</td>\n",
              "      <td>These are disgusting!!! Are we reviewing the s...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67790</th>\n",
              "      <td>A36WGHR8TO5DKT</td>\n",
              "      <td>B0029JDUO8</td>\n",
              "      <td>First, let me just say that the Xyla mint prod...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60239</th>\n",
              "      <td>A27NTHPTRXB766</td>\n",
              "      <td>B001M0AL8I</td>\n",
              "      <td>Never liked oatmeal until fruit entered the pi...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122962</th>\n",
              "      <td>A2HTPS0JV3Q8ZD</td>\n",
              "      <td>B006MONQMC</td>\n",
              "      <td>While I like the MIO additives, I am always he...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106568</th>\n",
              "      <td>A1ITRGMT80D5TK</td>\n",
              "      <td>B004U43ZO0</td>\n",
              "      <td>These cookies are okay but they certainly aren...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53091</th>\n",
              "      <td>A2V0I904FH7ABY</td>\n",
              "      <td>B001EQ55ZO</td>\n",
              "      <td>The cashew are well roasted giving it a nice f...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143427</th>\n",
              "      <td>A2OBDNQ5ZYU1L8</td>\n",
              "      <td>B00CQ92YPW</td>\n",
              "      <td>I don't use a lot of salt in cooking or direct...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87341</th>\n",
              "      <td>ABKNRVFFLEA3M</td>\n",
              "      <td>B003XDH6M6</td>\n",
              "      <td>This is really great tasting licorice.  The po...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41182</th>\n",
              "      <td>A4EWLZLUCY5QR</td>\n",
              "      <td>B0016COPU2</td>\n",
              "      <td>now THIS is a real nice meal when i just can't...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                userID      itemID  \\\n",
              "31052    A31LKJUARHR0Y  B000PGHLNS   \n",
              "145051  A3Q1J68QY1MZQ9  B00DBSG2WI   \n",
              "67790   A36WGHR8TO5DKT  B0029JDUO8   \n",
              "60239   A27NTHPTRXB766  B001M0AL8I   \n",
              "122962  A2HTPS0JV3Q8ZD  B006MONQMC   \n",
              "106568  A1ITRGMT80D5TK  B004U43ZO0   \n",
              "53091   A2V0I904FH7ABY  B001EQ55ZO   \n",
              "143427  A2OBDNQ5ZYU1L8  B00CQ92YPW   \n",
              "87341    ABKNRVFFLEA3M  B003XDH6M6   \n",
              "41182    A4EWLZLUCY5QR  B0016COPU2   \n",
              "\n",
              "                                               reviewText  rating  \n",
              "31052   I remember these cookies from the early 1980s ...     5.0  \n",
              "145051  These are disgusting!!! Are we reviewing the s...     1.0  \n",
              "67790   First, let me just say that the Xyla mint prod...     4.0  \n",
              "60239   Never liked oatmeal until fruit entered the pi...     5.0  \n",
              "122962  While I like the MIO additives, I am always he...     5.0  \n",
              "106568  These cookies are okay but they certainly aren...     3.0  \n",
              "53091   The cashew are well roasted giving it a nice f...     3.0  \n",
              "143427  I don't use a lot of salt in cooking or direct...     5.0  \n",
              "87341   This is really great tasting licorice.  The po...     5.0  \n",
              "41182   now THIS is a real nice meal when i just can't...     5.0  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYb9dMRLXjZT"
      },
      "source": [
        "# **Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yY4NJsolgtl"
      },
      "source": [
        "**User Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9kPFQ1GJr_I"
      },
      "source": [
        "determining all unique users with their reviews and ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "l_eSbOjWRQD3",
        "outputId": "63fd7abe-5957-46a2-aa85-8ca3721bdec5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00177463W0XWB16A9O05</td>\n",
              "      <td>[It is a good stand by coffee you can count on...</td>\n",
              "      <td>[5.0, 5.0, 4.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A022899328A0QROR32DCT</td>\n",
              "      <td>[awesome texture for even the gluten eating ea...</td>\n",
              "      <td>[5.0, 1.0, 4.0, 5.0, 5.0, 5.0, 1.0, 2.0, 3.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A04309042SDSL8YX2HRR7</td>\n",
              "      <td>[I love roasted garlic &amp; sweet bell peppers. Y...</td>\n",
              "      <td>[5.0, 2.0, 4.0, 4.0, 4.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A068255029AHTHDXZURNU</td>\n",
              "      <td>[These bars are especially delicious for cocon...</td>\n",
              "      <td>[5.0, 5.0, 3.0, 5.0, 3.0, 3.0, 5.0, 5.0, 5.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A06944662TFWOKKV4GJKX</td>\n",
              "      <td>[UGH!  My stomach has been really killing me l...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14676</th>\n",
              "      <td>AZWRZZAMX90VT</td>\n",
              "      <td>[Very nice. Not spicy, not too salty, lots of ...</td>\n",
              "      <td>[5.0, 5.0, 2.0, 5.0, 5.0, 4.0, 4.0, 3.0, 4.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14677</th>\n",
              "      <td>AZXKAH2DE6C8A</td>\n",
              "      <td>[Could not imagine having such a rich tasting ...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14678</th>\n",
              "      <td>AZXON596A1VXC</td>\n",
              "      <td>[I was a bit skeptical when I bought this prod...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14679</th>\n",
              "      <td>AZYXC63SS008M</td>\n",
              "      <td>[This is just about the healthiest you can get...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14680</th>\n",
              "      <td>AZZ5ASC403N74</td>\n",
              "      <td>[Everybody loves homemade spaghetti sauce, but...</td>\n",
              "      <td>[5.0, 2.0, 3.0, 3.0, 4.0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14681 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      userID  \\\n",
              "0      A00177463W0XWB16A9O05   \n",
              "1      A022899328A0QROR32DCT   \n",
              "2      A04309042SDSL8YX2HRR7   \n",
              "3      A068255029AHTHDXZURNU   \n",
              "4      A06944662TFWOKKV4GJKX   \n",
              "...                      ...   \n",
              "14676          AZWRZZAMX90VT   \n",
              "14677          AZXKAH2DE6C8A   \n",
              "14678          AZXON596A1VXC   \n",
              "14679          AZYXC63SS008M   \n",
              "14680          AZZ5ASC403N74   \n",
              "\n",
              "                                              reviewText  \\\n",
              "0      [It is a good stand by coffee you can count on...   \n",
              "1      [awesome texture for even the gluten eating ea...   \n",
              "2      [I love roasted garlic & sweet bell peppers. Y...   \n",
              "3      [These bars are especially delicious for cocon...   \n",
              "4      [UGH!  My stomach has been really killing me l...   \n",
              "...                                                  ...   \n",
              "14676  [Very nice. Not spicy, not too salty, lots of ...   \n",
              "14677  [Could not imagine having such a rich tasting ...   \n",
              "14678  [I was a bit skeptical when I bought this prod...   \n",
              "14679  [This is just about the healthiest you can get...   \n",
              "14680  [Everybody loves homemade spaghetti sauce, but...   \n",
              "\n",
              "                                                  rating  \n",
              "0      [5.0, 5.0, 4.0, 4.0, 5.0, 3.0, 4.0, 5.0, 5.0, ...  \n",
              "1      [5.0, 1.0, 4.0, 5.0, 5.0, 5.0, 1.0, 2.0, 3.0, ...  \n",
              "2                              [5.0, 2.0, 4.0, 4.0, 4.0]  \n",
              "3          [5.0, 5.0, 3.0, 5.0, 3.0, 3.0, 5.0, 5.0, 5.0]  \n",
              "4          [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0]  \n",
              "...                                                  ...  \n",
              "14676  [5.0, 5.0, 2.0, 5.0, 5.0, 4.0, 4.0, 3.0, 4.0, ...  \n",
              "14677  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, ...  \n",
              "14678                [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]  \n",
              "14679                     [5.0, 5.0, 5.0, 5.0, 5.0, 5.0]  \n",
              "14680                          [5.0, 2.0, 3.0, 3.0, 4.0]  \n",
              "\n",
              "[14681 rows x 3 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_df=df[['userID','reviewText','rating']].groupby('userID')['rating','reviewText'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['rating'])],index=['reviewText', 'rating'])).reset_index()\n",
        "user_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReAbgOc7q3Tp"
      },
      "source": [
        "**Item Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvKphT7KCTg"
      },
      "source": [
        "determining all unique items with their reviews and ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "qAjW8mmJq9Cj",
        "outputId": "e03f94a0-8923-417b-c9c0-fd8cd33ea676"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>616719923X</td>\n",
              "      <td>[Just another flavor of Kit Kat but the taste ...</td>\n",
              "      <td>[4.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 5.0, 5.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9742356831</td>\n",
              "      <td>[This curry paste makes a delicious curry.  I ...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B00004S1C5</td>\n",
              "      <td>[These dyes create awesome colors for kids cra...</td>\n",
              "      <td>[5.0, 1.0, 5.0, 5.0, 5.0, 4.0, 4.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B0000531B7</td>\n",
              "      <td>[I really enjoy these bars as a quick breakfas...</td>\n",
              "      <td>[5.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 5.0, 4.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B00005344V</td>\n",
              "      <td>[Traditional Medicinals' \"Breathe Easy\" is an ...</td>\n",
              "      <td>[5.0, 3.0, 5.0, 5.0, 3.0, 5.0, 5.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8708</th>\n",
              "      <td>B00JGPG60I</td>\n",
              "      <td>[We switched to this formula 5 days ago and fo...</td>\n",
              "      <td>[4.0, 4.0, 4.0, 5.0, 4.0, 3.0, 5.0, 5.0, 2.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8709</th>\n",
              "      <td>B00JL6LTMW</td>\n",
              "      <td>[We have enjoyed Larabar's variety of bars for...</td>\n",
              "      <td>[4.0, 5.0, 5.0, 5.0, 5.0, 4.0, 5.0, 4.0, 5.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8710</th>\n",
              "      <td>B00K00H9I6</td>\n",
              "      <td>[This 100% pure Canadian maple syrup is a Grad...</td>\n",
              "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8711</th>\n",
              "      <td>B00KC0LGI8</td>\n",
              "      <td>[I followed the directions on the box exactly ...</td>\n",
              "      <td>[2.0, 4.0, 4.0, 4.0, 4.0, 5.0, 2.0, 5.0, 3.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8712</th>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>[Usually the label &amp;#34;gluten free&amp;#34; is a ...</td>\n",
              "      <td>[5.0, 5.0, 4.0, 5.0, 5.0, 3.0, 4.0, 4.0, 5.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8713 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          itemID                                         reviewText  \\\n",
              "0     616719923X  [Just another flavor of Kit Kat but the taste ...   \n",
              "1     9742356831  [This curry paste makes a delicious curry.  I ...   \n",
              "2     B00004S1C5  [These dyes create awesome colors for kids cra...   \n",
              "3     B0000531B7  [I really enjoy these bars as a quick breakfas...   \n",
              "4     B00005344V  [Traditional Medicinals' \"Breathe Easy\" is an ...   \n",
              "...          ...                                                ...   \n",
              "8708  B00JGPG60I  [We switched to this formula 5 days ago and fo...   \n",
              "8709  B00JL6LTMW  [We have enjoyed Larabar's variety of bars for...   \n",
              "8710  B00K00H9I6  [This 100% pure Canadian maple syrup is a Grad...   \n",
              "8711  B00KC0LGI8  [I followed the directions on the box exactly ...   \n",
              "8712  B00KCJRVO2  [Usually the label &#34;gluten free&#34; is a ...   \n",
              "\n",
              "                                                 rating  \n",
              "0     [4.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 5.0, 5.0, ...  \n",
              "1     [5.0, 5.0, 5.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...  \n",
              "2                   [5.0, 1.0, 5.0, 5.0, 5.0, 4.0, 4.0]  \n",
              "3         [5.0, 5.0, 3.0, 5.0, 5.0, 4.0, 5.0, 5.0, 4.0]  \n",
              "4                   [5.0, 3.0, 5.0, 5.0, 3.0, 5.0, 5.0]  \n",
              "...                                                 ...  \n",
              "8708  [4.0, 4.0, 4.0, 5.0, 4.0, 3.0, 5.0, 5.0, 2.0, ...  \n",
              "8709  [4.0, 5.0, 5.0, 5.0, 5.0, 4.0, 5.0, 4.0, 5.0, ...  \n",
              "8710           [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]  \n",
              "8711  [2.0, 4.0, 4.0, 4.0, 4.0, 5.0, 2.0, 5.0, 3.0, ...  \n",
              "8712  [5.0, 5.0, 4.0, 5.0, 5.0, 3.0, 4.0, 4.0, 5.0, ...  \n",
              "\n",
              "[8713 rows x 3 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_df=df[['itemID','reviewText','rating']].groupby('itemID')['reviewText','rating'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['rating'])],index=['reviewText', 'rating'])).reset_index()\n",
        "item_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  after removing the cwd from sys.path.\n",
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  import sys\n",
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# dataset minimization\n",
        "sample_userID = list( user_df[user_df['reviewText'].apply(lambda x : len(x)> 80 )].sample(50)['userID'])\n",
        "new_df = df[ df['userID'].isin(sample_userID)]\n",
        "item_ddf = new_df[['itemID','reviewText','rating']].groupby('itemID')['reviewText','rating'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['rating'])],index=['reviewText', 'rating'])).reset_index()\n",
        "sample_itemID = list( item_ddf[ item_ddf['reviewText'].apply(lambda x : len(x)>4)]['itemID'] )\n",
        "df = new_df[new_df['itemID'].isin(sample_itemID)].reset_index(drop= True)\n",
        "user_df=df[['userID','reviewText','rating']].groupby('userID')['rating','reviewText'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['rating'])],index=['reviewText', 'rating'])).reset_index()\n",
        "item_df=df[['itemID','reviewText','rating']].groupby('itemID')['reviewText','rating'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['rating'])],index=['reviewText', 'rating'])).reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review num :  1914 user num :  50 item num :  198\n"
          ]
        }
      ],
      "source": [
        "print(\"review num : \", df.shape[0] ,\"user num : \" ,user_df.shape[0],\"item num : \",item_df.shape[0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCekVFHgXtom"
      },
      "source": [
        "##   Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "cZb_XiBoGHSa",
        "outputId": "f1cb72ea-8d50-467a-d412-92d75a9d167e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    1914.000000\n",
              "mean        3.964995\n",
              "std         1.032320\n",
              "min         1.000000\n",
              "25%         3.000000\n",
              "50%         4.000000\n",
              "75%         5.000000\n",
              "max         5.000000\n",
              "Name: rating, dtype: float64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.groupby('rating').size().plot(kind=\"bar\");\n",
        "df['rating'].describe()\n",
        "#histogram of ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "hPn4ftb2vqn-",
        "outputId": "91fde917-2de7-4a70-ea94-2e4ae376ea5a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "user_means=user_df.rating.apply(lambda x: np.mean(x))\n",
        "user_means[:50].plot(kind=\"bar\", grid=False, figsize=(16, 6), title=\"Mean ratings for 50 users\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZuVzD2bTv4iE",
        "outputId": "8533423d-08da-4eeb-c78b-51397de9ecff"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2304x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(32,6), sharey=True)\n",
        "user_means.nlargest(50).plot(kind=\"bar\", ax=ax1, title=\"Top 50 users in data set\")\n",
        "user_means.nsmallest(50).plot(kind=\"bar\", ax=ax2, title=\"Bottom 50 users in data set\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "FChjMJ0MvtiU",
        "outputId": "81b5374f-986d-4079-8cb6-a99614f56d4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f611c5387d0>]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "user_means = df.groupby(\"userID\").rating.mean().sort_values()\n",
        "_, ax = plt.subplots(figsize=(20, 6))\n",
        "ax.plot(np.arange(len(user_means)), user_means.values, \"k-\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "VQfmX7OwGaRN",
        "outputId": "65757a6a-d630-49fc-a97c-0742c3a1b7a5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "item_means=item_df.rating.apply(lambda x: np.mean(x))\n",
        "item_means[:50].plot(kind=\"bar\", grid=False, figsize=(16, 6), title=\"Mean ratings for 50 items\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "6Dyp-7KeGyoK",
        "outputId": "6d657471-73d2-4291-aa92-49de438f0a59"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2304x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(32,6), sharey=True)\n",
        "item_means.nlargest(50).plot(kind=\"bar\", ax=ax1, title=\"Top 50 items in data set\")\n",
        "item_means.nsmallest(50).plot(kind=\"bar\", ax=ax2, title=\"Bottom 50 items in data set\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxGLa04uYQRt"
      },
      "source": [
        "## Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0QEq-vnYohN",
        "outputId": "183d2ce2-65c2-49c9-ca8c-7f216083e4bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count                                                  1914\n",
              "unique                                                 1914\n",
              "top       Many reviewers have already commented on the f...\n",
              "freq                                                      1\n",
              "Name: reviewText, dtype: object"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['reviewText'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "OLw7n7EcbEKV",
        "outputId": "ffd1376f-19a1-42f2-a991-e490d04d84d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    1914.000000\n",
              "mean      744.041797\n",
              "std       509.664720\n",
              "min        78.000000\n",
              "25%       429.000000\n",
              "50%       613.500000\n",
              "75%       888.750000\n",
              "max      8136.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# review length\n",
        "rv_le=df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTgNznNQ8aR1",
        "outputId": "3c1809ac-2fc3-4d67-ce83-7253c8687332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean of reviews per user: 38.28\n",
            "mean of words per review: 744.0417972831766\n",
            "mean of words per user: 28481.920000000002\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "sum2=0\n",
        "for x in user_df['reviewText']:\n",
        "  sum+=len(x)\n",
        "  for y in x:\n",
        "   sum2 +=len(y)\n",
        "\n",
        "r_mean=sum/len(user_df['reviewText'])\n",
        "w_mean=sum2/(r_mean*len(user_df['reviewText']))\n",
        "print('mean of reviews per user:', r_mean)\n",
        "print('mean of words per review:', w_mean)\n",
        "print('mean of words per user:',r_mean * w_mean)\n",
        "# plot for reviews per user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Series([], Name: reviewText, dtype: int64)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=item_df['reviewText'].apply(len)\n",
        "a[a>40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    198.000000\n",
              "mean       9.666667\n",
              "std        4.472136\n",
              "min        5.000000\n",
              "25%        6.000000\n",
              "50%        9.000000\n",
              "75%       13.000000\n",
              "max       30.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rv_le=item_df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count     50.000000\n",
              "mean      38.280000\n",
              "std       30.700023\n",
              "min        1.000000\n",
              "25%       12.750000\n",
              "50%       31.000000\n",
              "75%       61.750000\n",
              "max      115.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rv_le=user_df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nje_ix7gAaf3",
        "outputId": "c3260e88-aa4f-46f3-aa38-85cbb25e0219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean of reviews per item: 9.666666666666666\n",
            "mean of words per review: 744.0417972831767\n",
            "mean of words per item: 7192.404040404042\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "sum2=0\n",
        "for x in item_df['reviewText']:\n",
        "  sum+=len(x)\n",
        "  for y in x:\n",
        "   sum2 +=len(y)\n",
        "\n",
        "r_mean=sum/len(item_df['reviewText'])\n",
        "w_mean=sum2/(r_mean*len(item_df['reviewText']))\n",
        "print('mean of reviews per item:', r_mean)\n",
        "print('mean of words per review:', w_mean)\n",
        "print('mean of words per item:',r_mean * w_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 192  193  194 ... 1911 1912 1913] 1722\n",
            "[   0    1    2 ... 1911 1912 1913] 1722\n",
            "[   0    1    2 ... 1911 1912 1913] 1722\n",
            "[   0    1    2 ... 1911 1912 1913] 1722\n",
            "[   0    1    2 ... 1911 1912 1913] 1723\n",
            "[   0    1    2 ... 1911 1912 1913] 1723\n",
            "[   0    1    2 ... 1911 1912 1913] 1723\n",
            "[   0    1    2 ... 1911 1912 1913] 1723\n",
            "[   0    1    2 ... 1911 1912 1913] 1723\n",
            "[   0    1    2 ... 1720 1721 1722] 1723\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A36WGHR8TO5DKT</td>\n",
              "      <td>B00099XNG0</td>\n",
              "      <td>Many reviewers have already commented on the f...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AKMEY1BSHSDG7</td>\n",
              "      <td>B00099XNG0</td>\n",
              "      <td>Chex Mix Sour Cream &amp; Onion flavor is a good s...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A1X1CEGHTHMBL1</td>\n",
              "      <td>B00099XNG0</td>\n",
              "      <td>Wow, what a mixture of great tasting things.  ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A1ZH9LWMX5UCFJ</td>\n",
              "      <td>B00099XNG0</td>\n",
              "      <td>I have always been a fan of sweet and salty sn...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2MNB77YGJ3CN0</td>\n",
              "      <td>B00099XNG0</td>\n",
              "      <td>Excellent snack for those of us who prefer our...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>A2V92F5R7MLCVI</td>\n",
              "      <td>B00DBSGJ9O</td>\n",
              "      <td>These are a version of gummies. They are sweet...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>A1LACH6MLQWZ</td>\n",
              "      <td>B00DDT116M</td>\n",
              "      <td>I was totally unaware of Matcha Green Tea Powd...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1720</th>\n",
              "      <td>A1ZPY91VE3IDN1</td>\n",
              "      <td>B00DDT116M</td>\n",
              "      <td>I regularly drink green tea and Matcha green t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>AEC90GPFKLAAW</td>\n",
              "      <td>B00DDT116M</td>\n",
              "      <td>I make smoothies for our family almost every d...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1722</th>\n",
              "      <td>A1UQBFCERIP7VJ</td>\n",
              "      <td>B00DDT116M</td>\n",
              "      <td>This culinary matcha lends a beautiful color a...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1723 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              userID      itemID  \\\n",
              "0     A36WGHR8TO5DKT  B00099XNG0   \n",
              "1      AKMEY1BSHSDG7  B00099XNG0   \n",
              "2     A1X1CEGHTHMBL1  B00099XNG0   \n",
              "3     A1ZH9LWMX5UCFJ  B00099XNG0   \n",
              "4     A2MNB77YGJ3CN0  B00099XNG0   \n",
              "...              ...         ...   \n",
              "1718  A2V92F5R7MLCVI  B00DBSGJ9O   \n",
              "1719    A1LACH6MLQWZ  B00DDT116M   \n",
              "1720  A1ZPY91VE3IDN1  B00DDT116M   \n",
              "1721   AEC90GPFKLAAW  B00DDT116M   \n",
              "1722  A1UQBFCERIP7VJ  B00DDT116M   \n",
              "\n",
              "                                             reviewText  rating  \n",
              "0     Many reviewers have already commented on the f...     5.0  \n",
              "1     Chex Mix Sour Cream & Onion flavor is a good s...     4.0  \n",
              "2     Wow, what a mixture of great tasting things.  ...     5.0  \n",
              "3     I have always been a fan of sweet and salty sn...     5.0  \n",
              "4     Excellent snack for those of us who prefer our...     5.0  \n",
              "...                                                 ...     ...  \n",
              "1718  These are a version of gummies. They are sweet...     3.0  \n",
              "1719  I was totally unaware of Matcha Green Tea Powd...     5.0  \n",
              "1720  I regularly drink green tea and Matcha green t...     4.0  \n",
              "1721  I make smoothies for our family almost every d...     5.0  \n",
              "1722  This culinary matcha lends a beautiful color a...     5.0  \n",
              "\n",
              "[1723 rows x 4 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_fold=10\n",
        "kfold = KFold(10)\n",
        "random_iterator=kfold.split(df)\n",
        "for i in range(current_fold):\n",
        "  train_index, test_index = next(random_iterator, None)\n",
        "  print(train_index,len(train_index))\n",
        "  train_df, test_df =df.iloc[train_index], df.iloc[test_index]\n",
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvMysaUvxxU3"
      },
      "source": [
        "# **Embedding Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WovD0fA5q-J6"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "embedding_dim=300\n",
        "min_frequent_word_num=1\n",
        "max_vocab_size=10000\n",
        "sequence_length=64\n",
        "document_length=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text= tf.strings.reduce_join( tf.strings.split(text)[:,:sequence_length-2],axis=-1,separator=' ')\n",
        "  return  text.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "    user_corpus =list(map(tf_lower_and_split_punct,user_df['reviewText'])) \n",
        "    item_corpus = list( map(tf_lower_and_split_punct,item_df['reviewText']) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_word2vec_model():\n",
        "    if os.path.exists(\"word2vec.wordvectors\"):\n",
        "        print(\"loaded from word2vec.wordvectors\")\n",
        "        return KeyedVectors.load(\"word2vec.wordvectors\",mmap='r')\n",
        "    else:\n",
        "        # downloading google news word2vec model\n",
        "        if not os.path.exists(\"word2vec_google.bin\"):\n",
        "            downloaded_model = api.load('word2vec-google-news-300')\n",
        "            downloaded_model.save_word2vec_format('word2vec_google.bin',binary=True)\n",
        "            del downloaded_model\n",
        "        # loading google news word2vec model\n",
        "        google_word2vec = KeyedVectors.load_word2vec_format(\"word2vec_google.bin\", binary=True)\n",
        "\n",
        "       # tokenizing the whole reviews in text_corpus\n",
        "        text_corpus=[]\n",
        "        for i,doc in enumerate(user_corpus):    # iterate through each sentence in the reviews\n",
        "            for rv in doc:\n",
        "                for sen in sent_tokenize(rv.decode(\"utf-8\")):\n",
        "                    temp = []\n",
        "                    # tokenize the sentence into words          \n",
        "                    for j in word_tokenize(sen):\n",
        "                        temp.append(j.lower())\n",
        "                    text_corpus.append(temp)\n",
        "                    del temp\n",
        "\n",
        "\n",
        "        # creating a new word2vec model and initializing it from pretrained google_word2vec\n",
        "        word2vec_model=Word2Vec( text_corpus,max_final_vocab=max_vocab_size,min_count=min_frequent_word_num ,vector_size= embedding_dim,window = 5,workers=16, sg=1,epochs=1)\n",
        "        word2vec_model.build_vocab(text_corpus)\n",
        "\n",
        "        word2vec_model.build_vocab([google_word2vec.index_to_key],update=True)\n",
        "        word2vec_model.wv.vectors_lockf = np.ones(len(word2vec_model.wv))\n",
        "        word2vec_model.wv.intersect_word2vec_format(\"word2vec_google.bin\",binary=True,lockf=1.0)\n",
        "        \n",
        "        # fine tuning the model and saving it\n",
        "        word2vec_model.train(text_corpus, epochs=5, total_examples=word2vec_model.corpus_count)\n",
        "        word2vec_model.wv.save(\"word2vec.wordvectors\")\n",
        "        \n",
        "        del google_word2vec\n",
        "        del text_corpus[:]\n",
        "        gc.collect()\n",
        "  \n",
        "        return word2vec_model.wv\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding matrix shape : ( 6367  , 300 )\n"
          ]
        }
      ],
      "source": [
        "# loading the embedding lookup matrix shape=( 30k ,300 ) approximately\n",
        "embedding_matrix = load_word2vec_model() \n",
        "print( \"embedding matrix shape : (\",len(embedding_matrix.index_to_key),\" ,\",embedding_matrix.vector_size,\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.',\n",
              " 'the',\n",
              " ',',\n",
              " 'a',\n",
              " 'and',\n",
              " 'i',\n",
              " 'of',\n",
              " 'to',\n",
              " 'is',\n",
              " 'it',\n",
              " 'this',\n",
              " 'in',\n",
              " 'that',\n",
              " 'but',\n",
              " 'with',\n",
              " 'for',\n",
              " 'are',\n",
              " 'was',\n",
              " 'not',\n",
              " 'like',\n",
              " 'my',\n",
              " 'flavor',\n",
              " 'as',\n",
              " 'have',\n",
              " 'these',\n",
              " 'on',\n",
              " 'they',\n",
              " 'taste',\n",
              " 'good',\n",
              " 'you',\n",
              " 'tea',\n",
              " 'so',\n",
              " 'or',\n",
              " 'its',\n",
              " 'very',\n",
              " 'be',\n",
              " 'just',\n",
              " 'has',\n",
              " 'one',\n",
              " 'more',\n",
              " 'than',\n",
              " 'me',\n",
              " 'really',\n",
              " 'water',\n",
              " 'when',\n",
              " 'can',\n",
              " 'all',\n",
              " 'them',\n",
              " 'some',\n",
              " 'if',\n",
              " 'from',\n",
              " 'at',\n",
              " 'nice',\n",
              " 'little',\n",
              " 'pasta',\n",
              " 'an',\n",
              " 'too',\n",
              " 'much',\n",
              " 'coffee',\n",
              " 'great',\n",
              " 'had',\n",
              " '!',\n",
              " 'love',\n",
              " 'there',\n",
              " 'out',\n",
              " 'sweet',\n",
              " 'would',\n",
              " 'bit',\n",
              " 'ive',\n",
              " 'up',\n",
              " 'tried',\n",
              " 'we',\n",
              " 'which',\n",
              " 'drink',\n",
              " 'no',\n",
              " 'easy',\n",
              " 'make',\n",
              " 'rice',\n",
              " 'side',\n",
              " 'im',\n",
              " 'well',\n",
              " 'chocolate',\n",
              " 'do',\n",
              " 'other',\n",
              " 'were',\n",
              " 'texture',\n",
              " 'use',\n",
              " 'sauce',\n",
              " 'because',\n",
              " 'sugar',\n",
              " 'product',\n",
              " 'about',\n",
              " 'tastes',\n",
              " 'also',\n",
              " 'what',\n",
              " 'dont',\n",
              " 'cup',\n",
              " 'better',\n",
              " 'get',\n",
              " 'only',\n",
              " 'makes',\n",
              " 'quite',\n",
              " 'been',\n",
              " 'first',\n",
              " 'cheese',\n",
              " 'mix',\n",
              " 'cookies',\n",
              " 'any',\n",
              " 'whole',\n",
              " 'try',\n",
              " 'bag',\n",
              " 'pretty',\n",
              " 'used',\n",
              " 'delicious',\n",
              " 'even',\n",
              " 'made',\n",
              " 'hot',\n",
              " 'couscous',\n",
              " 'by',\n",
              " 'didnt',\n",
              " 'box',\n",
              " 'ingredients',\n",
              " 'add',\n",
              " 'tasty',\n",
              " 'will',\n",
              " 'flavors',\n",
              " 'barilla',\n",
              " 'lot',\n",
              " 'something',\n",
              " 'after',\n",
              " 'calories',\n",
              " 'fruit',\n",
              " 'oil',\n",
              " 'think',\n",
              " 'did',\n",
              " 'always',\n",
              " 'time',\n",
              " 'your',\n",
              " 'green',\n",
              " 'most',\n",
              " 'while',\n",
              " 'am',\n",
              " 'eat',\n",
              " 'way',\n",
              " 'added',\n",
              " 'then',\n",
              " 'dish',\n",
              " 'does',\n",
              " 'crackers',\n",
              " 'different',\n",
              " 'snack',\n",
              " 'without',\n",
              " 'chicken',\n",
              " 'each',\n",
              " 'butter',\n",
              " 'energy',\n",
              " 'over',\n",
              " 'bar',\n",
              " 'apple',\n",
              " 'both',\n",
              " 'free',\n",
              " 'regular',\n",
              " 'however',\n",
              " 'liked',\n",
              " 'milk',\n",
              " 'into',\n",
              " 'chips',\n",
              " 'enough',\n",
              " 'comes',\n",
              " 'their',\n",
              " 'favorite',\n",
              " 'fresh',\n",
              " 'enjoy',\n",
              " 'popcorn',\n",
              " 'go',\n",
              " 'perfect',\n",
              " 'still',\n",
              " 'tasted',\n",
              " 'many',\n",
              " 'best',\n",
              " 'two',\n",
              " 'being',\n",
              " 'right',\n",
              " 'since',\n",
              " 'strong',\n",
              " 'found',\n",
              " 'small',\n",
              " 'garlic',\n",
              " 'big',\n",
              " 'our',\n",
              " 'serving',\n",
              " 'cinnamon',\n",
              " 'size',\n",
              " 'less',\n",
              " 'ounce',\n",
              " 'salt',\n",
              " 'caffeine',\n",
              " 'say',\n",
              " 'tasting',\n",
              " 'organic',\n",
              " 'bars',\n",
              " 'though',\n",
              " 'bags',\n",
              " 'those',\n",
              " 'brand',\n",
              " 'food',\n",
              " 'fan',\n",
              " 'few',\n",
              " 'package',\n",
              " 'high',\n",
              " 'wasnt',\n",
              " 'could',\n",
              " 'coconut',\n",
              " 'pack',\n",
              " 'thought',\n",
              " 'family',\n",
              " 'new',\n",
              " 'light',\n",
              " 'black',\n",
              " 'quality',\n",
              " 'seasoning',\n",
              " 'actually',\n",
              " 'things',\n",
              " 'find',\n",
              " 'lemon',\n",
              " 'real',\n",
              " 'bad',\n",
              " 'rather',\n",
              " 'crunchy',\n",
              " 'want',\n",
              " 'gluten',\n",
              " 'trying',\n",
              " 'thing',\n",
              " 'blend',\n",
              " 'using',\n",
              " 'lipton',\n",
              " 'flavored',\n",
              " 'spicy',\n",
              " 'grain',\n",
              " 'come',\n",
              " 'never',\n",
              " 'salty',\n",
              " 'before',\n",
              " 'prefer',\n",
              " 'teas',\n",
              " 'quick',\n",
              " 'corn',\n",
              " 'how',\n",
              " 'definitely',\n",
              " 'cook',\n",
              " 'doesnt',\n",
              " 'snacks',\n",
              " 'minutes',\n",
              " 'isnt',\n",
              " 'give',\n",
              " 'prepare',\n",
              " 'honey',\n",
              " 'same',\n",
              " 'hard',\n",
              " 'meal',\n",
              " 'anything',\n",
              " 'ginger',\n",
              " 'contains',\n",
              " 'usually',\n",
              " 'vanilla',\n",
              " 'theyre',\n",
              " 'per',\n",
              " 'plain',\n",
              " 'natural',\n",
              " 'white',\n",
              " 'similar',\n",
              " 'buy',\n",
              " 'who',\n",
              " 'take',\n",
              " 'spaghetti',\n",
              " 'bottle',\n",
              " 'amount',\n",
              " 'off',\n",
              " 'need',\n",
              " 'healthy',\n",
              " 'sure',\n",
              " 'enjoyed',\n",
              " 'caramel',\n",
              " 'plus',\n",
              " 'drinks',\n",
              " 'protein',\n",
              " 'work',\n",
              " 'dressing',\n",
              " 'eating',\n",
              " 'put',\n",
              " 'noodles',\n",
              " 'dark',\n",
              " 'soup',\n",
              " 'crunch',\n",
              " 'version',\n",
              " 'looking',\n",
              " 'variety',\n",
              " 'diet',\n",
              " 'now',\n",
              " 'cookie',\n",
              " 'people',\n",
              " 'cheddar',\n",
              " 'products',\n",
              " 'microwave',\n",
              " 'several',\n",
              " 'type',\n",
              " 'thin',\n",
              " 'aftertaste',\n",
              " 'dried',\n",
              " 'artificial',\n",
              " 'either',\n",
              " 'stuff',\n",
              " 'mild',\n",
              " 'although',\n",
              " '?',\n",
              " 'nothing',\n",
              " 'review',\n",
              " 'having',\n",
              " 'cups',\n",
              " 'pleasant',\n",
              " 'mates',\n",
              " 'chewy',\n",
              " 'juice',\n",
              " 'slightly',\n",
              " 'day',\n",
              " 'seems',\n",
              " 'long',\n",
              " 'cant',\n",
              " 'foods',\n",
              " 'making',\n",
              " 'id',\n",
              " 'home',\n",
              " 'through',\n",
              " 'know',\n",
              " 'cooked',\n",
              " 'packet',\n",
              " 'especially',\n",
              " 'wheat',\n",
              " 'mixed',\n",
              " 'might',\n",
              " 'thats',\n",
              " 'decent',\n",
              " 'kids',\n",
              " 'top',\n",
              " 'dry',\n",
              " 'pepper',\n",
              " 'formula',\n",
              " 'cereal',\n",
              " 'creamy',\n",
              " 'knorr',\n",
              " 'instead',\n",
              " 'own',\n",
              " 'fact',\n",
              " 'years',\n",
              " 'salad',\n",
              " 'loved',\n",
              " 'sweetness',\n",
              " 'oz',\n",
              " 'often',\n",
              " 'far',\n",
              " 'ok',\n",
              " 'cooking',\n",
              " 'got',\n",
              " 'wonderful',\n",
              " 'smell',\n",
              " 'almost',\n",
              " 'keep',\n",
              " 'oatmeal',\n",
              " 'tuna',\n",
              " 'beverage',\n",
              " 'convenient',\n",
              " 'down',\n",
              " 'kind',\n",
              " 'three',\n",
              " 'flour',\n",
              " 'fiber',\n",
              " 'price',\n",
              " 'part',\n",
              " 'surprised',\n",
              " 'olive',\n",
              " 'pieces',\n",
              " 'see',\n",
              " 'together',\n",
              " 'broccoli',\n",
              " 'treat',\n",
              " 'feel',\n",
              " 'here',\n",
              " 'peanut',\n",
              " 'traditional',\n",
              " 'directions',\n",
              " 'theres',\n",
              " 'grams',\n",
              " 'absolutely',\n",
              " 'bland',\n",
              " 'simple',\n",
              " 'powder',\n",
              " 'seem',\n",
              " 'combination',\n",
              " 'filling',\n",
              " 'excellent',\n",
              " 'fine',\n",
              " 'roast',\n",
              " 'pastas',\n",
              " 'sometimes',\n",
              " 'every',\n",
              " 'ever',\n",
              " 'pearl',\n",
              " 'recommended',\n",
              " 'heat',\n",
              " 'adding',\n",
              " 'easily',\n",
              " 'open',\n",
              " 'low',\n",
              " 'again',\n",
              " 'back',\n",
              " 'brown',\n",
              " 'normally',\n",
              " 'probably',\n",
              " 'tomato',\n",
              " 'cracker',\n",
              " 'half',\n",
              " 'said',\n",
              " 'keurig',\n",
              " 'youre',\n",
              " 'unfortunately',\n",
              " 'expected',\n",
              " 'line',\n",
              " 'sort',\n",
              " 'convenience',\n",
              " 'mio',\n",
              " 'choice',\n",
              " 'graham',\n",
              " 'idea',\n",
              " 'pretzels',\n",
              " 'care',\n",
              " 'crispy',\n",
              " 'instant',\n",
              " 'going',\n",
              " 'amazon',\n",
              " 'drinking',\n",
              " 'arent',\n",
              " 'refreshing',\n",
              " 'beans',\n",
              " 'hint',\n",
              " 'cooks',\n",
              " 'red',\n",
              " 'ones',\n",
              " 'hand',\n",
              " 'smooth',\n",
              " 'full',\n",
              " 'expect',\n",
              " 'parmesan',\n",
              " 'morning',\n",
              " 'prepared',\n",
              " 'huge',\n",
              " 'quinoa',\n",
              " 'packets',\n",
              " 'able',\n",
              " 'spice',\n",
              " 'mac',\n",
              " 'flavorful',\n",
              " 'simply',\n",
              " 'fat',\n",
              " 'shape',\n",
              " 'blackberry',\n",
              " 'received',\n",
              " 'extra',\n",
              " 'fudge',\n",
              " 'bitter',\n",
              " 'potato',\n",
              " 'around',\n",
              " 'he',\n",
              " 'house',\n",
              " 'baby',\n",
              " 'fun',\n",
              " 'syrup',\n",
              " 'such',\n",
              " 'breakfast',\n",
              " 'bits',\n",
              " 'fast',\n",
              " 'cocoa',\n",
              " 'curry',\n",
              " 'until',\n",
              " 'myself',\n",
              " 'grains',\n",
              " 'preparation',\n",
              " 'ill',\n",
              " 'itself',\n",
              " 'experience',\n",
              " 'iced',\n",
              " 'came',\n",
              " 'packaging',\n",
              " 'soft',\n",
              " 'sale',\n",
              " 'goes',\n",
              " 'servings',\n",
              " 'ranch',\n",
              " 'container',\n",
              " 'herbal',\n",
              " 'vitamin',\n",
              " 'overall',\n",
              " 'brew',\n",
              " 'nicely',\n",
              " 'recipe',\n",
              " 'times',\n",
              " 'orange',\n",
              " 'chipotle',\n",
              " 'certainly',\n",
              " 'couple',\n",
              " 'difference',\n",
              " 'macaroni',\n",
              " 'candy',\n",
              " 'peppermint',\n",
              " 'cold',\n",
              " 'let',\n",
              " 'slight',\n",
              " 'between',\n",
              " 'lemonade',\n",
              " 'happy',\n",
              " 'another',\n",
              " 'original',\n",
              " 'sides',\n",
              " 'she',\n",
              " 'bite',\n",
              " 'old',\n",
              " 'may',\n",
              " 'wouldnt',\n",
              " 'glass',\n",
              " 'crisp',\n",
              " 'contain',\n",
              " 'brands',\n",
              " 'sodium',\n",
              " 'perfectly',\n",
              " 'fairly',\n",
              " 'chai',\n",
              " 'weve',\n",
              " 'quickly',\n",
              " 'generally',\n",
              " 'ice',\n",
              " 'away',\n",
              " 'available',\n",
              " 'daughter',\n",
              " 'apples',\n",
              " 'course',\n",
              " 'should',\n",
              " 'vegetables',\n",
              " 'roasted',\n",
              " 'says',\n",
              " 'kick',\n",
              " 'fruity',\n",
              " 'pyramid',\n",
              " 'mg',\n",
              " 'granola',\n",
              " 'cut',\n",
              " 'thai',\n",
              " 'yet',\n",
              " 'us',\n",
              " 'decided',\n",
              " 'serve',\n",
              " 'amazing',\n",
              " 'normal',\n",
              " 'mind',\n",
              " 'powdered',\n",
              " 'almonds',\n",
              " 'reason',\n",
              " 'ounces',\n",
              " 'ingredient',\n",
              " 'clean',\n",
              " 'large',\n",
              " 'vine',\n",
              " 'rich',\n",
              " 'types',\n",
              " 'strawberry',\n",
              " 'main',\n",
              " 'brewed',\n",
              " 'works',\n",
              " 'pleasantly',\n",
              " 'leaves',\n",
              " 'okay',\n",
              " 'store',\n",
              " 'gave',\n",
              " 'sweetened',\n",
              " 'thick',\n",
              " 'cream',\n",
              " 'noticed',\n",
              " 'boil',\n",
              " 'linguine',\n",
              " 'peach',\n",
              " 'spices',\n",
              " 'potatoes',\n",
              " 'days',\n",
              " 'pleased',\n",
              " 'g',\n",
              " 'calorie',\n",
              " 'berry',\n",
              " 'baked',\n",
              " 'cheesy',\n",
              " 'lunch',\n",
              " 'nuts',\n",
              " 'bought',\n",
              " 'list',\n",
              " 'overly',\n",
              " 'getting',\n",
              " 'chip',\n",
              " 'content',\n",
              " 'ended',\n",
              " 'where',\n",
              " 'mouth',\n",
              " 'addition',\n",
              " 'bacon',\n",
              " 'tomatoes',\n",
              " 'coating',\n",
              " 'eaten',\n",
              " 'italian',\n",
              " 'husband',\n",
              " 'penne',\n",
              " 'result',\n",
              " 'lots',\n",
              " 'mango',\n",
              " 'mixes',\n",
              " 'stick',\n",
              " 'somewhat',\n",
              " 'pouch',\n",
              " 'last',\n",
              " 'went',\n",
              " 'dishes',\n",
              " 'expecting',\n",
              " 'opened',\n",
              " 'glutenfree',\n",
              " 'excited',\n",
              " 'opinion',\n",
              " 'purchased',\n",
              " 'past',\n",
              " 'year',\n",
              " 'kettle',\n",
              " 'satisfying',\n",
              " 'usual',\n",
              " 'color',\n",
              " 'recommend',\n",
              " 'her',\n",
              " 'look',\n",
              " 'served',\n",
              " 'boxes',\n",
              " 'others',\n",
              " 'couldnt',\n",
              " 'oats',\n",
              " 'maybe',\n",
              " 'occasionally',\n",
              " 'including',\n",
              " 'fit',\n",
              " 'alternative',\n",
              " 'standard',\n",
              " 'end',\n",
              " 'particular',\n",
              " 'sardines',\n",
              " 'dinner',\n",
              " 'needs',\n",
              " 'varieties',\n",
              " 'change',\n",
              " 'fantastic',\n",
              " 'baking',\n",
              " 'name',\n",
              " 'paper',\n",
              " 'easier',\n",
              " 'onion',\n",
              " 'super',\n",
              " 'flavoring',\n",
              " 'takes',\n",
              " 'least',\n",
              " 'peppers',\n",
              " 'someone',\n",
              " 'recently',\n",
              " 'puerh',\n",
              " 'turned',\n",
              " 'took',\n",
              " 'v',\n",
              " 'interesting',\n",
              " 'heavy',\n",
              " 'based',\n",
              " 'why',\n",
              " 'hit',\n",
              " 'packaged',\n",
              " 'keebler',\n",
              " 'case',\n",
              " 'compared',\n",
              " 'special',\n",
              " 'impressed',\n",
              " 'admit',\n",
              " 'kcups',\n",
              " 'ready',\n",
              " 'instructions',\n",
              " 'perhaps',\n",
              " 'plastic',\n",
              " 'provides',\n",
              " 'soda',\n",
              " 'seeds',\n",
              " 'homemade',\n",
              " 'sweetener',\n",
              " 'gives',\n",
              " 'particularly',\n",
              " 'pomegranate',\n",
              " 'soy',\n",
              " 'pleasing',\n",
              " 'single',\n",
              " 'place',\n",
              " 'vegetable',\n",
              " 'loves',\n",
              " 'meat',\n",
              " 'followed',\n",
              " 'weak',\n",
              " 'pretzel',\n",
              " 'mayonnaise',\n",
              " 'everyone',\n",
              " 'sticks',\n",
              " 'test',\n",
              " 'vinegar',\n",
              " 'blueberry',\n",
              " 'canned',\n",
              " 'option',\n",
              " 'expensive',\n",
              " 'style',\n",
              " 'chili',\n",
              " 'subtle',\n",
              " 'surprisingly',\n",
              " 'mint',\n",
              " 'start',\n",
              " 'pure',\n",
              " 'mine',\n",
              " 'mostly',\n",
              " 'purchase',\n",
              " 'disappointed',\n",
              " 'appealing',\n",
              " 'left',\n",
              " 'bell',\n",
              " 'ways',\n",
              " 'miss',\n",
              " 'likes',\n",
              " 'reminds',\n",
              " 'overpowering',\n",
              " 'buying',\n",
              " 'worked',\n",
              " 'base',\n",
              " 'ate',\n",
              " 'pot',\n",
              " 'wish',\n",
              " 'pop',\n",
              " 'else',\n",
              " 'boxed',\n",
              " 'spread',\n",
              " 'lid',\n",
              " 'health',\n",
              " 'entire',\n",
              " 'tiny',\n",
              " 'balls',\n",
              " 'clusters',\n",
              " 'arrived',\n",
              " 'rest',\n",
              " 'vitamins',\n",
              " 'pita',\n",
              " 'trail',\n",
              " 'exception',\n",
              " 'point',\n",
              " 'consider',\n",
              " 'covered',\n",
              " 'youll',\n",
              " 'filled',\n",
              " 'despite',\n",
              " 'ago',\n",
              " 'handy',\n",
              " 'sauces',\n",
              " 'must',\n",
              " 'break',\n",
              " 'appearance',\n",
              " 'exactly',\n",
              " 'smells',\n",
              " 'six',\n",
              " 'second',\n",
              " 'reviews',\n",
              " 'options',\n",
              " 'quaker',\n",
              " 'bee',\n",
              " 'bumble',\n",
              " 'recipes',\n",
              " 'completely',\n",
              " 'turn',\n",
              " 'needed',\n",
              " 'compare',\n",
              " 'program',\n",
              " 'close',\n",
              " 'fish',\n",
              " 'along',\n",
              " 'liquid',\n",
              " 'stars',\n",
              " 'balance',\n",
              " 'fill',\n",
              " 'basically',\n",
              " 'pour',\n",
              " 'medium',\n",
              " 'done',\n",
              " 'looks',\n",
              " 'tend',\n",
              " 'allows',\n",
              " 'daily',\n",
              " 'aloe',\n",
              " 'surprise',\n",
              " 'veggies',\n",
              " 'aroma',\n",
              " 'garden',\n",
              " 'unlike',\n",
              " 'boiling',\n",
              " 'dip',\n",
              " 'yummy',\n",
              " 'due',\n",
              " 'bowl',\n",
              " 'short',\n",
              " 'mushy',\n",
              " 'under',\n",
              " 'consistency',\n",
              " 'results',\n",
              " 'grew',\n",
              " 'issue',\n",
              " 'pantry',\n",
              " 'nutrition',\n",
              " 'ordered',\n",
              " 'yes',\n",
              " 'benefits',\n",
              " 'stove',\n",
              " 'chunks',\n",
              " 'mushroom',\n",
              " 'healthier',\n",
              " 'except',\n",
              " 'attractive',\n",
              " 'havent',\n",
              " 'highly',\n",
              " 'once',\n",
              " 'machine',\n",
              " 'punch',\n",
              " 'given',\n",
              " 'packs',\n",
              " 'larger',\n",
              " 'designed',\n",
              " 'item',\n",
              " 'msg',\n",
              " 'everything',\n",
              " 'reading',\n",
              " 'diabetic',\n",
              " 'cherry',\n",
              " 'months',\n",
              " 'bold',\n",
              " 'star',\n",
              " 'adds',\n",
              " 'according',\n",
              " 'herb',\n",
              " 'includes',\n",
              " 'items',\n",
              " 'typical',\n",
              " 'four',\n",
              " 'wanted',\n",
              " 'carbs',\n",
              " 'helps',\n",
              " 'basic',\n",
              " 'discovered',\n",
              " 'friends',\n",
              " 'whether',\n",
              " 'flat',\n",
              " 'inside',\n",
              " 'cardamom',\n",
              " 'extremely',\n",
              " 'nature',\n",
              " 'contents',\n",
              " 'egg',\n",
              " 'sample',\n",
              " 'raisins',\n",
              " 'generic',\n",
              " 'odd',\n",
              " 'k',\n",
              " 'truly',\n",
              " 'longer',\n",
              " 'boost',\n",
              " 'switch',\n",
              " 'choose',\n",
              " 'individual',\n",
              " 'eggs',\n",
              " 'seemed',\n",
              " 'felt',\n",
              " 'lightly',\n",
              " 'nacho',\n",
              " 'gooey',\n",
              " 'honestly',\n",
              " 'difficult',\n",
              " 'portion',\n",
              " 'tell',\n",
              " 'extract',\n",
              " 'nut',\n",
              " 'familiar',\n",
              " 'beef',\n",
              " 'breast',\n",
              " 'squeeze',\n",
              " 'chance',\n",
              " 'bring',\n",
              " 'hold',\n",
              " 'included',\n",
              " 'enjoyable',\n",
              " 'gourmet',\n",
              " 'problem',\n",
              " 'packages',\n",
              " 'nearly',\n",
              " 'set',\n",
              " 'piece',\n",
              " 'foil',\n",
              " 'minute',\n",
              " 'c',\n",
              " 'taco',\n",
              " 'cheezit',\n",
              " 'overwhelming',\n",
              " 'grainy',\n",
              " 'kcup',\n",
              " 'classic',\n",
              " 'hopes',\n",
              " 'fake',\n",
              " 'become',\n",
              " 'hours',\n",
              " 'n',\n",
              " 'awesome',\n",
              " 'unpleasant',\n",
              " 'soups',\n",
              " 'stomach',\n",
              " 'typically',\n",
              " 'popular',\n",
              " 'help',\n",
              " 'stand',\n",
              " 'average',\n",
              " 'child',\n",
              " 'cfh',\n",
              " 'worth',\n",
              " 'order',\n",
              " 'grab',\n",
              " 'appreciate',\n",
              " 'avoid',\n",
              " 'campbells',\n",
              " 'acid',\n",
              " 'jar',\n",
              " 'paired',\n",
              " 'notice',\n",
              " 'broth',\n",
              " 'form',\n",
              " 'person',\n",
              " 'balanced',\n",
              " 'wow',\n",
              " 'wrong',\n",
              " 'offered',\n",
              " 'stronger',\n",
              " 'tender',\n",
              " 'call',\n",
              " 'tap',\n",
              " 'satisfy',\n",
              " 'system',\n",
              " 'pamelas',\n",
              " 'important',\n",
              " 'method',\n",
              " 'higgins',\n",
              " 'awful',\n",
              " 'ground',\n",
              " 'scent',\n",
              " 'shot',\n",
              " 'during',\n",
              " 'opportunity',\n",
              " 'illy',\n",
              " 'themselves',\n",
              " 'lighter',\n",
              " 'numi',\n",
              " 'sea',\n",
              " 'coffees',\n",
              " 'fair',\n",
              " 'wont',\n",
              " 'shapes',\n",
              " 'totally',\n",
              " 'thinner',\n",
              " 'smaller',\n",
              " 'hate',\n",
              " 'versions',\n",
              " 'hibiscus',\n",
              " 'bread',\n",
              " 'five',\n",
              " 'decaf',\n",
              " 'mushrooms',\n",
              " 'squirt',\n",
              " 'bake',\n",
              " 'possible',\n",
              " 'wild',\n",
              " 'pads',\n",
              " 'pronounced',\n",
              " 'pilaf',\n",
              " 'meals',\n",
              " 'date',\n",
              " 'son',\n",
              " 'dente',\n",
              " 'world',\n",
              " 'filter',\n",
              " 'means',\n",
              " ...]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix.index_to_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6371, 300)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# vocab_size is equal to full_embedding_matrix shape[0]\n",
        "special_token_embedding = np.random.rand(4,embedding_matrix.vector_size)\n",
        "full_embedding_matrix = np.concatenate((special_token_embedding,embedding_matrix.vectors))\n",
        "full_embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example\n",
        "# embedding_matrix['keep']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVT3Ur54LsIz"
      },
      "source": [
        "# **Text Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of batches in train data 107\n",
            "number of batches in test data 23\n"
          ]
        }
      ],
      "source": [
        "# hyperparameter\n",
        "\n",
        "batch_size = 16 # for seq2seq model\n",
        "batch_size_test = 8 # for seq2seq model\n",
        "num_batches=int(train_df.shape[0]/batch_size)\n",
        "num_batches_test=int(test_df.shape[0]/batch_size_test)\n",
        "print(\"number of batches in train data\",num_batches)\n",
        "print(\"number of batches in test data\",num_batches_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct_enc(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "\n",
        "def tf_lower_and_split_punct_decin(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text], separator=' ') \n",
        "  return text\n",
        "\n",
        "\n",
        "def tf_lower_and_split_punct_decout(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,Â¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,Â¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join([ text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QVJNF8EGGe"
      },
      "source": [
        "- The conversion of tokens to ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_input_processor = tf.keras.layers.TextVectorization( max_tokens = max_vocab_size,\n",
        "                                                     standardize = tf_lower_and_split_punct_enc,\n",
        "                                                     output_sequence_length = sequence_length,  )\n",
        "dec_input_processor = tf.keras.layers.TextVectorization( max_tokens = max_vocab_size,\n",
        "                                                     standardize = tf_lower_and_split_punct_decin,\n",
        "                                                     output_sequence_length = sequence_length,  )\n",
        "\n",
        "dec_output_processor = tf.keras.layers.TextVectorization( max_tokens = max_vocab_size,\n",
        "                                                     standardize = tf_lower_and_split_punct_decout,\n",
        "                                                     output_sequence_length = sequence_length,  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_input_processor.set_vocabulary(['','[UNK]','[START]','[END]'] +  embedding_matrix.index_to_key )\n",
        "dec_input_processor.set_vocabulary(['','[UNK]','[START]','[END]'] +  embedding_matrix.index_to_key )\n",
        "dec_output_processor.set_vocabulary(['','[UNK]','[START]','[END]'] +  embedding_matrix.index_to_key )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "243\n",
            "spectacular\n"
          ]
        }
      ],
      "source": [
        "print(enc_input_processor.get_vocabulary().index('come'))\n",
        "print(enc_input_processor.get_vocabulary()[3351])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_text_processor vocab size : 6371\n",
            "['', '[UNK]', '[START]', '[END]', '.', 'the', ',', 'a', 'and', 'i', 'of', 'to', 'is', 'it', 'this', 'in']\n",
            "Many reviewers have already commented on the fact that this mix is tasty.  I find it difficult to describe, because it is definitely chocolatey, and sweet, but yet it's not quite like eating candy or cookies etc.  It will handle multiple types of snacking cravings whether you want chocolate, candy, sweet in general, crunchy, crispy, or sweet-salty etc.One thing that surprised me was that all of the pieces in the mix, except the candy drops, are basically sized to match the Chex squares, so they are smaller than I had anticipated.  This is a big plus for me, as I find most \"bite-sized\" products to be more like \"3-bite-sized\", and this mix is actually really bite-sized.  This makes it easy to eat and it also is very charming.  I think it would be a good option for children, to whom you could give a small amount in their lunch/bento box or as a snack, and they will feel they are getting a treat without eating too much junk.The mix contains adorable, crispy, and tasty tiny chocolate-coated pretzels.  There are mini chocolate cookies, uncoated and unfilled, which resemble the outer cookie of an Oreo.  The mix contains some M&M; like chocolate candies.  It also contains two different types of Chex pieces.  One is lightly coated in a nice cocoa flavor, the other with a bit thicker, sweeter vanilla coating.  There are comparatively few of the vanilla Chex pieces; they serve just to add counterpoint to the chocolate.This mix is labelled \"dark chocolate\" but it isn't too strong or bitter.  Everything is fresh, crispy/crunchy etc. as it should be.  The flavor intensities of the different types of pieces keeps the mix from coming off like a variant of bridge mix.  It is essentially a simple idea that results in an unusually different snack.The bag seems on the small size, but this mix has some punch to it in terms of snacking satisfaction; I'm finding that one bag gives me about 8 servings.\n",
            "tf.Tensor(\n",
            "[ 182 1352   27 1042 5578   29    5  356   16   14  109   12  127    4\n",
            "    9  227   13  893   11 1057    6   92   13   12  252 1635    6    8\n",
            "   69    6   17  559   37   22  105   23  292  515   36  110 1193    4\n",
            "   13  128 1467 3108  575   10 1326 2001  867   33  233   85    6  515\n",
            "    6   69   15 1008    6  232    6  439], shape=(64,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# included sos, eos, unk and space \n",
        "print(\"input_text_processor vocab size :\" ,len(enc_input_processor.get_vocabulary()))\n",
        "# Here are the first 16 words from the vocabulary:\n",
        "print(enc_input_processor.get_vocabulary()[:16])\n",
        "print(df['reviewText'][0])\n",
        "print(dec_output_processor(  df['reviewText'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **TF-IDF Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the length of tf-idf vocab 6365\n",
            "['', '.', '!', '[END]', '[START]', ',', '?']\n",
            "['!', ',', '.', '?']\n"
          ]
        }
      ],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "\n",
        "  tfidf_calculator = tf.keras.layers.TextVectorization(\n",
        "    max_tokens  = max_vocab_size,\n",
        "    output_mode ='tf-idf',\n",
        "    pad_to_max_tokens = True  \n",
        "    )\n",
        "  tfidf_calculator.adapt(enc_input_processor.get_vocabulary()[:] )\n",
        "  print(\"the length of tf-idf vocab\",len(tfidf_calculator.get_vocabulary()))\n",
        "  print( list(set(enc_input_processor.get_vocabulary())-set(tfidf_calculator.get_vocabulary()))[:9])\n",
        "  print( list(set(embedding_matrix.index_to_key)-set(tfidf_calculator.get_vocabulary()))[:9])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with tf.device('/CPU:0'):\n",
        "\n",
        "#     user_tfidf = list( map(tfidf_calculator, user_corpus))\n",
        "#     item_tfidf = list( map(tfidf_calculator, item_corpus))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Forming Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert string id to int id\n",
        "user_to_row = {}\n",
        "item_to_column = {}\n",
        "\n",
        "for i, user_id in enumerate(np.unique(df['userID'])):\n",
        "    user_to_row[user_id] = i\n",
        "\n",
        "for j, item_id in enumerate(np.unique(df['itemID'].tolist())):\n",
        "    item_to_column[item_id] = j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds_seq = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "       [user_to_row[dp] for dp in train_df['userID']],\n",
        "       [item_to_column[dp] for dp in train_df['itemID']],\n",
        "       tf.cast(dec_input_processor(train_df['reviewText']),dtype=tf.int32),\n",
        "       tf.cast(dec_output_processor(train_df['reviewText']),dtype=tf.int32)       \n",
        "    )\n",
        ").shuffle(131072).batch(batch_size,drop_remainder=True)\n",
        "test_ds_seq = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "      [user_to_row[dp] for dp in test_df['userID']],\n",
        "      [item_to_column[dp] for dp in test_df['itemID']],\n",
        "      tf.cast(dec_input_processor(test_df['reviewText']),dtype=tf.int32),\n",
        "      tf.cast(dec_output_processor(test_df['reviewText']),dtype=tf.int32)         \n",
        "    )\n",
        ").shuffle(16384).batch(batch_size_test,drop_remainder=True)\n",
        "\n",
        "train_ds_pmf = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "       [user_to_row[dp] for dp in train_df['userID']],\n",
        "       [item_to_column[dp] for dp in train_df['itemID']],\n",
        "       tf.cast(train_df['rating'],dtype=tf.int8)\n",
        "    )\n",
        ").shuffle(131072).batch(1024,drop_remainder=True)\n",
        "test_ds_pmf = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "      [user_to_row[dp] for dp in test_df['userID']],\n",
        "      [item_to_column[dp] for dp in test_df['itemID']],\n",
        "      tf.cast(test_df['rating'],dtype=tf.int8)\n",
        "    )\n",
        ").shuffle(16384).batch(1024,drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train and test data number :  1723  ,  191\n"
          ]
        }
      ],
      "source": [
        "train_data_num=train_df.shape[0]\n",
        "test_data_num=test_df.shape[0]\n",
        "print(\"train and test data number : \",train_data_num,\" , \",test_data_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV4PGxjttli6"
      },
      "source": [
        "# **User and Item Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BwcnBKDxVMk"
      },
      "source": [
        "**User Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_doc = list( map(enc_input_processor,[doc[:document_length] for doc in user_df['reviewText']]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDCul-D_N99"
      },
      "source": [
        "**Item Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "phoN10nsXZGc"
      },
      "outputs": [],
      "source": [
        "item_doc = list( map(enc_input_processor,[doc[:document_length] for doc in item_df['reviewText']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1G79SrZySBqJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(item_doc[0])\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(item_doc[0] != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbR2h5ML5h70"
      },
      "source": [
        "### Converting ID to Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"My grandmother always had cashews in pretty glass containers in her living room. She loved cashews and lived to her late 80's eating them. I grew up on them. Now that we know that nuts are a healthy source of protein, they are a fairly non-guilty snack. Planters is a quality brand. This is our usual go-to brand when we want cashews. If you can eat about 1/8 cup at a time, they shouldn't destroy your diet.\""
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_df['reviewText'][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[START] my grandmother always had cashews in pretty glass containers in her living room . she loved cashews and lived to her late s eating them . i grew up on them . now that we know that nuts are a healthy source of protein , they are a fairly nonguilty snack . planters is a quality brand . this is our usual goto'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_vocab = enc_input_processor.get_vocabulary()\n",
        "\" \".join([input_vocab[id] for id in user_doc[0][0]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Context-aware Matrix Factorization for Rating Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "num_users=len(user_to_row)\n",
        "num_items=len(item_to_column)\n",
        "mean_inv = np.float32( train_df['rating'].mean())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_num=128 # number of topics\n",
        "units=int(feature_num/2 )# gru units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# del df\n",
        "# del user_df\n",
        "# del item_df\n",
        "# del user_corpus[:]\n",
        "# del item_corpus[:]\n",
        "# del train_df\n",
        "# del test_df\n",
        "\n",
        "# gc.collect()\n",
        "# gc.collect(0)\n",
        "# gc.collect(1)\n",
        "# gc.collect(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_textual_features = np.zeros(shape=(num_users, feature_num),dtype=np.float32)\n",
        "item_textual_features = np.zeros(shape=(num_items, feature_num),dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_recommender_features = np.zeros(shape=(num_users, feature_num),dtype=np.float64)\n",
        "item_recommender_features = np.zeros(shape=(num_items, feature_num),dtype=np.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uKWBtzhmMA5"
      },
      "source": [
        "## **PMF (Probabilistic Matrix Factorization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5JAoyyjOnJis"
      },
      "outputs": [],
      "source": [
        "class PMF():\n",
        "    def __init__(self, num_feat=16, epsilon=1, _lambda=0.15, momentum=0.8,  batch_size=1024,num_item=9000,num_user=15000,mean_inv=3):\n",
        "        self.num_feat = num_feat  # Number of latent features,\n",
        "        self.epsilon = epsilon  # learning rate,\n",
        "        self._lambda = _lambda  # L2 regularization,\n",
        "        self.momentum = momentum  # momentum of the gradient,\n",
        "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
        "        self.num_item=num_item\n",
        "        self.num_user=num_user\n",
        "        self.V =  0.1 * np.random.randn(self.num_item, self.num_feat).astype(np.float64)  # Item feature vectors\n",
        "        self.U =  0.1 * np.random.randn(self.num_user, self.num_feat).astype(np.float64)  # User feature vectors\n",
        "        self.V_inc = np.zeros((self.num_item, self.num_feat),dtype=np.float64)\n",
        "        self.U_inc = np.zeros((self.num_user, self.num_feat),dtype=np.float64)\n",
        "        self.rmse_train = []\n",
        "        self.rmse_test = []\n",
        "        self.mean_inv= mean_inv  \n",
        "        \n",
        "    # ***Fit the model with train_tuple and evaluate RMSE on both train and test data.  ***********#\n",
        "    # ***************** train_vec=TrainData, test_vec=TestData*************#\n",
        "    def train(self):\n",
        "            \n",
        "            for batch_UserID,batch_ItemID, batch_rating  in train_ds_pmf:\n",
        "                  \n",
        "                # Compute Objective Function             \n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                              self.V[batch_ItemID, :]),\n",
        "                                  axis=1)  # mean_inv subtracted # np.multiply\n",
        "                \n",
        "                rawErr = pred_out - batch_rating.numpy() + self.mean_inv\n",
        "\n",
        "                # Compute gradients\n",
        "                Ix_User = 2 * np.multiply(rawErr[:, np.newaxis], self.V[batch_ItemID, :]) \\\n",
        "                       + self._lambda * (self.U[batch_UserID, :] - user_textual_features[batch_UserID,:] )\n",
        "                Ix_Item = 2 * np.multiply(rawErr[:, np.newaxis], self.U[batch_UserID, :]) \\\n",
        "                       + self._lambda * (self.V[batch_ItemID, :] - item_textual_features[batch_ItemID,:] ) \n",
        "                       # np.newaxis :increase the dimension\n",
        "               \n",
        "                dw_Item = np.zeros((self.num_item, self.num_feat))\n",
        "                dw_User = np.zeros((self.num_user, self.num_feat))\n",
        "                \n",
        "                # loop to aggreate the gradients of the same element\n",
        "                for i in range(self.batch_size):\n",
        "                    dw_Item[batch_ItemID[i], :] += Ix_Item[i, :]\n",
        "                    dw_User[batch_UserID[i], :] += Ix_User[i, :]\n",
        "\n",
        "                # Update with momentum\n",
        "              \n",
        "                self.V_inc = self.momentum * self.V_inc + self.epsilon * dw_Item / self.batch_size\n",
        "                self.U_inc = self.momentum * self.U_inc + self.epsilon * dw_User / self.batch_size\n",
        "                \n",
        "                self.V = self.V - self.V_inc\n",
        "                self.U = self.U - self.U_inc\n",
        "            \n",
        "                # Compute Objective Function after\n",
        "            self.evaluate()\n",
        "            self.update_recommender_features()\n",
        "    \n",
        "    \n",
        "    def evaluate(self):\n",
        "            rawErr=np.zeros((self.batch_size),dtype=np.float64).tolist()\n",
        "            for batch_UserID,batch_ItemID, batch_rating  in train_ds_pmf:\n",
        "                        \n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                        self.V[batch_ItemID, :]),\n",
        "                                axis=1)  # mean_inv subtracted\n",
        "\n",
        "                rawErr += pred_out - batch_rating.numpy() + self.mean_inv\n",
        "        \n",
        "            obj = np.linalg.norm(rawErr) ** 2 \\\n",
        "                    + 0.5 * self._lambda * (np.linalg.norm(self.U - user_textual_features) ** 2 + np.linalg.norm(self.V - item_textual_features) ** 2)\n",
        "\n",
        "            self.rmse_train.append(np.sqrt(obj / train_data_num))\n",
        "\n",
        "           # Compute test error\n",
        "            rawErr=np.zeros((self.batch_size),dtype=np.float64).tolist()\n",
        "            for batch_UserID,batch_ItemID, batch_rating  in test_ds_pmf:\n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                        self.V[batch_ItemID, :]),\n",
        "                                axis=1)  # mean_inv subtracted\n",
        "\n",
        "                rawErr += pred_out - batch_rating.numpy() + self.mean_inv\n",
        "\n",
        "            self.rmse_test.append(np.linalg.norm(rawErr) / np.sqrt(test_data_num))\n",
        "\n",
        "            # Print info\n",
        "            print('\\nTraining RMSE: %f, Test RMSE %f' % (self.rmse_train[-1], self.rmse_test[-1]))\n",
        "\n",
        "\n",
        "    def update_recommender_features(self,):\n",
        "        user_recommender_features = self.U\n",
        "        item_recommender_features = self.V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0y4xKXSRYU-"
      },
      "source": [
        "# **Adversarial Seq2Seq Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# log files for training and test\n",
        "train_log_dir = 'logs/gradient_tape/'  + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_log_dir = 'logs/gradient_tape/'  + '/test'\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "graph_log_dir = 'logs/graph/'  \n",
        "graph_summary_writer = tf.summary.create_file_writer(graph_log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerplexityMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self,name='perplexity',**kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.perplexity=self.add_weight(name='pl',initializer='zeros')\n",
        "\n",
        "    def update_state(self, nll_loss):\n",
        "        self.perplexity= 2 ** nll_loss\n",
        "\n",
        "    def result(self):\n",
        "        return self.perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_plx_gen_tch = PerplexityMetric(name='generator perplexity')  # teacher forcing mode\n",
        "train_plx_gen_plc = PerplexityMetric(name='generator perplexity')  # policy gradient mode\n",
        "train_acc_dis = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
        "\n",
        "test_plx_gen = PerplexityMetric(name='generator perplexity')  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Recurrent Review Generator**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- it consists of two encoders and one decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UVYfYjX9YW"
      },
      "source": [
        "### **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "TwRd0VNHkMf0"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,vocab_size, embedding_dim, enc_units):\n",
        "    super().__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding( self.vocab_size, self.embedding_dim, embeddings_initializer=keras.initializers.Constant(full_embedding_matrix),trainable=False)\n",
        "    self.gru= tf.keras.layers.Bidirectional(tf.keras.layers.GRU(  self.enc_units,return_state=True,  recurrent_initializer='glorot_uniform' ))\n",
        "\n",
        "\n",
        "  def call(self, reviews, state=None):\n",
        "    vectors = self.embedding(reviews)\n",
        "    _,encoder_forward_state,encoder_backward_state  = self.gru(vectors, initial_state=state)\n",
        " \n",
        "    return  tf.concat([ encoder_forward_state, encoder_backward_state],-1)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Decoder (Generator)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "skIgKP4iCABs"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,vocab_size, embedding_dim, dec_units):\n",
        "    super().__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim,embeddings_initializer=keras.initializers.Constant(full_embedding_matrix), trainable=False)\n",
        "    self.gru=tf.keras.layers.GRU( self.dec_units , return_state=True,return_sequences=True)\n",
        "    self.fc = tf.keras.layers.Dense(self.vocab_size, use_bias=False)\n",
        "    self.sf=tf.keras.layers.Activation('softmax')\n",
        "\n",
        "  def call(self, decoder_input, context_vector,state=None):\n",
        "\n",
        "     embedded_input = self.embedding(decoder_input)\n",
        "     joint_input_context = tf.concat([embedded_input, tf.tile(tf.expand_dims(context_vector,1),[1,embedded_input.shape[1],1])],2 )\n",
        "     outputs,dec_state = self.gru(joint_input_context, initial_state=state)\n",
        "     logits = self.fc( outputs)\n",
        "     prob_dist = self.sf(logits)\n",
        "\n",
        "     return prob_dist,dec_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "DyhJk5pGKW_N"
      },
      "outputs": [],
      "source": [
        "class GeneratorLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,vocab_size):\n",
        "    self.name = 'masked_loss'\n",
        "    self.vocab_size=vocab_size\n",
        "    self.scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculating the loss for a batch of reviews.\n",
        "\n",
        "    loss = self.scce(y_true,y_pred)\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss*=  mask\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v_zte_RHR5a"
      },
      "source": [
        "## **Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "84ZgGwB9HXi5"
      },
      "outputs": [],
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__( self, sequence_length, vocab_size, embedding_dim, filter_sizes, num_filters):\n",
        "      super().__init__()\n",
        "      self.sequence_length=sequence_length\n",
        "      self.vocab_size=vocab_size\n",
        "      self.embedding_dim=embedding_dim\n",
        "      self.filter_sizes=filter_sizes\n",
        "      self.num_filters= num_filters\n",
        "      self.dis_dropout_keep_prob = 0.75\n",
        "\n",
        "      self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim,weights=[full_embedding_matrix],name=\"discriminator_embedding\",trainable=False)\n",
        "     \n",
        "      self.conv_unigram= tf.keras.layers.Conv1D(self.num_filters, filter_sizes[0], activation=\"tanh\",name=\"conv_unigram\")\n",
        "      self.conv_bigram= tf.keras.layers.Conv1D(self.num_filters, filter_sizes[1],activation=\"tanh\",name=\"conv_bigram\")\n",
        "      self.conv_trigram= tf.keras.layers.Conv1D(self.num_filters, filter_sizes[2],activation=\"tanh\",name=\"conv_trigram\")\n",
        "      self.conv_fourgram= tf.keras.layers.Conv1D(self.num_filters, filter_sizes[3],activation=\"tanh\",name=\"conv_fourgram\")\n",
        "      self.conv_fivegram= tf.keras.layers.Conv1D(self.num_filters, filter_sizes[4],activation=\"tanh\",name=\"conv_fivegram\")\n",
        "\n",
        "      self.gmp_unigram = tf.keras.layers.GlobalMaxPooling1D(name=\"gmp_unigram\")\n",
        "      self.gmp_bigram = tf.keras.layers.GlobalMaxPooling1D(name=\"gmp_bigram\")\n",
        "      self.gmp_trigram = tf.keras.layers.GlobalMaxPooling1D(name=\"gmp_trigram\")\n",
        "      self.gmp_fourgram = tf.keras.layers.GlobalMaxPooling1D(name=\"gmp_fourgram\")\n",
        "      self.gmp_fivegram = tf.keras.layers.GlobalMaxPooling1D(name=\"gmp_fivegram\")\n",
        "\n",
        "      self.dropout=tf.keras.layers.Dropout(self.dis_dropout_keep_prob,name=\"dropout\")\n",
        "      self.fc=tf.keras.layers.Dense(1,activation=\"sigmoid\",use_bias= True)\n",
        "\n",
        "\n",
        "    def get_reward(self, discriminator_inputs , context_vector,training=False ):\n",
        "      \n",
        "      embedded_input= self.embedding(discriminator_inputs)\n",
        "      joint_input_context =tf.concat([embedded_input, tf.tile(tf.expand_dims(context_vector,1),[1,embedded_input.shape[1],1])],2 )\n",
        "\n",
        "      cv_unigram=self.conv_unigram(joint_input_context)\n",
        "      cv_bigram=self.conv_bigram(joint_input_context)\n",
        "      cv_trigram=self.conv_trigram(joint_input_context)\n",
        "      cv_fourgram=self.conv_fourgram(joint_input_context)\n",
        "      cv_fivegram=self.conv_fivegram(joint_input_context)\n",
        "\n",
        "      gmp_unigram = self.gmp_unigram(cv_unigram)\n",
        "      gmp_bigram = self.gmp_bigram(cv_bigram)\n",
        "      gmp_trigram =  self.gmp_trigram(cv_trigram)\n",
        "      gmp_fourgram = self.gmp_fourgram(cv_fourgram)\n",
        "      gmp_fivegram = self.gmp_fivegram(cv_fivegram)\n",
        "    \n",
        "      gmp_overal = tf.concat([gmp_unigram,gmp_bigram,gmp_trigram,gmp_fourgram,gmp_fivegram],1)\n",
        "      \n",
        "      if training:\n",
        "        dropout = self.dropout(gmp_overal,training=training)\n",
        "        ath= self.fc(dropout)\n",
        "        gc.collect()\n",
        "        return ath[:,0],self.fc.weights[0][:,0],self.fc.bias\n",
        "      else:\n",
        "        ath= self.fc(gmp_overal)\n",
        "        gc.collect()\n",
        "        return ath[:,0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "FCRHY3LgfD2_"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'cross-entropy_loss'\n",
        "    self.bce = tf.keras.losses.BinaryCrossentropy( reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "\n",
        "    loss = self.bce(y_true, y_pred)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "MB7HymOiTfdl"
      },
      "outputs": [],
      "source": [
        "class MaskedLossReward(tf.keras.losses.Loss):\n",
        "  def __init__(self,vocab_size):\n",
        "    self.name = 'masked_loss_reward'\n",
        "    self.vocab_size=vocab_size\n",
        "    self.scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred,reward):\n",
        "    # Calculate the loss for each step.\n",
        "\n",
        "    loss = self.scce(y_true,y_pred)\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss*=  mask\n",
        "    loss *= reward\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Z3NlswFZRbub"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(tf.keras.Model): \n",
        "    def __init__(self,num_topic,num_item ,num_user, units ,embedding_dim,vocab_size ,sequence_length, num_batches,num_batches_test, batch_size, use_tf_function=False):\n",
        "        super().__init__()\n",
        "        self.use_tf_function = use_tf_function\n",
        "        self.num_batches_test = num_batches_test\n",
        "        self.num_batches = num_batches # train batch number\n",
        "        self.batch_size = batch_size\n",
        "        self.num_topic = num_topic\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.units = units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_item = num_item\n",
        "        self.num_user = num_user\n",
        "        self.alpha = 10.0\n",
        "        self.disc_l2_reg_lambda = 0.2\n",
        "        self.user_encoder = Encoder(vocab_size= self.vocab_size,  embedding_dim= self.embedding_dim, enc_units= self.units)\n",
        "        self.item_encoder = Encoder(vocab_size= self.vocab_size,  embedding_dim= self.embedding_dim, enc_units= self.units)\n",
        "        self.decoder = Decoder(vocab_size= self.vocab_size,  embedding_dim= self.embedding_dim, dec_units= 4 * self.units) #generator\n",
        "        self.discriminator = Discriminator(sequence_length= self.sequence_length, vocab_size= self.vocab_size,embedding_dim= self.embedding_dim, filter_sizes= [1, 2, 3 ,4, 5] ,num_filters=128)\n",
        "      \n",
        "        self.loss_gn = GeneratorLoss(vocab_size=self.vocab_size)\n",
        "        self.loss_ds = DiscriminatorLoss()\n",
        "        self.loss_fn = MaskedLossReward(vocab_size=self.vocab_size) # generator loss with reward\n",
        "\n",
        "        self.optimizer_gn = tf.optimizers.Adam(1e-4)\n",
        "        self.optimizer_ds = tf.optimizers.Adam(1e-4)\n",
        "\n",
        "    def generate_textual_features(self):\n",
        "            \n",
        "          for start_index in range(0, self.num_user, self.batch_size):\n",
        "            end_index = min(start_index + self.batch_size, self.num_user)                           \n",
        "            batch_userID = np.arange(start_index, end_index)\n",
        "\n",
        "            # fetch a batch of user doc\n",
        "            batch_userdoc_flattend=[]\n",
        "            userdoc_slice_idx=[0]\n",
        "            for doc_id in batch_userID:\n",
        "              userdoc_slice_idx.append(userdoc_slice_idx[-1] + user_doc[doc_id].shape[0])\n",
        "              batch_userdoc_flattend = np.append(batch_userdoc_flattend,user_doc[doc_id])\n",
        "            batch_userdoc = tf.reshape(tf.convert_to_tensor( batch_userdoc_flattend,dtype=tf.int32),[userdoc_slice_idx[-1],self.sequence_length])\n",
        "\n",
        "            user_enc_state = self.user_encoder(batch_userdoc) # user vector representations      \n",
        "            user_context_vector = tf.stack([ tf.reduce_mean(user_enc_state[ userdoc_slice_idx[sl_num] : userdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(len(batch_userID))])\n",
        "            \n",
        "            user_textual_features[batch_userID]=user_context_vector.numpy()\n",
        "            \n",
        "          for start_index in range(0, self.num_item, self.batch_size):\n",
        "            end_index = min(start_index + self.batch_size, self.num_item)                           \n",
        "            batch_itemID = np.arange(start_index, end_index)\n",
        "\n",
        "            # fetch a batch of item doc\n",
        "            batch_itemdoc_flattend=[]\n",
        "            itemdoc_slice_idx=[0]\n",
        "            for doc_id in batch_itemID:\n",
        "              itemdoc_slice_idx.append(itemdoc_slice_idx[-1] + item_doc[doc_id].shape[0])\n",
        "              batch_itemdoc_flattend = np.append(batch_itemdoc_flattend,item_doc[doc_id])\n",
        "            batch_itemdoc = tf.reshape(tf.convert_to_tensor( batch_itemdoc_flattend,dtype=tf.int32),[itemdoc_slice_idx[-1],self.sequence_length])\n",
        "            \n",
        "            item_enc_state = self.item_encoder(batch_itemdoc) # item vector representations\n",
        "            item_context_vector = tf.stack([ tf.reduce_mean(item_enc_state[ itemdoc_slice_idx[sl_num] : itemdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(len(batch_itemID))])\n",
        "            \n",
        "            item_textual_features[batch_itemID]=item_context_vector.numpy()\n",
        "    \n",
        "    def teacher_forcing_train(self,num_steps=1):\n",
        "          total_gen_loss=0\n",
        "          for t_step in range(num_steps):\n",
        " \n",
        "            (batch_userID,batch_itemID ,batch_review_in,batch_review_out) = next(iter(train_ds_seq))\n",
        "            # fetch a batch of user doc\n",
        "            batch_userdoc_flattend=[]\n",
        "            userdoc_slice_idx=[0]\n",
        "            for doc_id in batch_userID:\n",
        "              userdoc_slice_idx.append(userdoc_slice_idx[-1] + user_doc[doc_id].shape[0])\n",
        "              batch_userdoc_flattend = np.append(batch_userdoc_flattend,user_doc[doc_id])\n",
        "            batch_userdoc = tf.reshape(tf.convert_to_tensor( batch_userdoc_flattend,dtype=tf.int32),[userdoc_slice_idx[-1],self.sequence_length])\n",
        "\n",
        "            # fetch a batch of item doc\n",
        "            batch_itemdoc_flattend=[]\n",
        "            itemdoc_slice_idx=[0]\n",
        "            for doc_id in batch_itemID:\n",
        "              itemdoc_slice_idx.append(itemdoc_slice_idx[-1] + item_doc[doc_id].shape[0])\n",
        "              batch_itemdoc_flattend = np.append(batch_itemdoc_flattend,item_doc[doc_id])\n",
        "            batch_itemdoc = tf.reshape(tf.convert_to_tensor( batch_itemdoc_flattend,dtype=tf.int32),[itemdoc_slice_idx[-1],self.sequence_length])\n",
        "            \n",
        "            with tf.GradientTape() as tape1 , tf.GradientTape() as tape2:\n",
        "              tape1.watch(self.user_encoder.trainable_variables + self.item_encoder.trainable_variables)\n",
        "              tape2.watch(self.decoder.trainable_variables)\n",
        "              \n",
        "              user_enc_state = self.user_encoder(batch_userdoc) # user vector representations      \n",
        "              item_enc_state = self.item_encoder(batch_itemdoc) # item vector representations\n",
        "                \n",
        "              user_context_vector = tf.stack([ tf.reduce_mean(user_enc_state[ userdoc_slice_idx[sl_num] : userdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(self.batch_size)])\n",
        "              item_context_vector = tf.stack([ tf.reduce_mean(item_enc_state[ itemdoc_slice_idx[sl_num] : itemdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(self.batch_size)])\n",
        "              context_vector = tf.concat( [user_context_vector,item_context_vector],1)\n",
        "                           \n",
        "              # regularization \n",
        "              user_regularization_loss= (np.linalg.norm( user_recommender_features[batch_userID.numpy()] - user_context_vector.numpy()) ** 2) /  user_context_vector.shape[0] \n",
        "              item_regularization_loss= (np.linalg.norm( item_recommender_features[batch_itemID.numpy()] - item_context_vector.numpy()) ** 2) /  item_context_vector.shape[0] \n",
        "              \n",
        "              regularization_loss = tf.constant( self.alpha * (user_regularization_loss + item_regularization_loss) )\n",
        "              dec_pred, _ = self.decoder( batch_review_in , context_vector)             \n",
        "              gen_loss = self.loss_gn(batch_review_out , dec_pred)\n",
        "           \n",
        "            grad_enc = tape1.gradient([gen_loss,regularization_loss],self.user_encoder.trainable_variables + self.item_encoder.trainable_variables )\n",
        "            grad_dec = tape2.gradient(gen_loss,self.decoder.trainable_variables )\n",
        "\n",
        "            self.optimizer_gn.apply_gradients(zip(grad_enc+grad_dec, self.user_encoder.trainable_variables + self.item_encoder.trainable_variables+self.decoder.trainable_variables ))                 \n",
        "            \n",
        "            total_gen_loss += gen_loss\n",
        "            # total_regularization_loss += regularization_loss\n",
        "            if t_step % 10 == 0:\n",
        "              print(\"batch number: \",t_step,\"\\tgen loss: \", gen_loss.numpy())\n",
        "    \n",
        "          train_plx_gen_tch.update_state(total_gen_loss / num_steps)\n",
        "\n",
        "    def test(self,num_steps):\n",
        "        total_loss=0\n",
        "        for step in range(num_steps):\n",
        "            (batch_userID,batch_itemID ,_,_) = next(iter(test_ds_seq))\n",
        "            context_vector = tf.concat([user_textual_features[batch_userID.numpy()],item_textual_features[batch_itemID.numpy()]],1)                    \n",
        "            predicted_samples,dec_prob = self.generate_sample(context_vector=context_vector)\n",
        "            loss = self.loss_gn(predicted_samples ,tf.transpose( tf.stack(dec_prob), [1,0,2])  )\n",
        "            total_loss += loss\n",
        "            if step % 4 == 0:\n",
        "              print(\"batch number: \",step, \"\\tgen loss: \",loss.numpy())\n",
        "\n",
        "        test_plx_gen.update_state(total_loss / num_steps)\n",
        "  \n",
        "\n",
        "    \n",
        "    def adversarial_train(self,num_steps=1):\n",
        "        total_gen_loss=0\n",
        "        train_acc_dis.reset_state()\n",
        "        for step in range(num_steps):\n",
        "          # train generator for one step\n",
        "          generated_samples,batch_userID,batch_itemID, gen_loss =  self._train_step_gn_policy() #adversarial samples\n",
        "          total_gen_loss += gen_loss\n",
        "          # train discriminator for one step\n",
        "          disc_loss = self._train_step_disc(generated_samples,batch_userID,batch_itemID)\n",
        "          if step % 5 == 0:\n",
        "            print(\"batch number: \",step, \"\\t\\tgen loss: \",gen_loss.numpy(),\"\\t\\t\\tdisc loss: \",disc_loss.numpy())   \n",
        "        train_plx_gen_plc.update_state(total_gen_loss / num_steps)\n",
        "\n",
        "\n",
        "    def _train_step_gn_policy(self):\n",
        "            batch_userID = np.random.choice(range(0,self.num_user),self.batch_size).astype('int32')\n",
        "            batch_itemID = np.random.choice(range(0,self.num_item),self.batch_size).astype('int32') \n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "              context_vector = tf.concat([user_textual_features[batch_userID],item_textual_features[batch_itemID]],1)                    \n",
        "              generated_samples,dec_prob = self.generate_sample(context_vector=context_vector)            \n",
        "              reward = self.discriminator.get_reward(generated_samples,context_vector)   # batchsize             \n",
        "              g_loss = self.loss_fn(generated_samples , tf.transpose( tf.stack(dec_prob) , [1,0,2]) ,tf.tile( tf.expand_dims(reward,1),[1,self.sequence_length]).numpy() )\n",
        "            \n",
        "            variables =  self.decoder.trainable_variables \n",
        "            gradients = tape.gradient(g_loss, variables)                  \n",
        "            self.optimizer_gn.apply_gradients(zip(gradients, variables))\n",
        "            \n",
        "            return generated_samples,batch_userID,batch_itemID,g_loss\n",
        "       \n",
        "    def _train_step_disc(self,negative_data,negative_user_indices,negative_item_indices):\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "            (positive_user_indices,positive_item_indices,_,positive_data) = next(iter(train_ds_seq)) # ground-truth data\n",
        "\n",
        "            #data shape (batch_size * seq length)\n",
        "            whole_user_indices = tf.concat([positive_user_indices,negative_user_indices],0)\n",
        "            whole_item_indices = tf.concat([positive_item_indices,negative_item_indices],0)\n",
        "\n",
        "            positive_labels = tf.zeros(batch_size, dtype=tf.int32) \n",
        "            negative_labels = tf.ones(batch_size, dtype=tf.int32) \n",
        "            labels = tf.concat([positive_labels, negative_labels], 0)\n",
        "\n",
        "            shuffle_indices = tf.random.shuffle( tf.range(0, len(labels),dtype=tf.int32))\n",
        "            whole_data = tf.gather(tf.concat([positive_data,negative_data],0),shuffle_indices)    \n",
        "            labels = tf.gather(labels,shuffle_indices)\n",
        "            whole_user_indices = tf.gather(whole_user_indices,shuffle_indices)\n",
        "            whole_item_indices = tf.gather(whole_item_indices,shuffle_indices)\n",
        "            query_embedding = tf.concat([tf.gather(user_textual_features , whole_user_indices) ,tf.gather( item_textual_features , whole_item_indices)], 1)\n",
        "            # query shape (batch_size ,2.feature_num)\n",
        "            dis_reward,weights,bias = self.discriminator.get_reward( whole_data,query_embedding,training=True)\n",
        "            l2_loss = tf.nn.l2_loss(2 * weights) + tf.nn.l2_loss(bias)\n",
        "            disc_loss = self.loss_ds(labels,dis_reward) + (self.disc_l2_reg_lambda * l2_loss)\n",
        "         \n",
        "          variables = self.discriminator.trainable_variables \n",
        "          gradients = tape.gradient(disc_loss, variables)\n",
        "          self.optimizer_ds.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "          train_acc_dis.update_state(labels,dis_reward)\n",
        "          return disc_loss\n",
        "\n",
        "\n",
        "    def generate_sample(self, context_vector ):\n",
        "   \n",
        "        dec_out = [tf.convert_to_tensor(context_vector.shape[0] * [[2]])]\n",
        "        dec_prob=[]\n",
        "        dec_state=None    \n",
        "        for _ in range(self.sequence_length):           \n",
        "            d_prob,dec_state = self.decoder( dec_out[-1], context_vector ,dec_state)          \n",
        "            dec_prob.append(d_prob[:,-1])   \n",
        "            dec_out.append(tf.random.categorical(d_prob[:,-1],1,dtype=tf.int32))            \n",
        "            del d_prob          \n",
        "        gc.collect()\n",
        "        \n",
        "        return tf.squeeze(tf.stack(dec_out,1))[:,1:], dec_prob \n",
        "\n",
        "\n",
        "    \n",
        "    def train(self,current_step):\n",
        "        print(\"\\n\\nTeacher Forcing Train:\")\n",
        "        self.teacher_forcing_train(num_steps= 16)  # t step\n",
        "        if current_step > -1:\n",
        "          print(\"\\n\\nAdversarial Train:\")          \n",
        "          self.adversarial_train(num_steps= 4)     # g step\n",
        "        print(\"\\n\\nGenerator Test:\")          \n",
        "        self.test(num_steps= self.num_batches_test)\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3YKdf1-uW8Q"
      },
      "source": [
        "# **Multi-Task Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "khZxlR9oRxVe"
      },
      "outputs": [],
      "source": [
        "class MultiTaskModel(tf.keras.Model):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pmf_model = PMF(num_feat=feature_num,num_item= num_items ,num_user= num_users, mean_inv=mean_inv)\n",
        "        self.seq2seq_model = Seq2Seq(num_topic=feature_num,num_item= num_items ,num_user= num_users,units=units, embedding_dim= embedding_dim,vocab_size=full_embedding_matrix.shape[0],sequence_length=sequence_length, num_batches= num_batches,num_batches_test=num_batches_test,batch_size= batch_size)      \n",
        "     \n",
        "      def train(self,n_epochs):    \n",
        "          ckpt.restore(manager.latest_checkpoint)\n",
        "          if manager.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "          else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "          #tf.profiler.experimental.start('logs')\n",
        "          for epoch in range(n_epochs):\n",
        "              print(\"\\n\\nepoch : \", int(ckpt.step))\n",
        "              self.seq2seq_model.generate_textual_features()\n",
        "\n",
        "              print(\"********************************************* PMF Model Training Turn *********************************************\")\n",
        "              self.pmf_model.train()\n",
        "              print(\"\\n\\n******************************************* Seq2Seq Model Training Turn *******************************************\")                                         \n",
        "              self.seq2seq_model.train(int(ckpt.step))\n",
        "             \n",
        "              with train_summary_writer.as_default():\n",
        "                  tf.summary.scalar('Training perplexity for generator in Teacher Forcing mode', train_plx_gen_tch.result(), step=int(ckpt.step))\n",
        "                  tf.summary.scalar('Training perplexity for generator in Policy Gradient mode', train_plx_gen_plc.result(), step=int(ckpt.step))\n",
        "                  tf.summary.scalar('Training accuracy for discriminator', train_acc_dis.result(), step=int(ckpt.step))\n",
        "                  \n",
        "              with test_summary_writer.as_default():\n",
        "                  tf.summary.scalar('Test perplexity for generator', test_plx_gen.result(), step=int(ckpt.step))\n",
        "  \n",
        "              ckpt.step.assign_add(1)\n",
        "              if int(ckpt.step) % 10 ==0:\n",
        "                save_path = manager.save()\n",
        "                print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.step), save_path))\n",
        "            \n",
        "          #tf.profiler.experimental.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzxSK6IUr6O",
        "outputId": "212b1776-5de7-4896-f09a-4ea54c2405df"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect(0)\n",
        "gc.collect(1)\n",
        "gc.collect(2)\n",
        "\n",
        "# creating an instance of Multi_Task Model\n",
        "mt_model = MultiTaskModel()\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=tf.keras.optimizers.Adam(), net=mt_model)\n",
        "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=6000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restored from ./tf_ckpts/ckpt-2\n",
            "\n",
            "\n",
            "epoch :  20\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:14:14.113540: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:14:14.129681: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.879698, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.360866\n",
            "batch number:  10 \tgen loss:  6.328337\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.3285007 \t\t\tdisc loss:  0.9710573\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.405975\n",
            "batch number:  4 \tgen loss:  9.3888235\n",
            "batch number:  8 \tgen loss:  9.393847\n",
            "batch number:  12 \tgen loss:  9.368016\n",
            "batch number:  16 \tgen loss:  9.419943\n",
            "batch number:  20 \tgen loss:  9.294751\n",
            "\n",
            "\n",
            "epoch :  21\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:14:29.490753: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.902280, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2672744\n",
            "batch number:  10 \tgen loss:  6.471242\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.4511123 \t\t\tdisc loss:  0.9530606\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4292345\n",
            "batch number:  4 \tgen loss:  9.453842\n",
            "batch number:  8 \tgen loss:  9.378931\n",
            "batch number:  12 \tgen loss:  9.41279\n",
            "batch number:  16 \tgen loss:  9.391293\n",
            "batch number:  20 \tgen loss:  9.371494\n",
            "\n",
            "\n",
            "epoch :  22\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.909929, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2773204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:14:44.887255: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.292012\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.0621734 \t\t\tdisc loss:  0.9357164\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.427204\n",
            "batch number:  4 \tgen loss:  9.3754015\n",
            "batch number:  8 \tgen loss:  9.451138\n",
            "batch number:  12 \tgen loss:  9.413824\n",
            "batch number:  16 \tgen loss:  9.416494\n",
            "batch number:  20 \tgen loss:  9.457578\n",
            "\n",
            "\n",
            "epoch :  23\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:15:00.223570: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.930355, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.291342\n",
            "batch number:  10 \tgen loss:  6.2095127\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.4619823 \t\t\tdisc loss:  0.89091146\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.412962\n",
            "batch number:  4 \tgen loss:  9.440087\n",
            "batch number:  8 \tgen loss:  9.389194\n",
            "batch number:  12 \tgen loss:  9.427068\n",
            "batch number:  16 \tgen loss:  9.393644\n",
            "batch number:  20 \tgen loss:  9.466146\n",
            "\n",
            "\n",
            "epoch :  24\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.943105, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4992294\n",
            "batch number:  10 \tgen loss:  6.42889\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.5032268 \t\t\tdisc loss:  0.9169545\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.479604\n",
            "batch number:  4 \tgen loss:  9.400525\n",
            "batch number:  8 \tgen loss:  9.452394\n",
            "batch number:  12 \tgen loss:  9.451698\n",
            "batch number:  16 \tgen loss:  9.419231\n",
            "batch number:  20 \tgen loss:  9.44266\n",
            "\n",
            "\n",
            "epoch :  25\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:15:30.943569: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.951973, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2264814\n",
            "batch number:  10 \tgen loss:  6.383242\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.5708623 \t\t\tdisc loss:  0.93256277\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.431897\n",
            "batch number:  4 \tgen loss:  9.436109\n",
            "batch number:  8 \tgen loss:  9.484731\n",
            "batch number:  12 \tgen loss:  9.404142\n",
            "batch number:  16 \tgen loss:  9.441756\n",
            "batch number:  20 \tgen loss:  9.415806\n",
            "\n",
            "\n",
            "epoch :  26\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.958435, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3579907\n",
            "batch number:  10 \tgen loss:  6.2256546\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.358433 \t\t\tdisc loss:  0.95342034\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.458988\n",
            "batch number:  4 \tgen loss:  9.458517\n",
            "batch number:  8 \tgen loss:  9.461547\n",
            "batch number:  12 \tgen loss:  9.412499\n",
            "batch number:  16 \tgen loss:  9.431196\n",
            "batch number:  20 \tgen loss:  9.434214\n",
            "\n",
            "\n",
            "epoch :  27\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:16:01.033661: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:16:01.051779: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.955109, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.288021\n",
            "batch number:  10 \tgen loss:  6.234245\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.760518 \t\t\tdisc loss:  0.8674736\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.413707\n",
            "batch number:  4 \tgen loss:  9.465059\n",
            "batch number:  8 \tgen loss:  9.536841\n",
            "batch number:  12 \tgen loss:  9.402872\n",
            "batch number:  16 \tgen loss:  9.474394\n",
            "batch number:  20 \tgen loss:  9.482113\n",
            "\n",
            "\n",
            "epoch :  28\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:16:16.474657: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.968050, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.368869\n",
            "batch number:  10 \tgen loss:  6.281488\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.985259 \t\t\tdisc loss:  0.84688175\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.448992\n",
            "batch number:  4 \tgen loss:  9.518636\n",
            "batch number:  8 \tgen loss:  9.538161\n",
            "batch number:  12 \tgen loss:  9.413977\n",
            "batch number:  16 \tgen loss:  9.427868\n",
            "batch number:  20 \tgen loss:  9.470406\n",
            "\n",
            "\n",
            "epoch :  29\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.968445, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.172269\n",
            "batch number:  10 \tgen loss:  6.2636185\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.996863 \t\t\tdisc loss:  0.8653822\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.533355\n",
            "batch number:  4 \tgen loss:  9.47197\n",
            "batch number:  8 \tgen loss:  9.488421\n",
            "batch number:  12 \tgen loss:  9.525424\n",
            "batch number:  16 \tgen loss:  9.525344\n",
            "batch number:  20 \tgen loss:  9.476404\n",
            "Saved checkpoint for epoch 30: ./tf_ckpts/ckpt-3\n",
            "\n",
            "\n",
            "epoch :  30\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.979143, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4780564\n",
            "batch number:  10 \tgen loss:  6.376553\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.786912 \t\t\tdisc loss:  0.8308955\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.507412\n",
            "batch number:  4 \tgen loss:  9.497126\n",
            "batch number:  8 \tgen loss:  9.544379\n",
            "batch number:  12 \tgen loss:  9.473444\n",
            "batch number:  16 \tgen loss:  9.52662\n",
            "batch number:  20 \tgen loss:  9.458827\n",
            "\n",
            "\n",
            "epoch :  31\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.977099, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3652663\n",
            "batch number:  10 \tgen loss:  6.163378\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.24462 \t\t\tdisc loss:  0.90922093\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.468894\n",
            "batch number:  4 \tgen loss:  9.485887\n",
            "batch number:  8 \tgen loss:  9.4784775\n",
            "batch number:  12 \tgen loss:  9.459552\n",
            "batch number:  16 \tgen loss:  9.462172\n",
            "batch number:  20 \tgen loss:  9.43952\n",
            "\n",
            "\n",
            "epoch :  32\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.977138, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.280792\n",
            "batch number:  10 \tgen loss:  6.322904\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.296695 \t\t\tdisc loss:  0.7850187\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.532164\n",
            "batch number:  4 \tgen loss:  9.563207\n",
            "batch number:  8 \tgen loss:  9.515458\n",
            "batch number:  12 \tgen loss:  9.480914\n",
            "batch number:  16 \tgen loss:  9.481066\n",
            "batch number:  20 \tgen loss:  9.484476\n",
            "\n",
            "\n",
            "epoch :  33\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.983960, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.174507\n",
            "batch number:  10 \tgen loss:  6.426097\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.6802645 \t\t\tdisc loss:  0.83121157\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.499347\n",
            "batch number:  4 \tgen loss:  9.4745865\n",
            "batch number:  8 \tgen loss:  9.471861\n",
            "batch number:  12 \tgen loss:  9.533161\n",
            "batch number:  16 \tgen loss:  9.521932\n",
            "batch number:  20 \tgen loss:  9.504496\n",
            "\n",
            "\n",
            "epoch :  34\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:17:44.809483: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:17:44.817403: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.983902, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.257028\n",
            "batch number:  10 \tgen loss:  6.2712984\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.9403896 \t\t\tdisc loss:  0.84020317\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4862385\n",
            "batch number:  4 \tgen loss:  9.480183\n",
            "batch number:  8 \tgen loss:  9.504729\n",
            "batch number:  12 \tgen loss:  9.547108\n",
            "batch number:  16 \tgen loss:  9.523131\n",
            "batch number:  20 \tgen loss:  9.48552\n",
            "\n",
            "\n",
            "epoch :  35\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.990250, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.275521\n",
            "batch number:  10 \tgen loss:  6.299051\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.315178 \t\t\tdisc loss:  0.84547997\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.506734\n",
            "batch number:  4 \tgen loss:  9.5221405\n",
            "batch number:  8 \tgen loss:  9.584455\n",
            "batch number:  12 \tgen loss:  9.539077\n",
            "batch number:  16 \tgen loss:  9.518544\n",
            "batch number:  20 \tgen loss:  9.516895\n",
            "\n",
            "\n",
            "epoch :  36\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.987407, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.25372\n",
            "batch number:  10 \tgen loss:  6.3901353\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.985244 \t\t\tdisc loss:  0.80918676\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.531166\n",
            "batch number:  4 \tgen loss:  9.531609\n",
            "batch number:  8 \tgen loss:  9.502815\n",
            "batch number:  12 \tgen loss:  9.561557\n",
            "batch number:  16 \tgen loss:  9.467166\n",
            "batch number:  20 \tgen loss:  9.511711\n",
            "\n",
            "\n",
            "epoch :  37\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:18:27.791090: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:18:27.799078: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.982626, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3353724\n",
            "batch number:  10 \tgen loss:  6.1861076\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.081358 \t\t\tdisc loss:  0.74921906\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.526111\n",
            "batch number:  4 \tgen loss:  9.461253\n",
            "batch number:  8 \tgen loss:  9.447353\n",
            "batch number:  12 \tgen loss:  9.461469\n",
            "batch number:  16 \tgen loss:  9.4850445\n",
            "batch number:  20 \tgen loss:  9.525976\n",
            "\n",
            "\n",
            "epoch :  38\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:18:42.290918: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.977274, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2903733\n",
            "batch number:  10 \tgen loss:  6.158684\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.9179106 \t\t\tdisc loss:  0.7309096\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.482026\n",
            "batch number:  4 \tgen loss:  9.490681\n",
            "batch number:  8 \tgen loss:  9.477136\n",
            "batch number:  12 \tgen loss:  9.486146\n",
            "batch number:  16 \tgen loss:  9.424507\n",
            "batch number:  20 \tgen loss:  9.548144\n",
            "\n",
            "\n",
            "epoch :  39\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.960566, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.255274\n",
            "batch number:  10 \tgen loss:  6.265963\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.052392 \t\t\tdisc loss:  0.741323\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.395796\n",
            "batch number:  4 \tgen loss:  9.454704\n",
            "batch number:  8 \tgen loss:  9.406236\n",
            "batch number:  12 \tgen loss:  9.457594\n",
            "batch number:  16 \tgen loss:  9.450826\n",
            "batch number:  20 \tgen loss:  9.399049\n",
            "Saved checkpoint for epoch 40: ./tf_ckpts/ckpt-4\n",
            "\n",
            "\n",
            "epoch :  40\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:19:11.255717: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.939267, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2735972\n",
            "batch number:  10 \tgen loss:  6.333999\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.334957 \t\t\tdisc loss:  0.72524786\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.353483\n",
            "batch number:  4 \tgen loss:  9.451221\n",
            "batch number:  8 \tgen loss:  9.455512\n",
            "batch number:  12 \tgen loss:  9.424515\n",
            "batch number:  16 \tgen loss:  9.37211\n",
            "batch number:  20 \tgen loss:  9.352058\n",
            "\n",
            "\n",
            "epoch :  41\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.915777, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.375932\n",
            "batch number:  10 \tgen loss:  6.185857\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.123775 \t\t\tdisc loss:  0.70294446\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.414852\n",
            "batch number:  4 \tgen loss:  9.388667\n",
            "batch number:  8 \tgen loss:  9.465575\n",
            "batch number:  12 \tgen loss:  9.434225\n",
            "batch number:  16 \tgen loss:  9.440228\n",
            "batch number:  20 \tgen loss:  9.387158\n",
            "\n",
            "\n",
            "epoch :  42\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.884875, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3508286\n",
            "batch number:  10 \tgen loss:  6.3430934\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.143127 \t\t\tdisc loss:  0.7131891\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.369181\n",
            "batch number:  4 \tgen loss:  9.420219\n",
            "batch number:  8 \tgen loss:  9.429163\n",
            "batch number:  12 \tgen loss:  9.399792\n",
            "batch number:  16 \tgen loss:  9.426187\n",
            "batch number:  20 \tgen loss:  9.437558\n",
            "\n",
            "\n",
            "epoch :  43\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.836454, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.317088\n",
            "batch number:  10 \tgen loss:  6.257414\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.9547467 \t\t\tdisc loss:  0.71300465\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.409306\n",
            "batch number:  4 \tgen loss:  9.411577\n",
            "batch number:  8 \tgen loss:  9.4417515\n",
            "batch number:  12 \tgen loss:  9.415591\n",
            "batch number:  16 \tgen loss:  9.428249\n",
            "batch number:  20 \tgen loss:  9.409653\n",
            "\n",
            "\n",
            "epoch :  44\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.778789, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3482985\n",
            "batch number:  10 \tgen loss:  6.344352\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.238833 \t\t\tdisc loss:  0.6706146\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.455724\n",
            "batch number:  4 \tgen loss:  9.45693\n",
            "batch number:  8 \tgen loss:  9.406533\n",
            "batch number:  12 \tgen loss:  9.391748\n",
            "batch number:  16 \tgen loss:  9.419362\n",
            "batch number:  20 \tgen loss:  9.45681\n",
            "\n",
            "\n",
            "epoch :  45\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:20:23.143430: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:20:23.155065: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.757339, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2555428\n",
            "batch number:  10 \tgen loss:  6.2157598\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.349657 \t\t\tdisc loss:  0.7012067\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.442803\n",
            "batch number:  4 \tgen loss:  9.393724\n",
            "batch number:  8 \tgen loss:  9.446673\n",
            "batch number:  12 \tgen loss:  9.458471\n",
            "batch number:  16 \tgen loss:  9.453194\n",
            "batch number:  20 \tgen loss:  9.448338\n",
            "\n",
            "\n",
            "epoch :  46\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:20:37.588015: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:20:37.601305: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.749563, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.263477\n",
            "batch number:  10 \tgen loss:  6.2188287\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.359975 \t\t\tdisc loss:  0.6721979\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.486345\n",
            "batch number:  4 \tgen loss:  9.431577\n",
            "batch number:  8 \tgen loss:  9.477226\n",
            "batch number:  12 \tgen loss:  9.434576\n",
            "batch number:  16 \tgen loss:  9.378165\n",
            "batch number:  20 \tgen loss:  9.450827\n",
            "\n",
            "\n",
            "epoch :  47\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.750876, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.296538\n",
            "batch number:  10 \tgen loss:  6.231449\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.160399 \t\t\tdisc loss:  0.67798203\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.409943\n",
            "batch number:  4 \tgen loss:  9.431039\n",
            "batch number:  8 \tgen loss:  9.417964\n",
            "batch number:  12 \tgen loss:  9.486232\n",
            "batch number:  16 \tgen loss:  9.459164\n",
            "batch number:  20 \tgen loss:  9.488112\n",
            "\n",
            "\n",
            "epoch :  48\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.750051, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.221998\n",
            "batch number:  10 \tgen loss:  6.2638054\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.9997845 \t\t\tdisc loss:  0.69779295\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.460114\n",
            "batch number:  4 \tgen loss:  9.467666\n",
            "batch number:  8 \tgen loss:  9.486125\n",
            "batch number:  12 \tgen loss:  9.448105\n",
            "batch number:  16 \tgen loss:  9.506359\n",
            "batch number:  20 \tgen loss:  9.502279\n",
            "\n",
            "\n",
            "epoch :  49\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.759683, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3555546\n",
            "batch number:  10 \tgen loss:  6.2657557\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.37759 \t\t\tdisc loss:  0.6527874\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.492809\n",
            "batch number:  4 \tgen loss:  9.494696\n",
            "batch number:  8 \tgen loss:  9.500497\n",
            "batch number:  12 \tgen loss:  9.506956\n",
            "batch number:  16 \tgen loss:  9.5403805\n",
            "batch number:  20 \tgen loss:  9.409673\n",
            "Saved checkpoint for epoch 50: ./tf_ckpts/ckpt-5\n",
            "\n",
            "\n",
            "epoch :  50\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:21:36.354568: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:21:36.361897: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.767345, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.31042\n",
            "batch number:  10 \tgen loss:  6.258762\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.29008 \t\t\tdisc loss:  0.6803786\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.50951\n",
            "batch number:  4 \tgen loss:  9.501814\n",
            "batch number:  8 \tgen loss:  9.478917\n",
            "batch number:  12 \tgen loss:  9.511037\n",
            "batch number:  16 \tgen loss:  9.559802\n",
            "batch number:  20 \tgen loss:  9.509382\n",
            "\n",
            "\n",
            "epoch :  51\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.779804, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.307356\n",
            "batch number:  10 \tgen loss:  6.2730303\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.558019 \t\t\tdisc loss:  0.6658879\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.505013\n",
            "batch number:  4 \tgen loss:  9.510247\n",
            "batch number:  8 \tgen loss:  9.494251\n",
            "batch number:  12 \tgen loss:  9.507858\n",
            "batch number:  16 \tgen loss:  9.466047\n",
            "batch number:  20 \tgen loss:  9.415269\n",
            "\n",
            "\n",
            "epoch :  52\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.795303, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.280042\n",
            "batch number:  10 \tgen loss:  6.3167067\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.435406 \t\t\tdisc loss:  0.6534263\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.511911\n",
            "batch number:  4 \tgen loss:  9.560001\n",
            "batch number:  8 \tgen loss:  9.524592\n",
            "batch number:  12 \tgen loss:  9.445215\n",
            "batch number:  16 \tgen loss:  9.516968\n",
            "batch number:  20 \tgen loss:  9.489973\n",
            "\n",
            "\n",
            "epoch :  53\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.815231, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2505045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:22:20.298854: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.2993526\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.416847 \t\t\tdisc loss:  0.66666424\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.49981\n",
            "batch number:  4 \tgen loss:  9.527451\n",
            "batch number:  8 \tgen loss:  9.550852\n",
            "batch number:  12 \tgen loss:  9.587485\n",
            "batch number:  16 \tgen loss:  9.498099\n",
            "batch number:  20 \tgen loss:  9.497066\n",
            "\n",
            "\n",
            "epoch :  54\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:22:34.860679: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:22:34.871786: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.834145, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.321064\n",
            "batch number:  10 \tgen loss:  6.235274\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.263021 \t\t\tdisc loss:  0.6523396\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.501143\n",
            "batch number:  4 \tgen loss:  9.539418\n",
            "batch number:  8 \tgen loss:  9.492176\n",
            "batch number:  12 \tgen loss:  9.515113\n",
            "batch number:  16 \tgen loss:  9.541795\n",
            "batch number:  20 \tgen loss:  9.534428\n",
            "\n",
            "\n",
            "epoch :  55\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:22:49.526135: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:22:49.535544: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.845471, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2857566\n",
            "batch number:  10 \tgen loss:  6.2344456\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.003906 \t\t\tdisc loss:  0.6777593\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.497583\n",
            "batch number:  4 \tgen loss:  9.509647\n",
            "batch number:  8 \tgen loss:  9.518266\n",
            "batch number:  12 \tgen loss:  9.488382\n",
            "batch number:  16 \tgen loss:  9.53308\n",
            "batch number:  20 \tgen loss:  9.510151\n",
            "\n",
            "\n",
            "epoch :  56\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.868232, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.200861\n",
            "batch number:  10 \tgen loss:  6.235212\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.551532 \t\t\tdisc loss:  0.6228777\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.556621\n",
            "batch number:  4 \tgen loss:  9.510816\n",
            "batch number:  8 \tgen loss:  9.496233\n",
            "batch number:  12 \tgen loss:  9.530987\n",
            "batch number:  16 \tgen loss:  9.485754\n",
            "batch number:  20 \tgen loss:  9.516988\n",
            "\n",
            "\n",
            "epoch :  57\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:23:18.343387: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:23:18.355469: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.881880, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.156247\n",
            "batch number:  10 \tgen loss:  6.2801967\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.58125 \t\t\tdisc loss:  0.59586704\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.527956\n",
            "batch number:  4 \tgen loss:  9.477583\n",
            "batch number:  8 \tgen loss:  9.540006\n",
            "batch number:  12 \tgen loss:  9.516265\n",
            "batch number:  16 \tgen loss:  9.47908\n",
            "batch number:  20 \tgen loss:  9.549417\n",
            "\n",
            "\n",
            "epoch :  58\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:23:33.117661: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:23:33.126031: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.900991, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3097596\n",
            "batch number:  10 \tgen loss:  6.21376\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.659312 \t\t\tdisc loss:  0.6223152\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.506695\n",
            "batch number:  4 \tgen loss:  9.440325\n",
            "batch number:  8 \tgen loss:  9.489377\n",
            "batch number:  12 \tgen loss:  9.42545\n",
            "batch number:  16 \tgen loss:  9.522827\n",
            "batch number:  20 \tgen loss:  9.527721\n",
            "\n",
            "\n",
            "epoch :  59\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.913384, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3505254\n",
            "batch number:  10 \tgen loss:  6.2882566\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.556246 \t\t\tdisc loss:  0.62936443\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.475537\n",
            "batch number:  4 \tgen loss:  9.506931\n",
            "batch number:  8 \tgen loss:  9.481473\n",
            "batch number:  12 \tgen loss:  9.566893\n",
            "batch number:  16 \tgen loss:  9.47203\n",
            "batch number:  20 \tgen loss:  9.463185\n",
            "Saved checkpoint for epoch 60: ./tf_ckpts/ckpt-6\n",
            "\n",
            "\n",
            "epoch :  60\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.918557, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.399164\n",
            "batch number:  10 \tgen loss:  6.292835\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.19316 \t\t\tdisc loss:  0.6564029\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.445771\n",
            "batch number:  4 \tgen loss:  9.426795\n",
            "batch number:  8 \tgen loss:  9.436463\n",
            "batch number:  12 \tgen loss:  9.489378\n",
            "batch number:  16 \tgen loss:  9.394827\n",
            "batch number:  20 \tgen loss:  9.440111\n",
            "\n",
            "\n",
            "epoch :  61\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.920880, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3386717\n",
            "batch number:  10 \tgen loss:  6.3665314\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.256175 \t\t\tdisc loss:  0.63961846\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4680395\n",
            "batch number:  4 \tgen loss:  9.414929\n",
            "batch number:  8 \tgen loss:  9.412816\n",
            "batch number:  12 \tgen loss:  9.432933\n",
            "batch number:  16 \tgen loss:  9.428392\n",
            "batch number:  20 \tgen loss:  9.411802\n",
            "\n",
            "\n",
            "epoch :  62\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.924159, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3083286\n",
            "batch number:  10 \tgen loss:  6.389433\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.440524 \t\t\tdisc loss:  0.5720558\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.463631\n",
            "batch number:  4 \tgen loss:  9.464317\n",
            "batch number:  8 \tgen loss:  9.44871\n",
            "batch number:  12 \tgen loss:  9.487324\n",
            "batch number:  16 \tgen loss:  9.480911\n",
            "batch number:  20 \tgen loss:  9.386796\n",
            "\n",
            "\n",
            "epoch :  63\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.930280, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2501345\n",
            "batch number:  10 \tgen loss:  6.279764\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.536873 \t\t\tdisc loss:  0.59443426\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.408745\n",
            "batch number:  4 \tgen loss:  9.455682\n",
            "batch number:  8 \tgen loss:  9.439848\n",
            "batch number:  12 \tgen loss:  9.458763\n",
            "batch number:  16 \tgen loss:  9.404615\n",
            "batch number:  20 \tgen loss:  9.416791\n",
            "\n",
            "\n",
            "epoch :  64\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:24:59.911909: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:24:59.922639: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.945131, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.293509\n",
            "batch number:  10 \tgen loss:  6.402426\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.607013 \t\t\tdisc loss:  0.6080944\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.457659\n",
            "batch number:  4 \tgen loss:  9.486336\n",
            "batch number:  8 \tgen loss:  9.4088545\n",
            "batch number:  12 \tgen loss:  9.426001\n",
            "batch number:  16 \tgen loss:  9.470843\n",
            "batch number:  20 \tgen loss:  9.449474\n",
            "\n",
            "\n",
            "epoch :  65\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:25:14.045398: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:25:14.053333: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.962191, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1588798\n",
            "batch number:  10 \tgen loss:  6.2749696\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.576792 \t\t\tdisc loss:  0.58800673\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.363681\n",
            "batch number:  4 \tgen loss:  9.447961\n",
            "batch number:  8 \tgen loss:  9.473111\n",
            "batch number:  12 \tgen loss:  9.449511\n",
            "batch number:  16 \tgen loss:  9.509232\n",
            "batch number:  20 \tgen loss:  9.429121\n",
            "\n",
            "\n",
            "epoch :  66\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:25:28.372676: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.972937, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2924547\n",
            "batch number:  10 \tgen loss:  6.265977\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.577767 \t\t\tdisc loss:  0.5895101\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.452166\n",
            "batch number:  4 \tgen loss:  9.445481\n",
            "batch number:  8 \tgen loss:  9.522498\n",
            "batch number:  12 \tgen loss:  9.430862\n",
            "batch number:  16 \tgen loss:  9.330677\n",
            "batch number:  20 \tgen loss:  9.454057\n",
            "\n",
            "\n",
            "epoch :  67\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:25:42.687434: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:25:42.703165: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.975849, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.200364\n",
            "batch number:  10 \tgen loss:  6.164773\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.76895 \t\t\tdisc loss:  0.62566024\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.451764\n",
            "batch number:  4 \tgen loss:  9.467002\n",
            "batch number:  8 \tgen loss:  9.45961\n",
            "batch number:  12 \tgen loss:  9.433374\n",
            "batch number:  16 \tgen loss:  9.406032\n",
            "batch number:  20 \tgen loss:  9.455607\n",
            "\n",
            "\n",
            "epoch :  68\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.979497, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.278956\n",
            "batch number:  10 \tgen loss:  6.367988\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.795179 \t\t\tdisc loss:  0.5645256\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.422656\n",
            "batch number:  4 \tgen loss:  9.467377\n",
            "batch number:  8 \tgen loss:  9.467212\n",
            "batch number:  12 \tgen loss:  9.456396\n",
            "batch number:  16 \tgen loss:  9.477877\n",
            "batch number:  20 \tgen loss:  9.481682\n",
            "\n",
            "\n",
            "epoch :  69\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:26:11.839362: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.983283, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.303424\n",
            "batch number:  10 \tgen loss:  6.3474326\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.620988 \t\t\tdisc loss:  0.54297507\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.440734\n",
            "batch number:  4 \tgen loss:  9.441041\n",
            "batch number:  8 \tgen loss:  9.434593\n",
            "batch number:  12 \tgen loss:  9.454558\n",
            "batch number:  16 \tgen loss:  9.369092\n",
            "batch number:  20 \tgen loss:  9.438761\n",
            "Saved checkpoint for epoch 70: ./tf_ckpts/ckpt-7\n",
            "\n",
            "\n",
            "epoch :  70\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.982203, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.222388\n",
            "batch number:  10 \tgen loss:  6.235471\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.9346876 \t\t\tdisc loss:  0.5838537\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.467042\n",
            "batch number:  4 \tgen loss:  9.445398\n",
            "batch number:  8 \tgen loss:  9.456842\n",
            "batch number:  12 \tgen loss:  9.469693\n",
            "batch number:  16 \tgen loss:  9.463996\n",
            "batch number:  20 \tgen loss:  9.409047\n",
            "\n",
            "\n",
            "epoch :  71\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:26:41.508695: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:26:41.518996: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.983031, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2454367\n",
            "batch number:  10 \tgen loss:  6.2889204\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.591831 \t\t\tdisc loss:  0.5658639\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.431415\n",
            "batch number:  4 \tgen loss:  9.481212\n",
            "batch number:  8 \tgen loss:  9.462161\n",
            "batch number:  12 \tgen loss:  9.471585\n",
            "batch number:  16 \tgen loss:  9.506378\n",
            "batch number:  20 \tgen loss:  9.440681\n",
            "\n",
            "\n",
            "epoch :  72\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.983399, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4059806\n",
            "batch number:  10 \tgen loss:  6.2368417\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.626501 \t\t\tdisc loss:  0.50082254\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.496214\n",
            "batch number:  4 \tgen loss:  9.505251\n",
            "batch number:  8 \tgen loss:  9.488355\n",
            "batch number:  12 \tgen loss:  9.423125\n",
            "batch number:  16 \tgen loss:  9.466307\n",
            "batch number:  20 \tgen loss:  9.50375\n",
            "\n",
            "\n",
            "epoch :  73\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.986680, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4063706\n",
            "batch number:  10 \tgen loss:  6.2950807\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.7427025 \t\t\tdisc loss:  0.5137102\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.488196\n",
            "batch number:  4 \tgen loss:  9.477686\n",
            "batch number:  8 \tgen loss:  9.510046\n",
            "batch number:  12 \tgen loss:  9.461547\n",
            "batch number:  16 \tgen loss:  9.424092\n",
            "batch number:  20 \tgen loss:  9.4896145\n",
            "\n",
            "\n",
            "epoch :  74\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:27:26.037678: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:27:26.046194: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.990903, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.410139\n",
            "batch number:  10 \tgen loss:  6.308615\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.567253 \t\t\tdisc loss:  0.5316661\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.400464\n",
            "batch number:  4 \tgen loss:  9.483015\n",
            "batch number:  8 \tgen loss:  9.449715\n",
            "batch number:  12 \tgen loss:  9.519094\n",
            "batch number:  16 \tgen loss:  9.5129595\n",
            "batch number:  20 \tgen loss:  9.502208\n",
            "\n",
            "\n",
            "epoch :  75\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.990653, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2138066\n",
            "batch number:  10 \tgen loss:  6.192598\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.604972 \t\t\tdisc loss:  0.5057467\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.446131\n",
            "batch number:  4 \tgen loss:  9.428132\n",
            "batch number:  8 \tgen loss:  9.447499\n",
            "batch number:  12 \tgen loss:  9.447063\n",
            "batch number:  16 \tgen loss:  9.499823\n",
            "batch number:  20 \tgen loss:  9.451086\n",
            "\n",
            "\n",
            "epoch :  76\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.987372, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3525715\n",
            "batch number:  10 \tgen loss:  6.203826\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.542494 \t\t\tdisc loss:  0.51386887\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.485285\n",
            "batch number:  4 \tgen loss:  9.483833\n",
            "batch number:  8 \tgen loss:  9.429984\n",
            "batch number:  12 \tgen loss:  9.472287\n",
            "batch number:  16 \tgen loss:  9.465187\n",
            "batch number:  20 \tgen loss:  9.551315\n",
            "\n",
            "\n",
            "epoch :  77\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:28:10.200930: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:28:10.212517: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.987920, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2346544\n",
            "batch number:  10 \tgen loss:  6.2373934\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.382186 \t\t\tdisc loss:  0.5523944\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.512709\n",
            "batch number:  4 \tgen loss:  9.427268\n",
            "batch number:  8 \tgen loss:  9.462674\n",
            "batch number:  12 \tgen loss:  9.527158\n",
            "batch number:  16 \tgen loss:  9.49682\n",
            "batch number:  20 \tgen loss:  9.439148\n",
            "\n",
            "\n",
            "epoch :  78\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:28:24.887762: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:28:24.895401: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.988153, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3327413\n",
            "batch number:  10 \tgen loss:  6.2316985\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.222429 \t\t\tdisc loss:  0.5180667\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.470422\n",
            "batch number:  4 \tgen loss:  9.466426\n",
            "batch number:  8 \tgen loss:  9.446546\n",
            "batch number:  12 \tgen loss:  9.456944\n",
            "batch number:  16 \tgen loss:  9.416057\n",
            "batch number:  20 \tgen loss:  9.471735\n",
            "\n",
            "\n",
            "epoch :  79\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.994214, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.203011\n",
            "batch number:  10 \tgen loss:  6.3107514\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.02479 \t\t\tdisc loss:  0.5326915\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.50976\n",
            "batch number:  4 \tgen loss:  9.471033\n",
            "batch number:  8 \tgen loss:  9.454252\n",
            "batch number:  12 \tgen loss:  9.415992\n",
            "batch number:  16 \tgen loss:  9.502309\n",
            "batch number:  20 \tgen loss:  9.495803\n",
            "Saved checkpoint for epoch 80: ./tf_ckpts/ckpt-8\n",
            "\n",
            "\n",
            "epoch :  80\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.000857, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.263067\n",
            "batch number:  10 \tgen loss:  6.3026414\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.5684905 \t\t\tdisc loss:  0.6069887\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.571147\n",
            "batch number:  4 \tgen loss:  9.52003\n",
            "batch number:  8 \tgen loss:  9.500654\n",
            "batch number:  12 \tgen loss:  9.535497\n",
            "batch number:  16 \tgen loss:  9.460654\n",
            "batch number:  20 \tgen loss:  9.513362\n",
            "\n",
            "\n",
            "epoch :  81\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:29:08.427795: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.008905, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.194071\n",
            "batch number:  10 \tgen loss:  6.325898\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.49478 \t\t\tdisc loss:  0.525619\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.476411\n",
            "batch number:  4 \tgen loss:  9.427786\n",
            "batch number:  8 \tgen loss:  9.425721\n",
            "batch number:  12 \tgen loss:  9.488537\n",
            "batch number:  16 \tgen loss:  9.461766\n",
            "batch number:  20 \tgen loss:  9.407278\n",
            "\n",
            "\n",
            "epoch :  82\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.011172, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.297803\n",
            "batch number:  10 \tgen loss:  6.262369\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.161779 \t\t\tdisc loss:  0.5567279\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.35219\n",
            "batch number:  4 \tgen loss:  9.424392\n",
            "batch number:  8 \tgen loss:  9.413882\n",
            "batch number:  12 \tgen loss:  9.439848\n",
            "batch number:  16 \tgen loss:  9.431504\n",
            "batch number:  20 \tgen loss:  9.468386\n",
            "\n",
            "\n",
            "epoch :  83\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.013669, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.381057\n",
            "batch number:  10 \tgen loss:  6.305394\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.746083 \t\t\tdisc loss:  0.4882576\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.425556\n",
            "batch number:  4 \tgen loss:  9.394978\n",
            "batch number:  8 \tgen loss:  9.428279\n",
            "batch number:  12 \tgen loss:  9.322532\n",
            "batch number:  16 \tgen loss:  9.453566\n",
            "batch number:  20 \tgen loss:  9.424375\n",
            "\n",
            "\n",
            "epoch :  84\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.010293, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3486524\n",
            "batch number:  10 \tgen loss:  6.3527613\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.370649 \t\t\tdisc loss:  0.5140475\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.402396\n",
            "batch number:  4 \tgen loss:  9.4322405\n",
            "batch number:  8 \tgen loss:  9.393269\n",
            "batch number:  12 \tgen loss:  9.355806\n",
            "batch number:  16 \tgen loss:  9.419431\n",
            "batch number:  20 \tgen loss:  9.400196\n",
            "\n",
            "\n",
            "epoch :  85\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:30:07.317994: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:30:07.326160: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.014789, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.409897\n",
            "batch number:  10 \tgen loss:  6.272279\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.14184 \t\t\tdisc loss:  0.50097716\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.419719\n",
            "batch number:  4 \tgen loss:  9.395065\n",
            "batch number:  8 \tgen loss:  9.372348\n",
            "batch number:  12 \tgen loss:  9.369238\n",
            "batch number:  16 \tgen loss:  9.460016\n",
            "batch number:  20 \tgen loss:  9.415129\n",
            "\n",
            "\n",
            "epoch :  86\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:30:21.759188: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:30:21.777340: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.021699, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2430444\n",
            "batch number:  10 \tgen loss:  6.2790103\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.583544 \t\t\tdisc loss:  0.497538\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.41804\n",
            "batch number:  4 \tgen loss:  9.403562\n",
            "batch number:  8 \tgen loss:  9.470395\n",
            "batch number:  12 \tgen loss:  9.42445\n",
            "batch number:  16 \tgen loss:  9.441184\n",
            "batch number:  20 \tgen loss:  9.405878\n",
            "\n",
            "\n",
            "epoch :  87\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.020789, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2926154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:30:36.402218: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.2479377\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.998566 \t\t\tdisc loss:  0.4706154\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.446037\n",
            "batch number:  4 \tgen loss:  9.4157\n",
            "batch number:  8 \tgen loss:  9.341268\n",
            "batch number:  12 \tgen loss:  9.483141\n",
            "batch number:  16 \tgen loss:  9.425275\n",
            "batch number:  20 \tgen loss:  9.470436\n",
            "\n",
            "\n",
            "epoch :  88\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:30:50.821842: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:30:50.830297: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.017957, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3675017\n",
            "batch number:  10 \tgen loss:  6.255624\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.091568 \t\t\tdisc loss:  0.45616212\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.495984\n",
            "batch number:  4 \tgen loss:  9.449945\n",
            "batch number:  8 \tgen loss:  9.401454\n",
            "batch number:  12 \tgen loss:  9.400617\n",
            "batch number:  16 \tgen loss:  9.415968\n",
            "batch number:  20 \tgen loss:  9.451801\n",
            "\n",
            "\n",
            "epoch :  89\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.013823, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.199736\n",
            "batch number:  10 \tgen loss:  6.23098\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.12635 \t\t\tdisc loss:  0.44998932\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.446436\n",
            "batch number:  4 \tgen loss:  9.454606\n",
            "batch number:  8 \tgen loss:  9.437325\n",
            "batch number:  12 \tgen loss:  9.376382\n",
            "batch number:  16 \tgen loss:  9.39512\n",
            "batch number:  20 \tgen loss:  9.464931\n",
            "Saved checkpoint for epoch 90: ./tf_ckpts/ckpt-9\n",
            "\n",
            "\n",
            "epoch :  90\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:31:19.904212: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:31:19.911828: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.001113, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.22277\n",
            "batch number:  10 \tgen loss:  6.32241\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.651221 \t\t\tdisc loss:  0.4904448\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.431215\n",
            "batch number:  4 \tgen loss:  9.386041\n",
            "batch number:  8 \tgen loss:  9.379879\n",
            "batch number:  12 \tgen loss:  9.338985\n",
            "batch number:  16 \tgen loss:  9.391188\n",
            "batch number:  20 \tgen loss:  9.387974\n",
            "\n",
            "\n",
            "epoch :  91\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.971736, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.402909\n",
            "batch number:  10 \tgen loss:  6.3832254\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.516708 \t\t\tdisc loss:  0.46524388\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.380566\n",
            "batch number:  4 \tgen loss:  9.329587\n",
            "batch number:  8 \tgen loss:  9.338967\n",
            "batch number:  12 \tgen loss:  9.340466\n",
            "batch number:  16 \tgen loss:  9.318155\n",
            "batch number:  20 \tgen loss:  9.38657\n",
            "\n",
            "\n",
            "epoch :  92\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.933483, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.365054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:31:49.553554: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.387428\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.628934 \t\t\tdisc loss:  0.44556236\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.284229\n",
            "batch number:  4 \tgen loss:  9.308734\n",
            "batch number:  8 \tgen loss:  9.318947\n",
            "batch number:  12 \tgen loss:  9.270627\n",
            "batch number:  16 \tgen loss:  9.327231\n",
            "batch number:  20 \tgen loss:  9.279209\n",
            "\n",
            "\n",
            "epoch :  93\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:32:04.166355: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:32:04.179057: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.886493, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3617578\n",
            "batch number:  10 \tgen loss:  6.3728943\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.392929 \t\t\tdisc loss:  0.4344619\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.333126\n",
            "batch number:  4 \tgen loss:  9.319619\n",
            "batch number:  8 \tgen loss:  9.264532\n",
            "batch number:  12 \tgen loss:  9.244478\n",
            "batch number:  16 \tgen loss:  9.273179\n",
            "batch number:  20 \tgen loss:  9.292151\n",
            "\n",
            "\n",
            "epoch :  94\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.848121, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2860203\n",
            "batch number:  10 \tgen loss:  6.3950644\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.0641985 \t\t\tdisc loss:  0.47595462\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.302034\n",
            "batch number:  4 \tgen loss:  9.3286\n",
            "batch number:  8 \tgen loss:  9.312231\n",
            "batch number:  12 \tgen loss:  9.311293\n",
            "batch number:  16 \tgen loss:  9.318429\n",
            "batch number:  20 \tgen loss:  9.296461\n",
            "\n",
            "\n",
            "epoch :  95\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:32:33.553983: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:32:33.571980: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.788092, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4469385\n",
            "batch number:  10 \tgen loss:  6.2137933\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.399881 \t\t\tdisc loss:  0.46648917\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.408937\n",
            "batch number:  4 \tgen loss:  9.390892\n",
            "batch number:  8 \tgen loss:  9.361823\n",
            "batch number:  12 \tgen loss:  9.372538\n",
            "batch number:  16 \tgen loss:  9.373976\n",
            "batch number:  20 \tgen loss:  9.397549\n",
            "\n",
            "\n",
            "epoch :  96\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:32:47.917407: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:32:47.925386: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.720933, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.318069\n",
            "batch number:  10 \tgen loss:  6.1998434\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.141964 \t\t\tdisc loss:  0.46950668\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.493931\n",
            "batch number:  4 \tgen loss:  9.578875\n",
            "batch number:  8 \tgen loss:  9.556373\n",
            "batch number:  12 \tgen loss:  9.549131\n",
            "batch number:  16 \tgen loss:  9.471521\n",
            "batch number:  20 \tgen loss:  9.540007\n",
            "\n",
            "\n",
            "epoch :  97\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.725520, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.272801\n",
            "batch number:  10 \tgen loss:  6.2823067\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.191389 \t\t\tdisc loss:  0.45910943\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.618784\n",
            "batch number:  4 \tgen loss:  9.601132\n",
            "batch number:  8 \tgen loss:  9.612235\n",
            "batch number:  12 \tgen loss:  9.560542\n",
            "batch number:  16 \tgen loss:  9.582107\n",
            "batch number:  20 \tgen loss:  9.5787735\n",
            "\n",
            "\n",
            "epoch :  98\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.757899, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.327911\n",
            "batch number:  10 \tgen loss:  6.2493634\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.241398 \t\t\tdisc loss:  0.45268995\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.632076\n",
            "batch number:  4 \tgen loss:  9.653318\n",
            "batch number:  8 \tgen loss:  9.666141\n",
            "batch number:  12 \tgen loss:  9.61815\n",
            "batch number:  16 \tgen loss:  9.597919\n",
            "batch number:  20 \tgen loss:  9.688124\n",
            "\n",
            "\n",
            "epoch :  99\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.787889, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.259592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:33:32.298883: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.182338\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.667182 \t\t\tdisc loss:  0.42955902\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.592017\n",
            "batch number:  4 \tgen loss:  9.592073\n",
            "batch number:  8 \tgen loss:  9.607881\n",
            "batch number:  12 \tgen loss:  9.55561\n",
            "batch number:  16 \tgen loss:  9.652433\n",
            "batch number:  20 \tgen loss:  9.60811\n",
            "Saved checkpoint for epoch 100: ./tf_ckpts/ckpt-10\n",
            "\n",
            "\n",
            "epoch :  100\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.816107, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.21181\n",
            "batch number:  10 \tgen loss:  6.2909107\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.41219 \t\t\tdisc loss:  0.42663234\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.576589\n",
            "batch number:  4 \tgen loss:  9.59134\n",
            "batch number:  8 \tgen loss:  9.653215\n",
            "batch number:  12 \tgen loss:  9.635906\n",
            "batch number:  16 \tgen loss:  9.662787\n",
            "batch number:  20 \tgen loss:  9.598158\n",
            "\n",
            "\n",
            "epoch :  101\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.852986, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.260558\n",
            "batch number:  10 \tgen loss:  6.2849545\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.757256 \t\t\tdisc loss:  0.41856942\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.640263\n",
            "batch number:  4 \tgen loss:  9.609878\n",
            "batch number:  8 \tgen loss:  9.5866585\n",
            "batch number:  12 \tgen loss:  9.610755\n",
            "batch number:  16 \tgen loss:  9.601713\n",
            "batch number:  20 \tgen loss:  9.605483\n",
            "\n",
            "\n",
            "epoch :  102\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:34:16.576767: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:34:16.595025: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.899466, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2826514\n",
            "batch number:  10 \tgen loss:  6.4067693\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.785156 \t\t\tdisc loss:  0.42076287\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.615438\n",
            "batch number:  4 \tgen loss:  9.635215\n",
            "batch number:  8 \tgen loss:  9.5167675\n",
            "batch number:  12 \tgen loss:  9.608967\n",
            "batch number:  16 \tgen loss:  9.49418\n",
            "batch number:  20 \tgen loss:  9.647607\n",
            "\n",
            "\n",
            "epoch :  103\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:34:31.233950: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:34:31.246969: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.941020, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2097154\n",
            "batch number:  10 \tgen loss:  6.201799\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.195416 \t\t\tdisc loss:  0.403037\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.583662\n",
            "batch number:  4 \tgen loss:  9.502528\n",
            "batch number:  8 \tgen loss:  9.559544\n",
            "batch number:  12 \tgen loss:  9.560331\n",
            "batch number:  16 \tgen loss:  9.582271\n",
            "batch number:  20 \tgen loss:  9.552594\n",
            "\n",
            "\n",
            "epoch :  104\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:34:45.466356: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.975041, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.281949\n",
            "batch number:  10 \tgen loss:  6.2019215\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.966024 \t\t\tdisc loss:  0.3890085\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.638453\n",
            "batch number:  4 \tgen loss:  9.600098\n",
            "batch number:  8 \tgen loss:  9.555592\n",
            "batch number:  12 \tgen loss:  9.575353\n",
            "batch number:  16 \tgen loss:  9.557265\n",
            "batch number:  20 \tgen loss:  9.545333\n",
            "\n",
            "\n",
            "epoch :  105\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.001662, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1936727\n",
            "batch number:  10 \tgen loss:  6.2785\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.953154 \t\t\tdisc loss:  0.40113705\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.621152\n",
            "batch number:  4 \tgen loss:  9.562189\n",
            "batch number:  8 \tgen loss:  9.5556555\n",
            "batch number:  12 \tgen loss:  9.571123\n",
            "batch number:  16 \tgen loss:  9.499438\n",
            "batch number:  20 \tgen loss:  9.6173\n",
            "\n",
            "\n",
            "epoch :  106\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.018310, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1977406\n",
            "batch number:  10 \tgen loss:  6.310592\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.988095 \t\t\tdisc loss:  0.3918507\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.499756\n",
            "batch number:  4 \tgen loss:  9.559696\n",
            "batch number:  8 \tgen loss:  9.588763\n",
            "batch number:  12 \tgen loss:  9.533533\n",
            "batch number:  16 \tgen loss:  9.564246\n",
            "batch number:  20 \tgen loss:  9.562036\n",
            "\n",
            "\n",
            "epoch :  107\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:35:29.331597: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.026489, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2453604\n",
            "batch number:  10 \tgen loss:  6.1776986\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.713419 \t\t\tdisc loss:  0.41944334\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.505574\n",
            "batch number:  4 \tgen loss:  9.601033\n",
            "batch number:  8 \tgen loss:  9.5155945\n",
            "batch number:  12 \tgen loss:  9.538986\n",
            "batch number:  16 \tgen loss:  9.5623455\n",
            "batch number:  20 \tgen loss:  9.553476\n",
            "\n",
            "\n",
            "epoch :  108\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.036059, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.0688424\n",
            "batch number:  10 \tgen loss:  6.2782764\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.851363 \t\t\tdisc loss:  0.40107054\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.587117\n",
            "batch number:  4 \tgen loss:  9.545224\n",
            "batch number:  8 \tgen loss:  9.5318\n",
            "batch number:  12 \tgen loss:  9.517746\n",
            "batch number:  16 \tgen loss:  9.528349\n",
            "batch number:  20 \tgen loss:  9.532102\n",
            "\n",
            "\n",
            "epoch :  109\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.040734, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.172033\n",
            "batch number:  10 \tgen loss:  6.321058\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.018399 \t\t\tdisc loss:  0.38845715\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.570726\n",
            "batch number:  4 \tgen loss:  9.510772\n",
            "batch number:  8 \tgen loss:  9.536018\n",
            "batch number:  12 \tgen loss:  9.530435\n",
            "batch number:  16 \tgen loss:  9.5686655\n",
            "batch number:  20 \tgen loss:  9.547266\n",
            "Saved checkpoint for epoch 110: ./tf_ckpts/ckpt-11\n",
            "\n",
            "\n",
            "epoch :  110\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.050222, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.195208\n",
            "batch number:  10 \tgen loss:  6.31471\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.971713 \t\t\tdisc loss:  0.38988218\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.569504\n",
            "batch number:  4 \tgen loss:  9.58461\n",
            "batch number:  8 \tgen loss:  9.492195\n",
            "batch number:  12 \tgen loss:  9.584638\n",
            "batch number:  16 \tgen loss:  9.528562\n",
            "batch number:  20 \tgen loss:  9.562366\n",
            "\n",
            "\n",
            "epoch :  111\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.057209, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3025103\n",
            "batch number:  10 \tgen loss:  6.3436317\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.9769 \t\t\tdisc loss:  0.3968926\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.605379\n",
            "batch number:  4 \tgen loss:  9.638772\n",
            "batch number:  8 \tgen loss:  9.5151615\n",
            "batch number:  12 \tgen loss:  9.548372\n",
            "batch number:  16 \tgen loss:  9.526796\n",
            "batch number:  20 \tgen loss:  9.58774\n",
            "\n",
            "\n",
            "epoch :  112\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.064320, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2579637\n",
            "batch number:  10 \tgen loss:  6.0759897\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.5027275 \t\t\tdisc loss:  0.3946817\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.534978\n",
            "batch number:  4 \tgen loss:  9.528199\n",
            "batch number:  8 \tgen loss:  9.543674\n",
            "batch number:  12 \tgen loss:  9.5682125\n",
            "batch number:  16 \tgen loss:  9.551404\n",
            "batch number:  20 \tgen loss:  9.595804\n",
            "\n",
            "\n",
            "epoch :  113\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.073101, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.296938\n",
            "batch number:  10 \tgen loss:  6.2115993\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.848114 \t\t\tdisc loss:  0.37315458\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.52248\n",
            "batch number:  4 \tgen loss:  9.5639515\n",
            "batch number:  8 \tgen loss:  9.546261\n",
            "batch number:  12 \tgen loss:  9.496225\n",
            "batch number:  16 \tgen loss:  9.479453\n",
            "batch number:  20 \tgen loss:  9.580857\n",
            "\n",
            "\n",
            "epoch :  114\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.068779, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1841955\n",
            "batch number:  10 \tgen loss:  6.3175244\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.771637 \t\t\tdisc loss:  0.363584\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.545496\n",
            "batch number:  4 \tgen loss:  9.5107155\n",
            "batch number:  8 \tgen loss:  9.537705\n",
            "batch number:  12 \tgen loss:  9.545567\n",
            "batch number:  16 \tgen loss:  9.532611\n",
            "batch number:  20 \tgen loss:  9.47901\n",
            "\n",
            "\n",
            "epoch :  115\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:37:25.601709: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:37:25.610566: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 1.032253, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.207381\n",
            "batch number:  10 \tgen loss:  6.222763\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.996907 \t\t\tdisc loss:  0.37134174\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.495286\n",
            "batch number:  4 \tgen loss:  9.549421\n",
            "batch number:  8 \tgen loss:  9.452527\n",
            "batch number:  12 \tgen loss:  9.551113\n",
            "batch number:  16 \tgen loss:  9.527258\n",
            "batch number:  20 \tgen loss:  9.48212\n",
            "\n",
            "\n",
            "epoch :  116\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:37:40.202506: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:37:40.210045: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.923674, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.300635\n",
            "batch number:  10 \tgen loss:  6.178267\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.836181 \t\t\tdisc loss:  0.35763836\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.509889\n",
            "batch number:  4 \tgen loss:  9.533127\n",
            "batch number:  8 \tgen loss:  9.5872135\n",
            "batch number:  12 \tgen loss:  9.508574\n",
            "batch number:  16 \tgen loss:  9.487192\n",
            "batch number:  20 \tgen loss:  9.53423\n",
            "\n",
            "\n",
            "epoch :  117\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.888965, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1526823\n",
            "batch number:  10 \tgen loss:  6.20942\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.723783 \t\t\tdisc loss:  0.36823282\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.508828\n",
            "batch number:  4 \tgen loss:  9.524273\n",
            "batch number:  8 \tgen loss:  9.52765\n",
            "batch number:  12 \tgen loss:  9.525857\n",
            "batch number:  16 \tgen loss:  9.550693\n",
            "batch number:  20 \tgen loss:  9.523677\n",
            "\n",
            "\n",
            "epoch :  118\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.890101, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1609807\n",
            "batch number:  10 \tgen loss:  6.238564\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.825997 \t\t\tdisc loss:  0.37149906\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.505146\n",
            "batch number:  4 \tgen loss:  9.517418\n",
            "batch number:  8 \tgen loss:  9.524662\n",
            "batch number:  12 \tgen loss:  9.467323\n",
            "batch number:  16 \tgen loss:  9.521062\n",
            "batch number:  20 \tgen loss:  9.525289\n",
            "\n",
            "\n",
            "epoch :  119\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.894077, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.222089\n",
            "batch number:  10 \tgen loss:  6.2107344\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.952838 \t\t\tdisc loss:  0.3698579\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.437977\n",
            "batch number:  4 \tgen loss:  9.527652\n",
            "batch number:  8 \tgen loss:  9.546095\n",
            "batch number:  12 \tgen loss:  9.5405\n",
            "batch number:  16 \tgen loss:  9.544857\n",
            "batch number:  20 \tgen loss:  9.551832\n",
            "Saved checkpoint for epoch 120: ./tf_ckpts/ckpt-12\n",
            "\n",
            "\n",
            "epoch :  120\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:38:37.878547: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:38:37.885891: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.899311, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3359385\n",
            "batch number:  10 \tgen loss:  6.254144\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.901127 \t\t\tdisc loss:  0.35051674\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.452821\n",
            "batch number:  4 \tgen loss:  9.579782\n",
            "batch number:  8 \tgen loss:  9.437457\n",
            "batch number:  12 \tgen loss:  9.473377\n",
            "batch number:  16 \tgen loss:  9.531166\n",
            "batch number:  20 \tgen loss:  9.4809475\n",
            "\n",
            "\n",
            "epoch :  121\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:38:52.611428: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:38:52.617851: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.887494, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.326286\n",
            "batch number:  10 \tgen loss:  6.1835804\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.966514 \t\t\tdisc loss:  0.33545765\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.506221\n",
            "batch number:  4 \tgen loss:  9.557883\n",
            "batch number:  8 \tgen loss:  9.555821\n",
            "batch number:  12 \tgen loss:  9.529601\n",
            "batch number:  16 \tgen loss:  9.470693\n",
            "batch number:  20 \tgen loss:  9.518136\n",
            "\n",
            "\n",
            "epoch :  122\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.881127, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.399763\n",
            "batch number:  10 \tgen loss:  6.2575207\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.696843 \t\t\tdisc loss:  0.37188065\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.497206\n",
            "batch number:  4 \tgen loss:  9.444793\n",
            "batch number:  8 \tgen loss:  9.553344\n",
            "batch number:  12 \tgen loss:  9.501128\n",
            "batch number:  16 \tgen loss:  9.496581\n",
            "batch number:  20 \tgen loss:  9.517675\n",
            "\n",
            "\n",
            "epoch :  123\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:39:23.510829: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:39:23.530026: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.929422, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1926723\n",
            "batch number:  10 \tgen loss:  6.2304063\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.097822 \t\t\tdisc loss:  0.38207424\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.575501\n",
            "batch number:  4 \tgen loss:  9.535783\n",
            "batch number:  8 \tgen loss:  9.566044\n",
            "batch number:  12 \tgen loss:  9.554401\n",
            "batch number:  16 \tgen loss:  9.604303\n",
            "batch number:  20 \tgen loss:  9.566909\n",
            "\n",
            "\n",
            "epoch :  124\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 1.001528, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.254065\n",
            "batch number:  10 \tgen loss:  6.3123617\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  6.147279 \t\t\tdisc loss:  0.5734873\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.570377\n",
            "batch number:  4 \tgen loss:  9.562617\n",
            "batch number:  8 \tgen loss:  9.563903\n",
            "batch number:  12 \tgen loss:  9.51629\n",
            "batch number:  16 \tgen loss:  9.614613\n",
            "batch number:  20 \tgen loss:  9.574488\n",
            "\n",
            "\n",
            "epoch :  125\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.981428, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2141643\n",
            "batch number:  10 \tgen loss:  6.1997833\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.167249 \t\t\tdisc loss:  0.38465416\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.495534\n",
            "batch number:  4 \tgen loss:  9.471187\n",
            "batch number:  8 \tgen loss:  9.493675\n",
            "batch number:  12 \tgen loss:  9.461279\n",
            "batch number:  16 \tgen loss:  9.523986\n",
            "batch number:  20 \tgen loss:  9.492736\n",
            "\n",
            "\n",
            "epoch :  126\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:40:09.328087: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:40:09.340911: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.963582, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2835116\n",
            "batch number:  10 \tgen loss:  6.198002\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.187473 \t\t\tdisc loss:  0.37903452\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.462157\n",
            "batch number:  4 \tgen loss:  9.467846\n",
            "batch number:  8 \tgen loss:  9.516722\n",
            "batch number:  12 \tgen loss:  9.518748\n",
            "batch number:  16 \tgen loss:  9.453275\n",
            "batch number:  20 \tgen loss:  9.481521\n",
            "\n",
            "\n",
            "epoch :  127\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.931249, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.263529\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:40:24.398012: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.414717\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.223301 \t\t\tdisc loss:  0.46515837\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.486922\n",
            "batch number:  4 \tgen loss:  9.517541\n",
            "batch number:  8 \tgen loss:  9.519613\n",
            "batch number:  12 \tgen loss:  9.477763\n",
            "batch number:  16 \tgen loss:  9.469366\n",
            "batch number:  20 \tgen loss:  9.45987\n",
            "\n",
            "\n",
            "epoch :  128\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:40:39.437317: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:40:39.445784: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.939199, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.306304\n",
            "batch number:  10 \tgen loss:  6.2764583\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.549641 \t\t\tdisc loss:  0.35213143\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.491226\n",
            "batch number:  4 \tgen loss:  9.476445\n",
            "batch number:  8 \tgen loss:  9.454258\n",
            "batch number:  12 \tgen loss:  9.443928\n",
            "batch number:  16 \tgen loss:  9.450552\n",
            "batch number:  20 \tgen loss:  9.510895\n",
            "\n",
            "\n",
            "epoch :  129\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:40:54.444895: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.933206, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.360258\n",
            "batch number:  10 \tgen loss:  6.2912745\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.484711 \t\t\tdisc loss:  0.35409015\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.474514\n",
            "batch number:  4 \tgen loss:  9.42407\n",
            "batch number:  8 \tgen loss:  9.4853735\n",
            "batch number:  12 \tgen loss:  9.427573\n",
            "batch number:  16 \tgen loss:  9.488422\n",
            "batch number:  20 \tgen loss:  9.458347\n",
            "Saved checkpoint for epoch 130: ./tf_ckpts/ckpt-13\n",
            "\n",
            "\n",
            "epoch :  130\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.828240, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.294864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:41:09.536056: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.2819123\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.68354 \t\t\tdisc loss:  0.31717396\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.46444\n",
            "batch number:  4 \tgen loss:  9.469519\n",
            "batch number:  8 \tgen loss:  9.424521\n",
            "batch number:  12 \tgen loss:  9.429929\n",
            "batch number:  16 \tgen loss:  9.470724\n",
            "batch number:  20 \tgen loss:  9.428483\n",
            "\n",
            "\n",
            "epoch :  131\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:41:24.545381: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:41:24.552294: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.861108, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2946076\n",
            "batch number:  10 \tgen loss:  6.2613025\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.879996 \t\t\tdisc loss:  0.34082562\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.491876\n",
            "batch number:  4 \tgen loss:  9.534224\n",
            "batch number:  8 \tgen loss:  9.467302\n",
            "batch number:  12 \tgen loss:  9.531457\n",
            "batch number:  16 \tgen loss:  9.471823\n",
            "batch number:  20 \tgen loss:  9.471233\n",
            "\n",
            "\n",
            "epoch :  132\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:41:39.749350: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.842650, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3487806\n",
            "batch number:  10 \tgen loss:  6.3383613\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.064034 \t\t\tdisc loss:  0.34494072\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4025345\n",
            "batch number:  4 \tgen loss:  9.46688\n",
            "batch number:  8 \tgen loss:  9.372482\n",
            "batch number:  12 \tgen loss:  9.443602\n",
            "batch number:  16 \tgen loss:  9.478806\n",
            "batch number:  20 \tgen loss:  9.436642\n",
            "\n",
            "\n",
            "epoch :  133\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.845198, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3874626\n",
            "batch number:  10 \tgen loss:  6.3962407\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.398243 \t\t\tdisc loss:  0.34983486\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.417995\n",
            "batch number:  4 \tgen loss:  9.4377365\n",
            "batch number:  8 \tgen loss:  9.456172\n",
            "batch number:  12 \tgen loss:  9.392323\n",
            "batch number:  16 \tgen loss:  9.449003\n",
            "batch number:  20 \tgen loss:  9.382115\n",
            "\n",
            "\n",
            "epoch :  134\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.847934, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.311579\n",
            "batch number:  10 \tgen loss:  6.2742386\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.234822 \t\t\tdisc loss:  0.35842666\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.445037\n",
            "batch number:  4 \tgen loss:  9.376199\n",
            "batch number:  8 \tgen loss:  9.369074\n",
            "batch number:  12 \tgen loss:  9.360094\n",
            "batch number:  16 \tgen loss:  9.383767\n",
            "batch number:  20 \tgen loss:  9.404502\n",
            "\n",
            "\n",
            "epoch :  135\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.828330, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.310942\n",
            "batch number:  10 \tgen loss:  6.2764244\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.521544 \t\t\tdisc loss:  0.34862414\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.393466\n",
            "batch number:  4 \tgen loss:  9.442238\n",
            "batch number:  8 \tgen loss:  9.496293\n",
            "batch number:  12 \tgen loss:  9.404227\n",
            "batch number:  16 \tgen loss:  9.377333\n",
            "batch number:  20 \tgen loss:  9.487189\n",
            "\n",
            "\n",
            "epoch :  136\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.804810, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.262652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:42:40.339592: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.3259563\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.813284 \t\t\tdisc loss:  0.31223512\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.516861\n",
            "batch number:  4 \tgen loss:  9.556463\n",
            "batch number:  8 \tgen loss:  9.529095\n",
            "batch number:  12 \tgen loss:  9.46784\n",
            "batch number:  16 \tgen loss:  9.502696\n",
            "batch number:  20 \tgen loss:  9.5253525\n",
            "\n",
            "\n",
            "epoch :  137\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.803311, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2251124\n",
            "batch number:  10 \tgen loss:  6.2927284\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.980586 \t\t\tdisc loss:  0.3108592\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4456215\n",
            "batch number:  4 \tgen loss:  9.51072\n",
            "batch number:  8 \tgen loss:  9.506277\n",
            "batch number:  12 \tgen loss:  9.542145\n",
            "batch number:  16 \tgen loss:  9.636836\n",
            "batch number:  20 \tgen loss:  9.474742\n",
            "\n",
            "\n",
            "epoch :  138\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:43:10.481123: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:43:10.490059: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.798929, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.25078\n",
            "batch number:  10 \tgen loss:  6.1966834\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.635361 \t\t\tdisc loss:  0.32944205\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.552014\n",
            "batch number:  4 \tgen loss:  9.562555\n",
            "batch number:  8 \tgen loss:  9.5181\n",
            "batch number:  12 \tgen loss:  9.62842\n",
            "batch number:  16 \tgen loss:  9.525817\n",
            "batch number:  20 \tgen loss:  9.5542965\n",
            "\n",
            "\n",
            "epoch :  139\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:43:25.606823: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:43:25.616505: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.815578, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.246578\n",
            "batch number:  10 \tgen loss:  6.223845\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.016926 \t\t\tdisc loss:  0.36380827\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.54965\n",
            "batch number:  4 \tgen loss:  9.557302\n",
            "batch number:  8 \tgen loss:  9.529528\n",
            "batch number:  12 \tgen loss:  9.561998\n",
            "batch number:  16 \tgen loss:  9.61228\n",
            "batch number:  20 \tgen loss:  9.564916\n",
            "Saved checkpoint for epoch 140: ./tf_ckpts/ckpt-14\n",
            "\n",
            "\n",
            "epoch :  140\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.845111, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2418323\n",
            "batch number:  10 \tgen loss:  6.247323\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.005782 \t\t\tdisc loss:  0.33754373\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.511159\n",
            "batch number:  4 \tgen loss:  9.53915\n",
            "batch number:  8 \tgen loss:  9.60971\n",
            "batch number:  12 \tgen loss:  9.586643\n",
            "batch number:  16 \tgen loss:  9.574621\n",
            "batch number:  20 \tgen loss:  9.582072\n",
            "\n",
            "\n",
            "epoch :  141\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.908522, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.242764\n",
            "batch number:  10 \tgen loss:  6.2380548\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.413084 \t\t\tdisc loss:  0.312783\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.601317\n",
            "batch number:  4 \tgen loss:  9.576989\n",
            "batch number:  8 \tgen loss:  9.544524\n",
            "batch number:  12 \tgen loss:  9.584437\n",
            "batch number:  16 \tgen loss:  9.526673\n",
            "batch number:  20 \tgen loss:  9.563764\n",
            "\n",
            "\n",
            "epoch :  142\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:44:11.181647: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:44:11.193453: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.936339, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2795296\n",
            "batch number:  10 \tgen loss:  6.349538\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.952249 \t\t\tdisc loss:  0.3137775\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.579939\n",
            "batch number:  4 \tgen loss:  9.472664\n",
            "batch number:  8 \tgen loss:  9.578099\n",
            "batch number:  12 \tgen loss:  9.581238\n",
            "batch number:  16 \tgen loss:  9.589991\n",
            "batch number:  20 \tgen loss:  9.531316\n",
            "\n",
            "\n",
            "epoch :  143\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:44:26.284716: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:44:26.293177: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.936675, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.243588\n",
            "batch number:  10 \tgen loss:  6.222779\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.319247 \t\t\tdisc loss:  0.29566097\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.604138\n",
            "batch number:  4 \tgen loss:  9.506115\n",
            "batch number:  8 \tgen loss:  9.568877\n",
            "batch number:  12 \tgen loss:  9.557277\n",
            "batch number:  16 \tgen loss:  9.488452\n",
            "batch number:  20 \tgen loss:  9.645866\n",
            "\n",
            "\n",
            "epoch :  144\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.948164, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2104936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:44:41.397709: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.347965\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.481638 \t\t\tdisc loss:  0.4025433\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.649964\n",
            "batch number:  4 \tgen loss:  9.554089\n",
            "batch number:  8 \tgen loss:  9.556599\n",
            "batch number:  12 \tgen loss:  9.559437\n",
            "batch number:  16 \tgen loss:  9.615385\n",
            "batch number:  20 \tgen loss:  9.630057\n",
            "\n",
            "\n",
            "epoch :  145\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:44:56.951846: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:44:56.971032: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.951398, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2272563\n",
            "batch number:  10 \tgen loss:  6.1531515\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.253021 \t\t\tdisc loss:  0.31248605\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.568535\n",
            "batch number:  4 \tgen loss:  9.553394\n",
            "batch number:  8 \tgen loss:  9.554675\n",
            "batch number:  12 \tgen loss:  9.591005\n",
            "batch number:  16 \tgen loss:  9.581864\n",
            "batch number:  20 \tgen loss:  9.563841\n",
            "\n",
            "\n",
            "epoch :  146\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:45:11.972679: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:45:11.984200: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.952748, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.184351\n",
            "batch number:  10 \tgen loss:  6.216586\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.010202 \t\t\tdisc loss:  0.2694642\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.553134\n",
            "batch number:  4 \tgen loss:  9.577995\n",
            "batch number:  8 \tgen loss:  9.594113\n",
            "batch number:  12 \tgen loss:  9.570034\n",
            "batch number:  16 \tgen loss:  9.524572\n",
            "batch number:  20 \tgen loss:  9.583153\n",
            "\n",
            "\n",
            "epoch :  147\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:45:26.999040: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.951517, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.209605\n",
            "batch number:  10 \tgen loss:  6.199247\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.071204 \t\t\tdisc loss:  0.29101607\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.524588\n",
            "batch number:  4 \tgen loss:  9.57549\n",
            "batch number:  8 \tgen loss:  9.5828085\n",
            "batch number:  12 \tgen loss:  9.536595\n",
            "batch number:  16 \tgen loss:  9.533188\n",
            "batch number:  20 \tgen loss:  9.546605\n",
            "\n",
            "\n",
            "epoch :  148\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.947514, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.277369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:45:42.068646: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.3387427\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.226766 \t\t\tdisc loss:  0.28618097\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.572672\n",
            "batch number:  4 \tgen loss:  9.566325\n",
            "batch number:  8 \tgen loss:  9.4504175\n",
            "batch number:  12 \tgen loss:  9.512532\n",
            "batch number:  16 \tgen loss:  9.531649\n",
            "batch number:  20 \tgen loss:  9.4556675\n",
            "\n",
            "\n",
            "epoch :  149\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.950346, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2316923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:45:57.236464: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.266522\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.216324 \t\t\tdisc loss:  0.27502912\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.536068\n",
            "batch number:  4 \tgen loss:  9.53745\n",
            "batch number:  8 \tgen loss:  9.574736\n",
            "batch number:  12 \tgen loss:  9.542586\n",
            "batch number:  16 \tgen loss:  9.538192\n",
            "batch number:  20 \tgen loss:  9.506548\n",
            "Saved checkpoint for epoch 150: ./tf_ckpts/ckpt-15\n",
            "\n",
            "\n",
            "epoch :  150\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:46:12.294074: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:46:12.312130: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.952913, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3457704\n",
            "batch number:  10 \tgen loss:  6.284004\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.943884 \t\t\tdisc loss:  0.28230143\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.533448\n",
            "batch number:  4 \tgen loss:  9.484356\n",
            "batch number:  8 \tgen loss:  9.545281\n",
            "batch number:  12 \tgen loss:  9.515867\n",
            "batch number:  16 \tgen loss:  9.444387\n",
            "batch number:  20 \tgen loss:  9.495968\n",
            "\n",
            "\n",
            "epoch :  151\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:46:27.588690: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.951171, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.148115\n",
            "batch number:  10 \tgen loss:  6.310725\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  5.6614137 \t\t\tdisc loss:  0.51080227\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.566994\n",
            "batch number:  4 \tgen loss:  9.599909\n",
            "batch number:  8 \tgen loss:  9.567475\n",
            "batch number:  12 \tgen loss:  9.561546\n",
            "batch number:  16 \tgen loss:  9.636155\n",
            "batch number:  20 \tgen loss:  9.48933\n",
            "\n",
            "\n",
            "epoch :  152\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.888607, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.297171\n",
            "batch number:  10 \tgen loss:  6.155688\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  5.2257595 \t\t\tdisc loss:  0.6187728\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.587807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:46:48.272235: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  4 \tgen loss:  9.548919\n",
            "batch number:  8 \tgen loss:  9.648973\n",
            "batch number:  12 \tgen loss:  9.67344\n",
            "batch number:  16 \tgen loss:  9.675692\n",
            "batch number:  20 \tgen loss:  9.599016\n",
            "\n",
            "\n",
            "epoch :  153\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.915305, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1784997\n",
            "batch number:  10 \tgen loss:  6.255342\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  6.505511 \t\t\tdisc loss:  0.44931453\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.547318\n",
            "batch number:  4 \tgen loss:  9.589897\n",
            "batch number:  8 \tgen loss:  9.573664\n",
            "batch number:  12 \tgen loss:  9.5708475\n",
            "batch number:  16 \tgen loss:  9.622768\n",
            "batch number:  20 \tgen loss:  9.566636\n",
            "\n",
            "\n",
            "epoch :  154\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:47:11.544406: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:47:11.562182: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.939183, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.192301\n",
            "batch number:  10 \tgen loss:  6.2057376\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.779994 \t\t\tdisc loss:  0.31037068\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.55802\n",
            "batch number:  4 \tgen loss:  9.571643\n",
            "batch number:  8 \tgen loss:  9.589577\n",
            "batch number:  12 \tgen loss:  9.493648\n",
            "batch number:  16 \tgen loss:  9.531287\n",
            "batch number:  20 \tgen loss:  9.576676\n",
            "\n",
            "\n",
            "epoch :  155\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.957854, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.343962\n",
            "batch number:  10 \tgen loss:  6.2739925\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.719571 \t\t\tdisc loss:  0.2796725\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.480928\n",
            "batch number:  4 \tgen loss:  9.585987\n",
            "batch number:  8 \tgen loss:  9.485146\n",
            "batch number:  12 \tgen loss:  9.486369\n",
            "batch number:  16 \tgen loss:  9.542613\n",
            "batch number:  20 \tgen loss:  9.497163\n",
            "\n",
            "\n",
            "epoch :  156\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.966512, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.210064\n",
            "batch number:  10 \tgen loss:  6.2442193\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.496561 \t\t\tdisc loss:  0.31153813\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.529441\n",
            "batch number:  4 \tgen loss:  9.581434\n",
            "batch number:  8 \tgen loss:  9.497911\n",
            "batch number:  12 \tgen loss:  9.566824\n",
            "batch number:  16 \tgen loss:  9.510883\n",
            "batch number:  20 \tgen loss:  9.49487\n",
            "\n",
            "\n",
            "epoch :  157\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.980380, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2773037\n",
            "batch number:  10 \tgen loss:  6.306832\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.243294 \t\t\tdisc loss:  0.29952458\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.559502\n",
            "batch number:  4 \tgen loss:  9.486856\n",
            "batch number:  8 \tgen loss:  9.474293\n",
            "batch number:  12 \tgen loss:  9.503641\n",
            "batch number:  16 \tgen loss:  9.496962\n",
            "batch number:  20 \tgen loss:  9.518733\n",
            "\n",
            "\n",
            "epoch :  158\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.981719, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.331563\n",
            "batch number:  10 \tgen loss:  6.396698\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.7874675 \t\t\tdisc loss:  0.31096134\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.488443\n",
            "batch number:  4 \tgen loss:  9.56422\n",
            "batch number:  8 \tgen loss:  9.498414\n",
            "batch number:  12 \tgen loss:  9.5144825\n",
            "batch number:  16 \tgen loss:  9.518267\n",
            "batch number:  20 \tgen loss:  9.4869375\n",
            "\n",
            "\n",
            "epoch :  159\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.960866, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3735023\n",
            "batch number:  10 \tgen loss:  6.249007\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.208199 \t\t\tdisc loss:  0.33252499\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.5478735\n",
            "batch number:  4 \tgen loss:  9.472085\n",
            "batch number:  8 \tgen loss:  9.470932\n",
            "batch number:  12 \tgen loss:  9.542417\n",
            "batch number:  16 \tgen loss:  9.4832735\n",
            "batch number:  20 \tgen loss:  9.496652\n",
            "Saved checkpoint for epoch 160: ./tf_ckpts/ckpt-16\n",
            "\n",
            "\n",
            "epoch :  160\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:48:41.098504: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:48:41.104857: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.922645, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3382096\n",
            "batch number:  10 \tgen loss:  6.2754974\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.000864 \t\t\tdisc loss:  0.26559013\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.506483\n",
            "batch number:  4 \tgen loss:  9.475388\n",
            "batch number:  8 \tgen loss:  9.4600725\n",
            "batch number:  12 \tgen loss:  9.4433365\n",
            "batch number:  16 \tgen loss:  9.428816\n",
            "batch number:  20 \tgen loss:  9.528242\n",
            "\n",
            "\n",
            "epoch :  161\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.944690, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.254847\n",
            "batch number:  10 \tgen loss:  6.353745\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.367075 \t\t\tdisc loss:  0.3185356\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.542133\n",
            "batch number:  4 \tgen loss:  9.53111\n",
            "batch number:  8 \tgen loss:  9.560557\n",
            "batch number:  12 \tgen loss:  9.542763\n",
            "batch number:  16 \tgen loss:  9.473123\n",
            "batch number:  20 \tgen loss:  9.55487\n",
            "\n",
            "\n",
            "epoch :  162\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.916125, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2808084\n",
            "batch number:  10 \tgen loss:  6.265187\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.1214 \t\t\tdisc loss:  0.2972143\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.472942\n",
            "batch number:  4 \tgen loss:  9.475403\n",
            "batch number:  8 \tgen loss:  9.527024\n",
            "batch number:  12 \tgen loss:  9.530594\n",
            "batch number:  16 \tgen loss:  9.536591\n",
            "batch number:  20 \tgen loss:  9.481255\n",
            "\n",
            "\n",
            "epoch :  163\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.923798, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.247331\n",
            "batch number:  10 \tgen loss:  6.354002\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.623435 \t\t\tdisc loss:  0.2785542\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.468286\n",
            "batch number:  4 \tgen loss:  9.531973\n",
            "batch number:  8 \tgen loss:  9.513817\n",
            "batch number:  12 \tgen loss:  9.518747\n",
            "batch number:  16 \tgen loss:  9.54743\n",
            "batch number:  20 \tgen loss:  9.509232\n",
            "\n",
            "\n",
            "epoch :  164\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:49:41.272055: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.922923, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2021036\n",
            "batch number:  10 \tgen loss:  6.3234587\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.167272 \t\t\tdisc loss:  0.29468843\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.500854\n",
            "batch number:  4 \tgen loss:  9.515952\n",
            "batch number:  8 \tgen loss:  9.5245495\n",
            "batch number:  12 \tgen loss:  9.498818\n",
            "batch number:  16 \tgen loss:  9.425688\n",
            "batch number:  20 \tgen loss:  9.479171\n",
            "\n",
            "\n",
            "epoch :  165\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:49:56.745274: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:49:56.755011: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.916581, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.192026\n",
            "batch number:  10 \tgen loss:  6.2587996\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.826637 \t\t\tdisc loss:  0.3295337\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.51719\n",
            "batch number:  4 \tgen loss:  9.449751\n",
            "batch number:  8 \tgen loss:  9.520053\n",
            "batch number:  12 \tgen loss:  9.425909\n",
            "batch number:  16 \tgen loss:  9.4515505\n",
            "batch number:  20 \tgen loss:  9.486774\n",
            "\n",
            "\n",
            "epoch :  166\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.921046, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.350654\n",
            "batch number:  10 \tgen loss:  6.257247\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  6.7819505 \t\t\tdisc loss:  0.40851212\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.449883\n",
            "batch number:  4 \tgen loss:  9.531443\n",
            "batch number:  8 \tgen loss:  9.512034\n",
            "batch number:  12 \tgen loss:  9.472598\n",
            "batch number:  16 \tgen loss:  9.512493\n",
            "batch number:  20 \tgen loss:  9.540184\n",
            "\n",
            "\n",
            "epoch :  167\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:50:27.781874: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:50:27.794730: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.941349, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2879453\n",
            "batch number:  10 \tgen loss:  6.1723704\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.2747617 \t\t\tdisc loss:  0.3540718\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.44543\n",
            "batch number:  4 \tgen loss:  9.497229\n",
            "batch number:  8 \tgen loss:  9.509052\n",
            "batch number:  12 \tgen loss:  9.486626\n",
            "batch number:  16 \tgen loss:  9.513657\n",
            "batch number:  20 \tgen loss:  9.474287\n",
            "\n",
            "\n",
            "epoch :  168\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:50:43.441808: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:50:43.460018: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.960873, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2568583\n",
            "batch number:  10 \tgen loss:  6.3833866\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.5804605 \t\t\tdisc loss:  0.36265224\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.485921\n",
            "batch number:  4 \tgen loss:  9.505692\n",
            "batch number:  8 \tgen loss:  9.511916\n",
            "batch number:  12 \tgen loss:  9.432967\n",
            "batch number:  16 \tgen loss:  9.492663\n",
            "batch number:  20 \tgen loss:  9.524355\n",
            "\n",
            "\n",
            "epoch :  169\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:50:59.015898: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:50:59.027638: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.952654, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.380136\n",
            "batch number:  10 \tgen loss:  6.217382\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.179678 \t\t\tdisc loss:  0.2997408\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.518509\n",
            "batch number:  4 \tgen loss:  9.52414\n",
            "batch number:  8 \tgen loss:  9.505634\n",
            "batch number:  12 \tgen loss:  9.519634\n",
            "batch number:  16 \tgen loss:  9.564971\n",
            "batch number:  20 \tgen loss:  9.473525\n",
            "Saved checkpoint for epoch 170: ./tf_ckpts/ckpt-17\n",
            "\n",
            "\n",
            "epoch :  170\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:51:14.170143: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:51:14.176696: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.910132, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.240961\n",
            "batch number:  10 \tgen loss:  6.2475324\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.341501 \t\t\tdisc loss:  0.28791165\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.526227\n",
            "batch number:  4 \tgen loss:  9.509559\n",
            "batch number:  8 \tgen loss:  9.5140085\n",
            "batch number:  12 \tgen loss:  9.512032\n",
            "batch number:  16 \tgen loss:  9.561864\n",
            "batch number:  20 \tgen loss:  9.58968\n",
            "\n",
            "\n",
            "epoch :  171\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:51:29.508057: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.868755, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.37962\n",
            "batch number:  10 \tgen loss:  6.3470106\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.230947 \t\t\tdisc loss:  0.25599697\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.487796\n",
            "batch number:  4 \tgen loss:  9.497917\n",
            "batch number:  8 \tgen loss:  9.469833\n",
            "batch number:  12 \tgen loss:  9.575515\n",
            "batch number:  16 \tgen loss:  9.541042\n",
            "batch number:  20 \tgen loss:  9.476913\n",
            "\n",
            "\n",
            "epoch :  172\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:51:44.928617: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:51:44.947340: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.880198, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2526937\n",
            "batch number:  10 \tgen loss:  6.3773527\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.985756 \t\t\tdisc loss:  0.27802527\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4582815\n",
            "batch number:  4 \tgen loss:  9.509366\n",
            "batch number:  8 \tgen loss:  9.522123\n",
            "batch number:  12 \tgen loss:  9.487453\n",
            "batch number:  16 \tgen loss:  9.556081\n",
            "batch number:  20 \tgen loss:  9.549776\n",
            "\n",
            "\n",
            "epoch :  173\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.889553, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.295417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:51:59.908785: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.2841578\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.155559 \t\t\tdisc loss:  0.28368783\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.505611\n",
            "batch number:  4 \tgen loss:  9.494133\n",
            "batch number:  8 \tgen loss:  9.533752\n",
            "batch number:  12 \tgen loss:  9.441197\n",
            "batch number:  16 \tgen loss:  9.477236\n",
            "batch number:  20 \tgen loss:  9.525311\n",
            "\n",
            "\n",
            "epoch :  174\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.891578, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.297287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:52:15.024499: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.308984\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.237306 \t\t\tdisc loss:  0.2739434\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.515658\n",
            "batch number:  4 \tgen loss:  9.438774\n",
            "batch number:  8 \tgen loss:  9.486778\n",
            "batch number:  12 \tgen loss:  9.477662\n",
            "batch number:  16 \tgen loss:  9.514702\n",
            "batch number:  20 \tgen loss:  9.4608965\n",
            "\n",
            "\n",
            "epoch :  175\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.897996, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1877475\n",
            "batch number:  10 \tgen loss:  6.376719\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.106142 \t\t\tdisc loss:  0.32041273\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.453585\n",
            "batch number:  4 \tgen loss:  9.4825\n",
            "batch number:  8 \tgen loss:  9.477478\n",
            "batch number:  12 \tgen loss:  9.541222\n",
            "batch number:  16 \tgen loss:  9.56501\n",
            "batch number:  20 \tgen loss:  9.493953\n",
            "\n",
            "\n",
            "epoch :  176\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.912883, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.154545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:52:44.383106: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.277994\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.019982 \t\t\tdisc loss:  0.25057486\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.46152\n",
            "batch number:  4 \tgen loss:  9.499943\n",
            "batch number:  8 \tgen loss:  9.490988\n",
            "batch number:  12 \tgen loss:  9.513964\n",
            "batch number:  16 \tgen loss:  9.508035\n",
            "batch number:  20 \tgen loss:  9.492979\n",
            "\n",
            "\n",
            "epoch :  177\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.916366, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.24776\n",
            "batch number:  10 \tgen loss:  6.3942966\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.539252 \t\t\tdisc loss:  0.29756492\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.484117\n",
            "batch number:  4 \tgen loss:  9.498132\n",
            "batch number:  8 \tgen loss:  9.524534\n",
            "batch number:  12 \tgen loss:  9.444489\n",
            "batch number:  16 \tgen loss:  9.551921\n",
            "batch number:  20 \tgen loss:  9.541264\n",
            "\n",
            "\n",
            "epoch :  178\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:53:13.118011: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:53:13.130406: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.925633, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.31637\n",
            "batch number:  10 \tgen loss:  6.257614\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.343474 \t\t\tdisc loss:  0.2877754\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.483135\n",
            "batch number:  4 \tgen loss:  9.496452\n",
            "batch number:  8 \tgen loss:  9.494446\n",
            "batch number:  12 \tgen loss:  9.551157\n",
            "batch number:  16 \tgen loss:  9.4474325\n",
            "batch number:  20 \tgen loss:  9.505552\n",
            "\n",
            "\n",
            "epoch :  179\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:53:27.524341: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.931898, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2578783\n",
            "batch number:  10 \tgen loss:  6.300141\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.656815 \t\t\tdisc loss:  0.2762698\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.465486\n",
            "batch number:  4 \tgen loss:  9.444979\n",
            "batch number:  8 \tgen loss:  9.425239\n",
            "batch number:  12 \tgen loss:  9.488569\n",
            "batch number:  16 \tgen loss:  9.50059\n",
            "batch number:  20 \tgen loss:  9.476833\n",
            "Saved checkpoint for epoch 180: ./tf_ckpts/ckpt-18\n",
            "\n",
            "\n",
            "epoch :  180\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.925450, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3034286\n",
            "batch number:  10 \tgen loss:  6.2158437\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.544991 \t\t\tdisc loss:  0.26122802\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.524202\n",
            "batch number:  4 \tgen loss:  9.48748\n",
            "batch number:  8 \tgen loss:  9.512315\n",
            "batch number:  12 \tgen loss:  9.540546\n",
            "batch number:  16 \tgen loss:  9.492904\n",
            "batch number:  20 \tgen loss:  9.493738\n",
            "\n",
            "\n",
            "epoch :  181\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:53:57.096883: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.924411, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.339078\n",
            "batch number:  10 \tgen loss:  6.172392\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.640195 \t\t\tdisc loss:  0.24024716\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.449815\n",
            "batch number:  4 \tgen loss:  9.496602\n",
            "batch number:  8 \tgen loss:  9.495259\n",
            "batch number:  12 \tgen loss:  9.544267\n",
            "batch number:  16 \tgen loss:  9.396717\n",
            "batch number:  20 \tgen loss:  9.461123\n",
            "\n",
            "\n",
            "epoch :  182\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.891867, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.322426\n",
            "batch number:  10 \tgen loss:  6.2660556\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.683594 \t\t\tdisc loss:  0.25067395\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.424015\n",
            "batch number:  4 \tgen loss:  9.461966\n",
            "batch number:  8 \tgen loss:  9.408686\n",
            "batch number:  12 \tgen loss:  9.514593\n",
            "batch number:  16 \tgen loss:  9.479833\n",
            "batch number:  20 \tgen loss:  9.396969\n",
            "\n",
            "\n",
            "epoch :  183\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:54:26.076637: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:54:26.086589: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.938657, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.263065\n",
            "batch number:  10 \tgen loss:  6.3579445\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.6120014 \t\t\tdisc loss:  0.29810697\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.496953\n",
            "batch number:  4 \tgen loss:  9.488272\n",
            "batch number:  8 \tgen loss:  9.497809\n",
            "batch number:  12 \tgen loss:  9.506801\n",
            "batch number:  16 \tgen loss:  9.509593\n",
            "batch number:  20 \tgen loss:  9.495829\n",
            "\n",
            "\n",
            "epoch :  184\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.920895, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2745543\n",
            "batch number:  10 \tgen loss:  6.1637697\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.1951847 \t\t\tdisc loss:  0.33409894\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.536842\n",
            "batch number:  4 \tgen loss:  9.509621\n",
            "batch number:  8 \tgen loss:  9.43808\n",
            "batch number:  12 \tgen loss:  9.469057\n",
            "batch number:  16 \tgen loss:  9.494315\n",
            "batch number:  20 \tgen loss:  9.54414\n",
            "\n",
            "\n",
            "epoch :  185\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:54:55.580479: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.930110, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.292554\n",
            "batch number:  10 \tgen loss:  6.297615\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.17692 \t\t\tdisc loss:  0.28177166\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.52598\n",
            "batch number:  4 \tgen loss:  9.556632\n",
            "batch number:  8 \tgen loss:  9.495156\n",
            "batch number:  12 \tgen loss:  9.435961\n",
            "batch number:  16 \tgen loss:  9.490268\n",
            "batch number:  20 \tgen loss:  9.457039\n",
            "\n",
            "\n",
            "epoch :  186\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:55:10.051889: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.939509, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3780546\n",
            "batch number:  10 \tgen loss:  6.164893\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.920884 \t\t\tdisc loss:  0.24433699\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.514675\n",
            "batch number:  4 \tgen loss:  9.500514\n",
            "batch number:  8 \tgen loss:  9.561167\n",
            "batch number:  12 \tgen loss:  9.510129\n",
            "batch number:  16 \tgen loss:  9.547419\n",
            "batch number:  20 \tgen loss:  9.492398\n",
            "\n",
            "\n",
            "epoch :  187\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.947962, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1798224\n",
            "batch number:  10 \tgen loss:  6.2894964\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.12011 \t\t\tdisc loss:  0.2402958\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.42041\n",
            "batch number:  4 \tgen loss:  9.509884\n",
            "batch number:  8 \tgen loss:  9.483206\n",
            "batch number:  12 \tgen loss:  9.4694195\n",
            "batch number:  16 \tgen loss:  9.450623\n",
            "batch number:  20 \tgen loss:  9.510809\n",
            "\n",
            "\n",
            "epoch :  188\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.927469, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.4297748\n",
            "batch number:  10 \tgen loss:  6.157032\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.376236 \t\t\tdisc loss:  0.24632525\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.456215\n",
            "batch number:  4 \tgen loss:  9.524417\n",
            "batch number:  8 \tgen loss:  9.482133\n",
            "batch number:  12 \tgen loss:  9.577045\n",
            "batch number:  16 \tgen loss:  9.524761\n",
            "batch number:  20 \tgen loss:  9.50165\n",
            "\n",
            "\n",
            "epoch :  189\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.940485, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3040185\n",
            "batch number:  10 \tgen loss:  6.2369404\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.366115 \t\t\tdisc loss:  0.32393616\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.471493\n",
            "batch number:  4 \tgen loss:  9.528238\n",
            "batch number:  8 \tgen loss:  9.510621\n",
            "batch number:  12 \tgen loss:  9.583586\n",
            "batch number:  16 \tgen loss:  9.508463\n",
            "batch number:  20 \tgen loss:  9.475461\n",
            "Saved checkpoint for epoch 190: ./tf_ckpts/ckpt-19\n",
            "\n",
            "\n",
            "epoch :  190\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.957881, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.222042\n",
            "batch number:  10 \tgen loss:  6.305953\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.067183 \t\t\tdisc loss:  0.2262246\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.525698\n",
            "batch number:  4 \tgen loss:  9.5013275\n",
            "batch number:  8 \tgen loss:  9.514697\n",
            "batch number:  12 \tgen loss:  9.463871\n",
            "batch number:  16 \tgen loss:  9.525986\n",
            "batch number:  20 \tgen loss:  9.438436\n",
            "\n",
            "\n",
            "epoch :  191\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:56:22.873047: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:56:22.881903: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.946108, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2923512\n",
            "batch number:  10 \tgen loss:  6.327203\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.922666 \t\t\tdisc loss:  0.23646635\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.467677\n",
            "batch number:  4 \tgen loss:  9.468521\n",
            "batch number:  8 \tgen loss:  9.472012\n",
            "batch number:  12 \tgen loss:  9.510163\n",
            "batch number:  16 \tgen loss:  9.455978\n",
            "batch number:  20 \tgen loss:  9.491509\n",
            "\n",
            "\n",
            "epoch :  192\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:56:37.201761: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:56:37.211033: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.952055, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2642903\n",
            "batch number:  10 \tgen loss:  6.357642\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.658087 \t\t\tdisc loss:  0.24135976\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.539875\n",
            "batch number:  4 \tgen loss:  9.483559\n",
            "batch number:  8 \tgen loss:  9.482063\n",
            "batch number:  12 \tgen loss:  9.459629\n",
            "batch number:  16 \tgen loss:  9.521862\n",
            "batch number:  20 \tgen loss:  9.414833\n",
            "\n",
            "\n",
            "epoch :  193\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.964330, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.268632\n",
            "batch number:  10 \tgen loss:  6.2456346\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.896302 \t\t\tdisc loss:  0.2102398\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.451913\n",
            "batch number:  4 \tgen loss:  9.5026455\n",
            "batch number:  8 \tgen loss:  9.477329\n",
            "batch number:  12 \tgen loss:  9.484085\n",
            "batch number:  16 \tgen loss:  9.472324\n",
            "batch number:  20 \tgen loss:  9.451183\n",
            "\n",
            "\n",
            "epoch :  194\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.963206, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3583975\n",
            "batch number:  10 \tgen loss:  6.250155\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.775853 \t\t\tdisc loss:  0.23205203\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.500479\n",
            "batch number:  4 \tgen loss:  9.460264\n",
            "batch number:  8 \tgen loss:  9.496294\n",
            "batch number:  12 \tgen loss:  9.472544\n",
            "batch number:  16 \tgen loss:  9.481609\n",
            "batch number:  20 \tgen loss:  9.463548\n",
            "\n",
            "\n",
            "epoch :  195\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.951321, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2515197\n",
            "batch number:  10 \tgen loss:  6.28268\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.777346 \t\t\tdisc loss:  0.2478492\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.456446\n",
            "batch number:  4 \tgen loss:  9.45898\n",
            "batch number:  8 \tgen loss:  9.477701\n",
            "batch number:  12 \tgen loss:  9.354449\n",
            "batch number:  16 \tgen loss:  9.429792\n",
            "batch number:  20 \tgen loss:  9.488906\n",
            "\n",
            "\n",
            "epoch :  196\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.924412, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.26967\n",
            "batch number:  10 \tgen loss:  6.3159966\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.742418 \t\t\tdisc loss:  0.23574674\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.460471\n",
            "batch number:  4 \tgen loss:  9.441318\n",
            "batch number:  8 \tgen loss:  9.457809\n",
            "batch number:  12 \tgen loss:  9.4929905\n",
            "batch number:  16 \tgen loss:  9.452363\n",
            "batch number:  20 \tgen loss:  9.4733305\n",
            "\n",
            "\n",
            "epoch :  197\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:57:50.056032: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:57:50.065606: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.908517, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.321091\n",
            "batch number:  10 \tgen loss:  6.2613497\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.352453 \t\t\tdisc loss:  0.22833587\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.426482\n",
            "batch number:  4 \tgen loss:  9.424947\n",
            "batch number:  8 \tgen loss:  9.493773\n",
            "batch number:  12 \tgen loss:  9.461702\n",
            "batch number:  16 \tgen loss:  9.397458\n",
            "batch number:  20 \tgen loss:  9.447096\n",
            "\n",
            "\n",
            "epoch :  198\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:58:04.682237: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:58:04.691581: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.899470, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.147461\n",
            "batch number:  10 \tgen loss:  6.2176394\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.095612 \t\t\tdisc loss:  0.2821254\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.481664\n",
            "batch number:  4 \tgen loss:  9.4667015\n",
            "batch number:  8 \tgen loss:  9.464043\n",
            "batch number:  12 \tgen loss:  9.429069\n",
            "batch number:  16 \tgen loss:  9.4956665\n",
            "batch number:  20 \tgen loss:  9.488869\n",
            "\n",
            "\n",
            "epoch :  199\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:58:19.167898: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:58:19.185963: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.870584, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3630657\n",
            "batch number:  10 \tgen loss:  6.3189974\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.042518 \t\t\tdisc loss:  0.252891\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.462476\n",
            "batch number:  4 \tgen loss:  9.511527\n",
            "batch number:  8 \tgen loss:  9.498185\n",
            "batch number:  12 \tgen loss:  9.530121\n",
            "batch number:  16 \tgen loss:  9.492917\n",
            "batch number:  20 \tgen loss:  9.4843235\n",
            "Saved checkpoint for epoch 200: ./tf_ckpts/ckpt-20\n",
            "\n",
            "\n",
            "epoch :  200\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:58:33.615121: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:58:33.623007: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.881557, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2728734\n",
            "batch number:  10 \tgen loss:  6.2380567\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  6.8071494 \t\t\tdisc loss:  0.32966885\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.589764\n",
            "batch number:  4 \tgen loss:  9.44654\n",
            "batch number:  8 \tgen loss:  9.5055275\n",
            "batch number:  12 \tgen loss:  9.467918\n",
            "batch number:  16 \tgen loss:  9.403466\n",
            "batch number:  20 \tgen loss:  9.513088\n",
            "\n",
            "\n",
            "epoch :  201\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.890534, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.21058\n",
            "batch number:  10 \tgen loss:  6.2090597\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.840709 \t\t\tdisc loss:  0.2731982\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.491127\n",
            "batch number:  4 \tgen loss:  9.46291\n",
            "batch number:  8 \tgen loss:  9.443827\n",
            "batch number:  12 \tgen loss:  9.431316\n",
            "batch number:  16 \tgen loss:  9.491422\n",
            "batch number:  20 \tgen loss:  9.466923\n",
            "\n",
            "\n",
            "epoch :  202\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.905412, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2698116\n",
            "batch number:  10 \tgen loss:  6.2807913\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.402063 \t\t\tdisc loss:  0.22825617\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.513609\n",
            "batch number:  4 \tgen loss:  9.458702\n",
            "batch number:  8 \tgen loss:  9.463185\n",
            "batch number:  12 \tgen loss:  9.483329\n",
            "batch number:  16 \tgen loss:  9.508373\n",
            "batch number:  20 \tgen loss:  9.487841\n",
            "\n",
            "\n",
            "epoch :  203\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 10:59:17.078444: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 10:59:17.097753: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.893587, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3052955\n",
            "batch number:  10 \tgen loss:  6.3867455\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.0397243 \t\t\tdisc loss:  0.3264866\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.48516\n",
            "batch number:  4 \tgen loss:  9.476462\n",
            "batch number:  8 \tgen loss:  9.52618\n",
            "batch number:  12 \tgen loss:  9.431003\n",
            "batch number:  16 \tgen loss:  9.494297\n",
            "batch number:  20 \tgen loss:  9.429167\n",
            "\n",
            "\n",
            "epoch :  204\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.947061, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1991324\n",
            "batch number:  10 \tgen loss:  6.3226495\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  3.420053 \t\t\tdisc loss:  0.7075842\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.538137\n",
            "batch number:  4 \tgen loss:  9.449505\n",
            "batch number:  8 \tgen loss:  9.510138\n",
            "batch number:  12 \tgen loss:  9.43091\n",
            "batch number:  16 \tgen loss:  9.545941\n",
            "batch number:  20 \tgen loss:  9.527613\n",
            "\n",
            "\n",
            "epoch :  205\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.957517, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2211227\n",
            "batch number:  10 \tgen loss:  6.27121\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  5.756766 \t\t\tdisc loss:  0.42464292\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.528843\n",
            "batch number:  4 \tgen loss:  9.513753\n",
            "batch number:  8 \tgen loss:  9.530119\n",
            "batch number:  12 \tgen loss:  9.514004\n",
            "batch number:  16 \tgen loss:  9.523861\n",
            "batch number:  20 \tgen loss:  9.570587\n",
            "\n",
            "\n",
            "epoch :  206\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:00:00.672837: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 11:00:00.686128: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.960294, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2283206\n",
            "batch number:  10 \tgen loss:  6.3597283\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.434133 \t\t\tdisc loss:  0.25119764\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.51935\n",
            "batch number:  4 \tgen loss:  9.491792\n",
            "batch number:  8 \tgen loss:  9.511615\n",
            "batch number:  12 \tgen loss:  9.543852\n",
            "batch number:  16 \tgen loss:  9.479894\n",
            "batch number:  20 \tgen loss:  9.535502\n",
            "\n",
            "\n",
            "epoch :  207\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.960338, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.35887\n",
            "batch number:  10 \tgen loss:  6.2932596\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.951741 \t\t\tdisc loss:  0.24273987\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.472506\n",
            "batch number:  4 \tgen loss:  9.501777\n",
            "batch number:  8 \tgen loss:  9.429764\n",
            "batch number:  12 \tgen loss:  9.438603\n",
            "batch number:  16 \tgen loss:  9.512895\n",
            "batch number:  20 \tgen loss:  9.483717\n",
            "\n",
            "\n",
            "epoch :  208\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.939652, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.355748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:00:29.737224: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.296368\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.014953 \t\t\tdisc loss:  0.22485483\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.468136\n",
            "batch number:  4 \tgen loss:  9.551107\n",
            "batch number:  8 \tgen loss:  9.547882\n",
            "batch number:  12 \tgen loss:  9.498278\n",
            "batch number:  16 \tgen loss:  9.513937\n",
            "batch number:  20 \tgen loss:  9.527008\n",
            "\n",
            "\n",
            "epoch :  209\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.940183, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.291588\n",
            "batch number:  10 \tgen loss:  6.18965\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.363151 \t\t\tdisc loss:  0.21937144\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.5158825\n",
            "batch number:  4 \tgen loss:  9.54355\n",
            "batch number:  8 \tgen loss:  9.531052\n",
            "batch number:  12 \tgen loss:  9.485174\n",
            "batch number:  16 \tgen loss:  9.512562\n",
            "batch number:  20 \tgen loss:  9.492994\n",
            "Saved checkpoint for epoch 210: ./tf_ckpts/ckpt-21\n",
            "\n",
            "\n",
            "epoch :  210\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:01:00.011429: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.954044, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3747897\n",
            "batch number:  10 \tgen loss:  6.320605\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.510851 \t\t\tdisc loss:  0.21780616\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.519882\n",
            "batch number:  4 \tgen loss:  9.43895\n",
            "batch number:  8 \tgen loss:  9.468828\n",
            "batch number:  12 \tgen loss:  9.417909\n",
            "batch number:  16 \tgen loss:  9.457001\n",
            "batch number:  20 \tgen loss:  9.465717\n",
            "\n",
            "\n",
            "epoch :  211\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:01:18.240732: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.960277, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2393956\n",
            "batch number:  10 \tgen loss:  6.2469015\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.300962 \t\t\tdisc loss:  0.22665115\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.550458\n",
            "batch number:  4 \tgen loss:  9.522692\n",
            "batch number:  8 \tgen loss:  9.518149\n",
            "batch number:  12 \tgen loss:  9.4704685\n",
            "batch number:  16 \tgen loss:  9.5233555\n",
            "batch number:  20 \tgen loss:  9.476242\n",
            "\n",
            "\n",
            "epoch :  212\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.970425, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3063264\n",
            "batch number:  10 \tgen loss:  6.3486814\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.291801 \t\t\tdisc loss:  0.22852427\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.490607\n",
            "batch number:  4 \tgen loss:  9.444374\n",
            "batch number:  8 \tgen loss:  9.454435\n",
            "batch number:  12 \tgen loss:  9.495168\n",
            "batch number:  16 \tgen loss:  9.416464\n",
            "batch number:  20 \tgen loss:  9.50576\n",
            "\n",
            "\n",
            "epoch :  213\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:01:54.717349: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.972346, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.246584\n",
            "batch number:  10 \tgen loss:  6.3026175\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  7.8054485 \t\t\tdisc loss:  0.25866994\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.498349\n",
            "batch number:  4 \tgen loss:  9.471684\n",
            "batch number:  8 \tgen loss:  9.464105\n",
            "batch number:  12 \tgen loss:  9.397038\n",
            "batch number:  16 \tgen loss:  9.430627\n",
            "batch number:  20 \tgen loss:  9.436466\n",
            "\n",
            "\n",
            "epoch :  214\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:02:11.977298: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 11:02:12.006290: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.973598, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.3797846\n",
            "batch number:  10 \tgen loss:  6.3343825\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.482695 \t\t\tdisc loss:  0.22201438\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.398522\n",
            "batch number:  4 \tgen loss:  9.453326\n",
            "batch number:  8 \tgen loss:  9.498211\n",
            "batch number:  12 \tgen loss:  9.446936\n",
            "batch number:  16 \tgen loss:  9.503451\n",
            "batch number:  20 \tgen loss:  9.461774\n",
            "\n",
            "\n",
            "epoch :  215\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:02:28.883076: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.967506, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.23818\n",
            "batch number:  10 \tgen loss:  6.364841\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.682985 \t\t\tdisc loss:  0.20237541\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.509821\n",
            "batch number:  4 \tgen loss:  9.504853\n",
            "batch number:  8 \tgen loss:  9.466955\n",
            "batch number:  12 \tgen loss:  9.462914\n",
            "batch number:  16 \tgen loss:  9.422119\n",
            "batch number:  20 \tgen loss:  9.4964905\n",
            "\n",
            "\n",
            "epoch :  216\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.974886, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2729297\n",
            "batch number:  10 \tgen loss:  6.2384624\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.34302 \t\t\tdisc loss:  0.28608274\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.511039\n",
            "batch number:  4 \tgen loss:  9.477391\n",
            "batch number:  8 \tgen loss:  9.441684\n",
            "batch number:  12 \tgen loss:  9.474701\n",
            "batch number:  16 \tgen loss:  9.473231\n",
            "batch number:  20 \tgen loss:  9.507364\n",
            "\n",
            "\n",
            "epoch :  217\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.975955, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.279525\n",
            "batch number:  10 \tgen loss:  6.330799\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.81782 \t\t\tdisc loss:  0.20093557\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.460196\n",
            "batch number:  4 \tgen loss:  9.465464\n",
            "batch number:  8 \tgen loss:  9.440215\n",
            "batch number:  12 \tgen loss:  9.436869\n",
            "batch number:  16 \tgen loss:  9.471228\n",
            "batch number:  20 \tgen loss:  9.454192\n",
            "\n",
            "\n",
            "epoch :  218\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:03:17.807585: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.978834, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2559648\n",
            "batch number:  10 \tgen loss:  6.140305\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.677764 \t\t\tdisc loss:  0.20364197\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.478739\n",
            "batch number:  4 \tgen loss:  9.480541\n",
            "batch number:  8 \tgen loss:  9.468155\n",
            "batch number:  12 \tgen loss:  9.470825\n",
            "batch number:  16 \tgen loss:  9.4425125\n",
            "batch number:  20 \tgen loss:  9.499689\n",
            "\n",
            "\n",
            "epoch :  219\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.974508, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.275163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:03:32.973183: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.274076\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.899131 \t\t\tdisc loss:  0.20217305\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.4443245\n",
            "batch number:  4 \tgen loss:  9.413782\n",
            "batch number:  8 \tgen loss:  9.491481\n",
            "batch number:  12 \tgen loss:  9.465697\n",
            "batch number:  16 \tgen loss:  9.441533\n",
            "batch number:  20 \tgen loss:  9.374177\n",
            "Saved checkpoint for epoch 220: ./tf_ckpts/ckpt-22\n",
            "\n",
            "\n",
            "epoch :  220\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.960158, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1551323\n",
            "batch number:  10 \tgen loss:  6.257539\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.970988 \t\t\tdisc loss:  0.18938932\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.503317\n",
            "batch number:  4 \tgen loss:  9.481642\n",
            "batch number:  8 \tgen loss:  9.432519\n",
            "batch number:  12 \tgen loss:  9.446281\n",
            "batch number:  16 \tgen loss:  9.405622\n",
            "batch number:  20 \tgen loss:  9.372344\n",
            "\n",
            "\n",
            "epoch :  221\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.960657, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2885337\n",
            "batch number:  10 \tgen loss:  6.377441\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.514806 \t\t\tdisc loss:  0.2192935\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.422238\n",
            "batch number:  4 \tgen loss:  9.425204\n",
            "batch number:  8 \tgen loss:  9.402281\n",
            "batch number:  12 \tgen loss:  9.395673\n",
            "batch number:  16 \tgen loss:  9.456491\n",
            "batch number:  20 \tgen loss:  9.3931675\n",
            "\n",
            "\n",
            "epoch :  222\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.917044, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.344259\n",
            "batch number:  10 \tgen loss:  6.2809567\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.675772 \t\t\tdisc loss:  0.1925347\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.347703\n",
            "batch number:  4 \tgen loss:  9.405926\n",
            "batch number:  8 \tgen loss:  9.398173\n",
            "batch number:  12 \tgen loss:  9.388571\n",
            "batch number:  16 \tgen loss:  9.420155\n",
            "batch number:  20 \tgen loss:  9.418102\n",
            "\n",
            "\n",
            "epoch :  223\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.929557, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.303753\n",
            "batch number:  10 \tgen loss:  6.310701\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.672773 \t\t\tdisc loss:  0.21004175\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.381068\n",
            "batch number:  4 \tgen loss:  9.443137\n",
            "batch number:  8 \tgen loss:  9.429065\n",
            "batch number:  12 \tgen loss:  9.440634\n",
            "batch number:  16 \tgen loss:  9.489374\n",
            "batch number:  20 \tgen loss:  9.427078\n",
            "\n",
            "\n",
            "epoch :  224\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.958884, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2727466\n",
            "batch number:  10 \tgen loss:  6.240947\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.922279 \t\t\tdisc loss:  0.20798364\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.420935\n",
            "batch number:  4 \tgen loss:  9.451633\n",
            "batch number:  8 \tgen loss:  9.419899\n",
            "batch number:  12 \tgen loss:  9.452742\n",
            "batch number:  16 \tgen loss:  9.433629\n",
            "batch number:  20 \tgen loss:  9.450588\n",
            "\n",
            "\n",
            "epoch :  225\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:05:05.023255: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.963661, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.2534018\n",
            "batch number:  10 \tgen loss:  6.2302084\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.858602 \t\t\tdisc loss:  0.19487217\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.443447\n",
            "batch number:  4 \tgen loss:  9.417386\n",
            "batch number:  8 \tgen loss:  9.423144\n",
            "batch number:  12 \tgen loss:  9.43995\n",
            "batch number:  16 \tgen loss:  9.505506\n",
            "batch number:  20 \tgen loss:  9.520137\n",
            "\n",
            "\n",
            "epoch :  226\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:05:20.689767: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 11:05:20.710315: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.941088, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.187401\n",
            "batch number:  10 \tgen loss:  6.286758\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.09938 \t\t\tdisc loss:  0.20666078\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.497944\n",
            "batch number:  4 \tgen loss:  9.473017\n",
            "batch number:  8 \tgen loss:  9.506714\n",
            "batch number:  12 \tgen loss:  9.483366\n",
            "batch number:  16 \tgen loss:  9.523637\n",
            "batch number:  20 \tgen loss:  9.502146\n",
            "\n",
            "\n",
            "epoch :  227\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.936404, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.284802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:05:36.257454: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch number:  10 \tgen loss:  6.2125764\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.301379 \t\t\tdisc loss:  0.2910789\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.553051\n",
            "batch number:  4 \tgen loss:  9.541781\n",
            "batch number:  8 \tgen loss:  9.598869\n",
            "batch number:  12 \tgen loss:  9.504289\n",
            "batch number:  16 \tgen loss:  9.600315\n",
            "batch number:  20 \tgen loss:  9.563799\n",
            "\n",
            "\n",
            "epoch :  228\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:05:52.355593: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.948330, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.172027\n",
            "batch number:  10 \tgen loss:  6.2914457\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.395523 \t\t\tdisc loss:  0.19899975\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.523305\n",
            "batch number:  4 \tgen loss:  9.540644\n",
            "batch number:  8 \tgen loss:  9.574851\n",
            "batch number:  12 \tgen loss:  9.509622\n",
            "batch number:  16 \tgen loss:  9.535105\n",
            "batch number:  20 \tgen loss:  9.623073\n",
            "\n",
            "\n",
            "epoch :  229\n",
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training RMSE: 0.957672, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.1225195\n",
            "batch number:  10 \tgen loss:  6.315182\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  9.042462 \t\t\tdisc loss:  0.1953794\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.500109\n",
            "batch number:  4 \tgen loss:  9.509874\n",
            "batch number:  8 \tgen loss:  9.505979\n",
            "batch number:  12 \tgen loss:  9.5464\n",
            "batch number:  16 \tgen loss:  9.601227\n",
            "batch number:  20 \tgen loss:  9.472962\n",
            "Saved checkpoint for epoch 230: ./tf_ckpts/ckpt-23\n",
            "\n",
            "\n",
            "epoch :  230\n",
            "********************************************* PMF Model Training Turn *********************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-18 11:06:23.938265: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n",
            "2022-12-18 11:06:23.944532: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training RMSE: 0.963949, Test RMSE 0.000000\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "\n",
            "\n",
            "Teacher Forcing Train:\n",
            "batch number:  0 \tgen loss:  6.206439\n",
            "batch number:  10 \tgen loss:  6.1945677\n",
            "\n",
            "\n",
            "Adversarial Train:\n",
            "batch number:  0 \t\tgen loss:  8.441091 \t\t\tdisc loss:  0.22280055\n",
            "\n",
            "\n",
            "Generator Test:\n",
            "batch number:  0 \tgen loss:  9.469019\n",
            "batch number:  4 \tgen loss:  9.512261\n",
            "batch number:  8 \tgen loss:  9.453023\n",
            "batch number:  12 \tgen loss:  9.481948\n",
            "batch number:  16 \tgen loss:  9.477147\n",
            "batch number:  20 \tgen loss:  9.53027\n"
          ]
        }
      ],
      "source": [
        "mt_model.train(n_epochs=3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ploting Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
        "\n",
        "ax.plot(np.arange(len(mt_model.pmf_model.rmse_train)), mt_model.pmf_model.rmse_train, label='RMSE train')\n",
        "ax.plot(np.arange(len(mt_model.pmf_model.rmse_test)), mt_model.pmf_model.rmse_test, label='RMSE test')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-c20f2b2347f8d3d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-c20f2b2347f8d3d5\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs\n",
        "# deactive tracking protection of the page if you get 403 error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniWi24qcZsb"
      },
      "source": [
        "# test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85ryd-vJrTix"
      },
      "outputs": [],
      "source": [
        "#log_ps, mse_train, mse_test= pm.train(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4meEBmwsqMYC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dec_result,dec_state=decoder(dec_output_tokens,dec_output_tokens,context_vector= user_context_vector,state=dec_state)\n",
        "# sampled_token = tf.random.categorical(dec_result[:,-1, :], num_samples=1)\n",
        "# print( sampled_token.numpy())\n",
        "# dec_output_tokens=np.append( dec_output_tokens , sampled_token.numpy()[0])\n",
        "# print(dec_output_tokens)\n",
        "\n",
        "# vocab = np.array(input_text_processor.get_vocabulary())\n",
        "# first_word = vocab[dec_output_tokens]\n",
        "# first_word"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Explainable Recommender System.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('colabenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5859fc1fd51a29bc96a6c335b5cef2533de774a99a73e1484108bae0d11f06ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
